{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCE74ZzIJmg3",
        "outputId": "4f43377d-3f78-4e5b-8d06-acf98e1e1301"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from tqdm import tqdm  # For progress bar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28DBGvW4RxFN"
      },
      "source": [
        "In the following we define the LoveDA dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lZI3lYqygKQt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class LoveDADataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None, target_transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (str): Path to the directory with input images.\n",
        "            mask_dir (str): Path to the directory with masks.\n",
        "            transform (callable, optional): Transformations for the input images.\n",
        "            target_transform (callable, optional): Transformations for the masks.\n",
        "        \"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.image_filenames = sorted(os.listdir(image_dir))\n",
        "        self.mask_filenames = sorted(os.listdir(mask_dir))\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.class_weights = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image and mask\n",
        "        image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])\n",
        "\n",
        "        # Use PIL to load images\n",
        "        image = Image.open(image_path).convert(\"RGB\")  # Convert to 3-channel RGB\n",
        "        mask = Image.open(mask_path)  # Grayscale mask (single channel)\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            mask = self.target_transform(mask)\n",
        "\n",
        "        return image, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import torch\n",
        "from datasets.base_dataset import BaseDataset\n",
        "\n",
        "class LoveDA(BaseDataset):\n",
        "    def __init__(self, \n",
        "                 image_dir, \n",
        "                 mask_dir,\n",
        "                 num_classes=7,\n",
        "                 multi_scale=True, \n",
        "                 flip=True, \n",
        "                 ignore_label=-1, \n",
        "                 base_size=1024, \n",
        "                 crop_size=(128, 128),\n",
        "                 scale_factor=16,\n",
        "                 mean=[0.485, 0.456, 0.406], \n",
        "                 std=[0.229, 0.224, 0.225],\n",
        "                 bd_dilate_size=4):\n",
        "\n",
        "        super(LoveDA, self).__init__(ignore_label, base_size,\n",
        "                crop_size, scale_factor, mean, std,)\n",
        "\n",
        "        self.mask_dir = mask_dir\n",
        "        self.image_dir = image_dir\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.multi_scale = multi_scale\n",
        "        self.flip = flip\n",
        "        self.image_filenames = sorted(os.listdir(image_dir))\n",
        "        self.mask_filenames = sorted(os.listdir(mask_dir))\n",
        "        \n",
        "        #self.img_list = [line.strip().split() for line in open(root + list_path)]\n",
        "\n",
        "        # Define label mapping for LoveDA\n",
        "        self.label_mapping = {\n",
        "            0: ignore_label,  # Background\n",
        "            1: 0,  # Urban land\n",
        "            2: 1,  # Agriculture\n",
        "            3: 2,  # Rangeland\n",
        "            4: 3,  # Forest\n",
        "            5: 4,  # Water\n",
        "            6: 5,  # Barren land\n",
        "            7: 6   # Unknown\n",
        "        }\n",
        "\n",
        "        # Class weights for LoveDA (example values; adjust as needed)\n",
        "        self.class_weights = torch.FloatTensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]).cuda()\n",
        "\n",
        "        self.bd_dilate_size = bd_dilate_size\n",
        "    \n",
        "        \n",
        "    def convert_label(self, label, inverse=False):\n",
        "        temp = label.copy()\n",
        "        if inverse:\n",
        "            for v, k in self.label_mapping.items():\n",
        "                label[temp == k] = v\n",
        "        else:\n",
        "            for k, v in self.label_mapping.items():\n",
        "                label[temp == k] = v\n",
        "        return label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = os.path.join(self.image_dir, self.image_filenames[index])\n",
        "        mask_path = os.path.join(self.mask_dir, self.mask_filenames[index])\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "        size = image.shape\n",
        "\n",
        "        '''if 'test' in self.list_path:\n",
        "            image = self.input_transform(image)\n",
        "            image = image.transpose((2, 0, 1))\n",
        "\n",
        "            return image.copy(), np.array(size), self.image_filenames[index]'''\n",
        "\n",
        "        label = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        label = self.convert_label(label)\n",
        "\n",
        "        image, label, edge = self.gen_sample(image, label, \n",
        "                                self.multi_scale, self.flip, edge_size=self.bd_dilate_size)\n",
        "\n",
        "        return image.copy(), label.copy(), edge.copy(), np.array(size), self.image_filenames[index]\n",
        "\n",
        "    def single_scale_inference(self, config, model, image):\n",
        "        pred = self.inference(config, model, image)\n",
        "        return pred\n",
        "\n",
        "    def save_pred(self, preds, sv_path, name):\n",
        "        preds = np.asarray(np.argmax(preds.cpu(), axis=1), dtype=np.uint8)\n",
        "        for i in range(preds.shape[0]):\n",
        "            pred = self.convert_label(preds[i], inverse=True)\n",
        "            save_img = Image.fromarray(pred)\n",
        "            save_img.save(os.path.join(sv_path, name[i] + '.png'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verifica codice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data\\LoveDA\\train\\Urban\\images_png\\1366.png\n",
            "data\\LoveDA\\train\\Urban\\masks_png\\1366.png\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "image_path = 'data\\\\LoveDA\\\\train\\\\Urban\\\\images_png'\n",
        "mask_path = 'data\\\\LoveDA\\\\train\\\\Urban\\\\masks_png'\n",
        "\n",
        "assert os.path.exists(image_path), f\"Image path does not exist: {image_path}\"\n",
        "assert os.path.exists(mask_path), f\"Mask path does not exist: {mask_path}\"\n",
        "\n",
        "image_path = os.path.join(image_path, '1366.png')\n",
        "print(image_path)\n",
        "mask_path = os.path.join(mask_path, '1366.png')\n",
        "print(mask_path)\n",
        "try:\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    mask = Image.open(mask_path)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Error loading image or mask at index : {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf4xul1YcZ-a"
      },
      "source": [
        "Load pretrained PIDNet-S on ImageNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uTRRh96tb7cQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Namespace(cfg='configs/LoveDA/pidnet_small_loveda_pretrained.yaml', seed=0, opts=[])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> creating output\\loveda\\pidnet_small_loveda_pretrained\n",
            "=> creating log\\loveda\\pidnet_small\\pidnet_small_loveda_pretrained_2024-12-23-18-10\n",
            "-1\n",
            "-1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_24916\\812570463.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_state = torch.load('pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar')['state_dict']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "DataLoader worker (pid(s) 26772, 30044) exited unexpectedly",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\envs\\aml\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1243\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
            "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\envs\\aml\\Lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
            "\u001b[1;31mEmpty\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 232\u001b[0m\n\u001b[0;32m    229\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 232\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[5], line 194\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_trainloader\u001b[38;5;241m.\u001b[39msampler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(current_trainloader\u001b[38;5;241m.\u001b[39msampler, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    192\u001b[0m     current_trainloader\u001b[38;5;241m.\u001b[39msampler\u001b[38;5;241m.\u001b[39mset_epoch(epoch)\n\u001b[1;32m--> 194\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_EPOCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepoch_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flag_rm \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m<\u001b[39m num_epochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m5\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (epoch \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m num_epochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m    199\u001b[0m     valid_loss, mean_IoU, IoU_array \u001b[38;5;241m=\u001b[39m validate(config, \n\u001b[0;32m    200\u001b[0m                 val_loader, model, writer_dict)\n",
            "File \u001b[1;32mc:\\Users\\Utente\\Desktop\\poli\\materie\\Advanced Machine Learning\\AML-Project\\PIDNet-main\\PIDNet-main\\utils\\function.py:40\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(config, epoch, num_epoch, epoch_iters, base_lr, num_iters, trainloader, optimizer, model, writer_dict)\u001b[0m\n\u001b[0;32m     35\u001b[0m writer \u001b[38;5;241m=\u001b[39m writer_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwriter\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     36\u001b[0m global_steps \u001b[38;5;241m=\u001b[39m writer_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_global_steps\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 40\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbd_gts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\envs\\aml\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
            "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\envs\\aml\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\envs\\aml\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\envs\\aml\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1256\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1255\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1257\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1258\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 26772, 30044) exited unexpectedly"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import sys\n",
        "import logging\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm  # Import tqdm for progress bars\n",
        "from torchmetrics.classification import JaccardIndex\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from models.pidnet import PIDNet\n",
        "from configs import config\n",
        "from configs import update_config\n",
        "from utils.criterion import CrossEntropy, OhemCrossEntropy, BondaryLoss\n",
        "from utils.function import train, validate\n",
        "from utils.utils import create_logger, FullModel\n",
        "import timeit\n",
        "import torch.backends.cudnn as cudnn\n",
        "import pprint\n",
        "import argparse\n",
        "import numpy as np\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "def squeeze_channel(tensor):\n",
        "    return tensor.squeeze(0).long()\n",
        "\n",
        "def rescale_labels(tensor):\n",
        "    \"\"\"\n",
        "    Rescales labels:\n",
        "      - Class 0 -> -1 (ignore)\n",
        "      - Classes 1–7 -> 0–6\n",
        "    \"\"\"\n",
        "    tensor = tensor.squeeze(0).long()\n",
        "    tensor = tensor - 1  # Shift labels down by 1\n",
        "    tensor[tensor == -1] = -1  # Ensure 0 becomes -1\n",
        "    return tensor\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description='Train segmentation network')\n",
        "    \n",
        "    parser.add_argument('--cfg',\n",
        "                        help='experiment configure file name',\n",
        "                        default=\"configs/LoveDA/pidnet_small_loveda_pretrained.yaml\",\n",
        "                        type=str)\n",
        "    parser.add_argument('--seed', type=int, default=0)    \n",
        "    parser.add_argument('opts',\n",
        "                        help=\"Modify config options using the command-line\",\n",
        "                        default=None,\n",
        "                        nargs=argparse.REMAINDER)\n",
        "\n",
        "    args, _ = parser.parse_known_args()  # This ignores unrecognized arguments\n",
        "    update_config(config, args)\n",
        "\n",
        "    return args\n",
        "\n",
        "def main():\n",
        "\n",
        "    args = parse_args()\n",
        "\n",
        "    if args.seed > 0:\n",
        "        import random\n",
        "        print('Seeding with', args.seed)\n",
        "        random.seed(args.seed)\n",
        "        torch.manual_seed(args.seed)        \n",
        "\n",
        "    logger, final_output_dir, tb_log_dir = create_logger(\n",
        "        config, args.cfg, 'train')\n",
        "\n",
        "    logger.info(pprint.pformat(args))\n",
        "\n",
        "    writer_dict = {\n",
        "        'writer': SummaryWriter(tb_log_dir),\n",
        "        'train_global_steps': 0,\n",
        "        'valid_global_steps': 0,\n",
        "    }\n",
        "\n",
        "    pretrained_state = torch.load('pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar')['state_dict']\n",
        "\n",
        "    model = PIDNet(m=2, n=3, num_classes=7, planes=32, ppm_planes=96, head_planes=128, augment=False)\n",
        "    model_dict = model.state_dict()\n",
        "    pretrained_state = {k: v for k, v in pretrained_state.items() if (k in model_dict and v.shape == model_dict[k].shape)}\n",
        "    model_dict.update(pretrained_state)\n",
        "    msg = 'Loaded {} parameters!'.format(len(pretrained_state))\n",
        "    #logging.info('Attention!!!')\n",
        "    #logging.info(msg)\n",
        "    #logging.info('Over!!!')\n",
        "    model.load_state_dict(model_dict, strict = False)\n",
        "\n",
        "\n",
        "\n",
        "    # Define transformations for images and masks\n",
        "    img_transforms = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),  # Resize to 512x512\n",
        "        transforms.ToTensor(),          # Convert image to tensor\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "    ])\n",
        "\n",
        "    mask_transforms = transforms.Compose([\n",
        "        transforms.Resize((128, 128), interpolation=transforms.InterpolationMode.NEAREST),  # Resize mask\n",
        "        transforms.PILToTensor(),  # Convert mask to tensor\n",
        "        transforms.Lambda(rescale_labels)  # Remove channel dimension\n",
        "    ])\n",
        "\n",
        "\n",
        "    # Dataset paths\n",
        "    train_dataset = LoveDA(\n",
        "        image_dir=\"data\\\\LoveDA\\\\train\\\\Urban\\\\images_png\",\n",
        "        mask_dir=\"data\\\\LoveDA\\\\train\\\\Urban\\\\masks_png\",\n",
        "        #transform=img_transforms,\n",
        "        #target_transform=mask_transforms\n",
        "    )\n",
        "\n",
        "    val_dataset = LoveDA(\n",
        "        image_dir=\"data\\\\LoveDA\\\\val\\\\Urban\\\\images_png\",\n",
        "        mask_dir=\"data\\\\LoveDA\\\\val\\\\Urban\\\\masks_png\",\n",
        "        #transform=img_transforms,\n",
        "        #target_transform=mask_transforms\n",
        "    )\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "    num_classes = 7  # Update based on LoveDA\n",
        "    \n",
        "    model = PIDNet(m=2, n=3, num_classes=num_classes, planes=32, ppm_planes=96, head_planes=128, augment=True)\n",
        "    model_dict = model.state_dict()\n",
        "    pretrained_state = {k: v for k, v in pretrained_state.items() if (k in model_dict and v.shape == model_dict[k].shape)}\n",
        "    model_dict.update(pretrained_state)\n",
        "    msg = 'Loaded {} parameters!'.format(len(pretrained_state))\n",
        "    #logging.info('Attention!!!')\n",
        "    #logging.info(msg)\n",
        "    #logging.info('Over!!!')\n",
        "    model.load_state_dict(model_dict, strict = False)\n",
        "    \n",
        "\n",
        "    # criterion\n",
        "    if config.LOSS.USE_OHEM:\n",
        "        sem_criterion = OhemCrossEntropy(ignore_label=config.TRAIN.IGNORE_LABEL,\n",
        "                                        thres=config.LOSS.OHEMTHRES,\n",
        "                                        min_kept=config.LOSS.OHEMKEEP,\n",
        "                                        weight=train_dataset.class_weights)\n",
        "    else:\n",
        "        sem_criterion = CrossEntropy(ignore_label=config.TRAIN.IGNORE_LABEL,\n",
        "                                    weight=train_dataset.class_weights)\n",
        "\n",
        "    bd_criterion = BondaryLoss()\n",
        "    \n",
        "    cudnn.benchmark = config.CUDNN.BENCHMARK\n",
        "    cudnn.deterministic = config.CUDNN.DETERMINISTIC\n",
        "    cudnn.enabled = config.CUDNN.ENABLED\n",
        "    gpus = list(config.GPUS)\n",
        "    if torch.cuda.device_count() != len(gpus):\n",
        "        print(\"The gpu numbers do not match!\")\n",
        "        return 0\n",
        "    epoch_iters = int(train_dataset.__len__() / config.TRAIN.BATCH_SIZE_PER_GPU / len(gpus))\n",
        "        \n",
        "    model = FullModel(model, sem_criterion, bd_criterion)\n",
        "    model = nn.DataParallel(model, device_ids=gpus).cuda()\n",
        "\n",
        "    # optimizer\n",
        "    if config.TRAIN.OPTIMIZER == 'sgd':\n",
        "        params_dict = dict(model.named_parameters())\n",
        "        params = [{'params': list(params_dict.values()), 'lr': config.TRAIN.LR}]\n",
        "\n",
        "        optimizer = torch.optim.SGD(params,\n",
        "                                lr=config.TRAIN.LR,\n",
        "                                momentum=config.TRAIN.MOMENTUM,\n",
        "                                weight_decay=config.TRAIN.WD,\n",
        "                                nesterov=config.TRAIN.NESTEROV,\n",
        "                                )\n",
        "    else:\n",
        "        raise ValueError('Only Support SGD optimizer')\n",
        "\n",
        "    # Training loop\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    num_epochs = 20  # Number of epochs\n",
        "    best_mIoU = 0\n",
        "    last_epoch = 0\n",
        "    start = timeit.default_timer()\n",
        "    end_epoch = config.TRAIN.END_EPOCH\n",
        "    num_iters = config.TRAIN.END_EPOCH * epoch_iters\n",
        "    real_end = 120+1 if 'camvid' in config.DATASET.TRAIN_SET else end_epoch\n",
        "    \n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        current_trainloader = train_loader\n",
        "        if current_trainloader.sampler is not None and hasattr(current_trainloader.sampler, 'set_epoch'):\n",
        "            current_trainloader.sampler.set_epoch(epoch)\n",
        "\n",
        "        train(config, epoch, config.TRAIN.END_EPOCH, \n",
        "                  epoch_iters, config.TRAIN.LR, num_iters,\n",
        "                  train_loader, optimizer, model, writer_dict)\n",
        "\n",
        "        if flag_rm == 1 or (epoch % 5 == 0 and epoch < num_epochs - 5) or (epoch >= num_epochs - 5):\n",
        "            valid_loss, mean_IoU, IoU_array = validate(config, \n",
        "                        val_loader, model, writer_dict)\n",
        "        if flag_rm == 1:\n",
        "            flag_rm = 0\n",
        "\n",
        "        logger.info('=> saving checkpoint to {}'.format(\n",
        "            final_output_dir + 'checkpoint.pth.tar'))\n",
        "        torch.save({\n",
        "            'epoch': epoch+1,\n",
        "            'best_mIoU': best_mIoU,\n",
        "            'state_dict': model.module.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "        }, os.path.join(final_output_dir,'checkpoint.pth.tar'))\n",
        "        if mean_IoU > best_mIoU:\n",
        "            best_mIoU = mean_IoU\n",
        "            torch.save(model.module.state_dict(),\n",
        "                    os.path.join(final_output_dir, 'best.pt'))\n",
        "        msg = 'Loss: {:.3f}, MeanIU: {: 4.4f}, Best_mIoU: {: 4.4f}'.format(\n",
        "                    valid_loss, mean_IoU, best_mIoU)\n",
        "        #logging.info(msg)\n",
        "        #logging.info(IoU_array)\n",
        "\n",
        "\n",
        "\n",
        "    torch.save(model.module.state_dict(),\n",
        "            os.path.join(final_output_dir, 'final_state.pt'))\n",
        "\n",
        "    writer_dict['writer'].close()\n",
        "    end = timeit.default_timer()\n",
        "    logger.info('Hours: %d' % int((end-start)/3600))\n",
        "    logger.info('Done')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
