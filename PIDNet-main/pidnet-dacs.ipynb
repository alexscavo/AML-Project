{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:02:57.323680Z",
     "iopub.status.busy": "2025-05-12T16:02:57.323377Z",
     "iopub.status.idle": "2025-05-12T16:02:57.359200Z",
     "shell.execute_reply": "2025-05-12T16:02:57.358335Z",
     "shell.execute_reply.started": "2025-05-12T16:02:57.323658Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "os.makedirs('/root/.config/kaggle', exist_ok=True)\n",
    "shutil.copy('/kaggle/input/kaggle-api/kaggle.json', '/root/.config/kaggle/kaggle.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:02:57.445718Z",
     "iopub.status.busy": "2025-05-12T16:02:57.445460Z",
     "iopub.status.idle": "2025-05-12T16:03:02.650222Z",
     "shell.execute_reply": "2025-05-12T16:03:02.649503Z",
     "shell.execute_reply.started": "2025-05-12T16:02:57.445699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q albumentations==1.4.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-12T16:03:02.651793Z",
     "iopub.status.busy": "2025-05-12T16:03:02.651557Z",
     "iopub.status.idle": "2025-05-12T16:03:09.036462Z",
     "shell.execute_reply": "2025-05-12T16:03:09.035724Z",
     "shell.execute_reply.started": "2025-05-12T16:03:02.651772Z"
    },
    "id": "Pqh-LuR5inRH",
    "outputId": "c05502f6-b016-43bd-b554-1e4ea2614543",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install yacs\n",
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:03:09.037422Z",
     "iopub.status.busy": "2025-05-12T16:03:09.037205Z",
     "iopub.status.idle": "2025-05-12T16:03:13.398075Z",
     "shell.execute_reply": "2025-05-12T16:03:13.397161Z",
     "shell.execute_reply.started": "2025-05-12T16:03:09.037402Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q kaggle\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import subprocess\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "dataset_id = user_secrets.get_secret(\"DATASET_ID\")\n",
    "\n",
    "\n",
    "# Step 1: Check if dataset already exists\n",
    "try:\n",
    "    # Run the Kaggle API command to check dataset\n",
    "    result = subprocess.run(\n",
    "        [\"kaggle\", \"datasets\", \"metadata\", \"-d\", dataset_id],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(f\"✅ Dataset already exists: {dataset_id} — Skipping creation.\")\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # Download the zip files\n",
    "    !wget https://zenodo.org/records/5706578/files/Train.zip?download=1 -O Train.zip\n",
    "    !wget https://zenodo.org/records/5706578/files/Val.zip?download=1 -O Val.zip\n",
    "    \n",
    "    # Unzip them\n",
    "    !unzip -q Train.zip -d splits/\n",
    "    !unzip -q Val.zip -d splits/\n",
    "    \n",
    "    # Step 2: Organize files\n",
    "    os.makedirs('zips/', exist_ok=True)\n",
    "    shutil.copy('Train.zip', 'zips')\n",
    "    shutil.copy('Val.zip', 'zips')\n",
    "\n",
    "    # Step 3: Create metadata\n",
    "    dataset_metadata = {\n",
    "        \"title\": \"LoveDA\",\n",
    "        \"id\": dataset_id,\n",
    "        \"licenses\": [{\"name\": \"CC0-1.0\"}]\n",
    "    }\n",
    "    with open('zips/dataset-metadata.json', 'w') as f:\n",
    "        json.dump(dataset_metadata, f)\n",
    "\n",
    "    # Step 4: Create the dataset\n",
    "    !kaggle datasets create -p zips\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-12T16:03:13.399946Z",
     "iopub.status.busy": "2025-05-12T16:03:13.399702Z",
     "iopub.status.idle": "2025-05-12T16:03:13.406708Z",
     "shell.execute_reply": "2025-05-12T16:03:13.405837Z",
     "shell.execute_reply.started": "2025-05-12T16:03:13.399924Z"
    },
    "id": "9qCYBoYv1WQZ",
    "outputId": "bc9151bb-d1fa-4c35-f86d-7f2aaa284b17",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_kaggle_path = \"/kaggle/working/\"\n",
    "folder_names = [\n",
    "    'data/loveda/train/',\n",
    "    'data/loveda/val/',\n",
    "    'data/list/loveda/rural',\n",
    "    'data/list/loveda/urban_rural',\n",
    "    'data/list/loveda/urban_urban',\n",
    "    'pretrained_models/imagenet'\n",
    "]\n",
    "\n",
    "for relative_path in folder_names:\n",
    "    full_path = os.path.join(base_kaggle_path, relative_path)\n",
    "    if not os.path.exists(full_path):\n",
    "        os.makedirs(full_path)\n",
    "        print(f\"Created: {full_path}\")\n",
    "    else:\n",
    "        print(f\"Already exists: {full_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:03:13.407795Z",
     "iopub.status.busy": "2025-05-12T16:03:13.407597Z",
     "iopub.status.idle": "2025-05-12T16:05:13.832860Z",
     "shell.execute_reply": "2025-05-12T16:05:13.832174Z",
     "shell.execute_reply.started": "2025-05-12T16:03:13.407779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "list_src = [\n",
    "    '/kaggle/input/loveda-splits/Train/Train/Rural',\n",
    "    '/kaggle/input/loveda-splits/Train/Train/Urban',\n",
    "    '/kaggle/input/loveda-splits/Val/Val/Rural',\n",
    "    '/kaggle/input/loveda-splits/Val/Val/Urban'\n",
    "]\n",
    "\n",
    "list_dst = [\n",
    "    '/kaggle/working/data/loveda/train/Rural',\n",
    "    '/kaggle/working/data/loveda/train/Urban',\n",
    "    '/kaggle/working/data/loveda/val/Rural',\n",
    "    '/kaggle/working/data/loveda/val/Urban'\n",
    "]\n",
    "\n",
    "for src, dst in zip(list_src, list_dst):\n",
    "    if not os.path.exists(dst):\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        shutil.copytree(src, dst)\n",
    "        print(f\"Copied: {src} → {dst}\")\n",
    "    else:\n",
    "        print(f\"Skipped (already exists): {dst}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:16.286704Z",
     "iopub.status.busy": "2025-05-12T16:05:16.286323Z",
     "iopub.status.idle": "2025-05-12T16:05:16.353147Z",
     "shell.execute_reply": "2025-05-12T16:05:16.352605Z",
     "shell.execute_reply.started": "2025-05-12T16:05:16.286679Z"
    },
    "id": "x-_xpQCZcbRg",
    "outputId": "d91de478-c474-4944-ca0e-0b3a8de56f7b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "shutil.copytree('/kaggle/input/configs/configs', '/kaggle/working/configs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:16.461558Z",
     "iopub.status.busy": "2025-05-12T16:05:16.461247Z",
     "iopub.status.idle": "2025-05-12T16:05:16.862777Z",
     "shell.execute_reply": "2025-05-12T16:05:16.862159Z",
     "shell.execute_reply.started": "2025-05-12T16:05:16.461530Z"
    },
    "id": "PDsG8UU9gU8e",
    "outputId": "5874c528-5f6c-4525-db46-52b965768d00",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "shutil.copy('/kaggle/input/pidnet-pretrained/PIDNet_S_ImageNet.pth.tar', '/kaggle/working/pretrained_models/imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:16.864113Z",
     "iopub.status.busy": "2025-05-12T16:05:16.863869Z",
     "iopub.status.idle": "2025-05-12T16:05:16.868736Z",
     "shell.execute_reply": "2025-05-12T16:05:16.868145Z",
     "shell.execute_reply.started": "2025-05-12T16:05:16.864093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd '/kaggle/working'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:16.869593Z",
     "iopub.status.busy": "2025-05-12T16:05:16.869386Z",
     "iopub.status.idle": "2025-05-12T16:05:16.908361Z",
     "shell.execute_reply": "2025-05-12T16:05:16.907907Z",
     "shell.execute_reply.started": "2025-05-12T16:05:16.869578Z"
    },
    "id": "WDp6olwIbTw9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_lst_file(image_dir, label_dir, output_lst):\n",
    "    # List and sort files numerically\n",
    "    images = sorted(os.listdir(image_dir), key=lambda x: int(os.path.splitext(x)[0]))\n",
    "    labels = sorted(os.listdir(label_dir), key=lambda x: int(os.path.splitext(x)[0]))\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_lst), exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "    with open(output_lst, 'w') as f:\n",
    "        for img, lbl in zip(images, labels):\n",
    "            # Generate full paths and normalize to use forward slashes\n",
    "            img_path = os.path.join(image_dir, img).replace(\"\\\\\", \"/\")\n",
    "            lbl_path = os.path.join(label_dir, lbl).replace(\"\\\\\", \"/\")\n",
    "            # Write formatted line with consistent spacing\n",
    "            f.write(f\"{img_path} {lbl_path}\\n\")\n",
    "\n",
    "# Paths to the LoveDA dataset directories\n",
    "urban_train_image_dir = \"data/loveda/train/Urban/images_png\"\n",
    "urban_train_label_dir = \"data/loveda/train/Urban/masks_png\"\n",
    "\n",
    "urban_test_image_dir = \"data/loveda/val/Urban/images_png\"\n",
    "urban_test_label_dir = \"data/loveda/val/Urban/masks_png\"\n",
    "\n",
    "rural_train_image_dir = \"data/loveda/train/Rural/images_png\"\n",
    "rural_train_label_dir = \"data/loveda/train/Rural/masks_png\"\n",
    "\n",
    "rural_test_image_dir = \"data/loveda/val/Rural/images_png\"\n",
    "rural_test_label_dir = \"data/loveda/val/Rural/masks_png\"\n",
    "\n",
    "\n",
    "\n",
    "# train on urban - test on urban\n",
    "train_lst_path = \"data/list/loveda/urban_urban/train.lst\"\n",
    "test_lst_path = \"data/list/loveda/urban_urban/val.lst\"\n",
    "\n",
    "# Create .lst files\n",
    "create_lst_file(urban_train_image_dir, urban_train_label_dir, train_lst_path)\n",
    "create_lst_file(urban_test_image_dir, urban_test_label_dir, test_lst_path)\n",
    "\n",
    "train_lst_path = \"data/list/loveda/urban_rural/train.lst\"\n",
    "target_lst_path = \"data/list/loveda/rural/train.lst\"\n",
    "test_lst_path = \"data/list/loveda/urban_rural/val.lst\"\n",
    "\n",
    "# Create .lst files\n",
    "create_lst_file(urban_train_image_dir, urban_train_label_dir, train_lst_path)\n",
    "create_lst_file(rural_train_image_dir, rural_train_label_dir, target_lst_path)\n",
    "create_lst_file(rural_test_image_dir, rural_test_label_dir, test_lst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLab implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:16.974044Z",
     "iopub.status.busy": "2025-05-12T16:05:16.973843Z",
     "iopub.status.idle": "2025-05-12T16:05:19.655646Z",
     "shell.execute_reply": "2025-05-12T16:05:19.655081Z",
     "shell.execute_reply.started": "2025-05-12T16:05:16.974029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet101\n",
    "affine_par = True\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes, affine=affine_par)\n",
    "        for i in self.bn1.parameters():\n",
    "            i.requires_grad = False\n",
    "        padding = dilation\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n",
    "                               padding=padding, bias=False, dilation=dilation)\n",
    "        self.bn2 = nn.BatchNorm2d(planes, affine=affine_par)\n",
    "        for i in self.bn2.parameters():\n",
    "            i.requires_grad = False\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4, affine=affine_par)\n",
    "        for i in self.bn3.parameters():\n",
    "            i.requires_grad = False\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ClassifierModule(nn.Module):\n",
    "    \"\"\" \n",
    "    This module implements the Atrous Spatial Pyramid Pooling (ASPP).\n",
    "    Parameters:\n",
    "        - inplanes: Number of input channels.\n",
    "        - dilation_series: List of dilation rates for the convolutions.\n",
    "        - padding_series: List of padding values corresponding to the dilation rates.\n",
    "        - num_classes: Number of output classes for semantic segmentation.\n",
    "    \"\"\" \n",
    "    def __init__(self, inplanes, dilation_series, padding_series, num_classes):\n",
    "        super(ClassifierModule, self).__init__()\n",
    "        self.conv2d_list = nn.ModuleList()\n",
    "        for dilation, padding in zip(dilation_series, padding_series):\n",
    "            self.conv2d_list.append(\n",
    "                nn.Conv2d(inplanes, num_classes, kernel_size=3, stride=1, padding=padding,\n",
    "                          dilation=dilation, bias=True))\n",
    "\n",
    "        for m in self.conv2d_list:\n",
    "            m.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv2d_list[0](x)\n",
    "        for i in range(len(self.conv2d_list) - 1):\n",
    "            out += self.conv2d_list[i + 1](x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetMulti(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the main DeepLabv2 model. It uses a ResNet backbone (with dilation) for feature extraction, \n",
    "    followed by a classification module (ASPP).\n",
    "\n",
    "    Layers: \n",
    "        - Convolutional Stem:\n",
    "            - conv1: Initial 7x7 convolution with stride 2, reducing spatial resolution.\n",
    "            - bn1: Batch normalization for the first layer.\n",
    "            - relu: ReLU activation.\n",
    "            - maxpool: Reduces resolution further with a 3x3 max-pooling layer.\n",
    "        - ResNet Layers:\n",
    "            - layer1 to layer4: Residual layers (ResNet blocks) with bottleneck structures.\n",
    "            - layer3 and layer4 use dilated convolutions for larger receptive fields, instead of downsampling.\n",
    "            - dilation=2 for layer3 and dilation=4 for layer4.\n",
    "        - ASPP:\n",
    "            - Implemented as layer6 (using ClassifierModule).\n",
    "            - Applies parallel atrous convolutions with dilation rates [6, 12, 18, 24] to the high-level feature map from layer4.\n",
    "\n",
    "    Forward Pass: the forward pass processes an input tensor through the following steps:\n",
    "        - Extract low-level features using conv1, bn1, relu, and maxpool.\n",
    "        - Pass through ResNet blocks (layer1 to layer4) to extract high-level features.\n",
    "        - Use ASPP (layer6) to capture multi-scale context.\n",
    "        - Upsample the final predictions back to the input resolution using bilinear interpolation.\n",
    "    Output\n",
    "        - During training: Returns segmentation predictions.\n",
    "        - During inference: Returns predictions resized to match the input size\n",
    "    \"\"\"\n",
    "    def __init__(self, block, layers, num_classes, multi_level=False):\n",
    "\n",
    "        \"\"\"\n",
    "        Layers: \n",
    "            - Convolutional Stem:\n",
    "                - conv1: Initial 7x7 convolution with stride 2, reducing spatial resolution.\n",
    "                - bn1: Batch normalization for the first layer.\n",
    "                - relu: ReLU activation.\n",
    "                - maxpool: Reduces resolution further with a 3x3 max-pooling layer.\n",
    "            - ResNet Layers:\n",
    "                - layer1 to layer4: Residual layers (ResNet blocks) with bottleneck structures.\n",
    "                - layer3 and layer4 use dilated convolutions for larger receptive fields, instead of downsampling.\n",
    "                - dilation=2 for layer3 and dilation=4 for layer4.\n",
    "            - ASPP:\n",
    "                - Implemented as layer6 (using ClassifierModule).\n",
    "                - Applies parallel atrous convolutions with dilation rates [6, 12, 18, 24] to the high-level feature map from layer4.\n",
    "        \"\"\"\n",
    "        self.multi_level = multi_level\n",
    "        self.inplanes = 64\n",
    "        super(ResNetMulti, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64, affine=affine_par)\n",
    "        for i in self.bn1.parameters():\n",
    "            i.requires_grad = False\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True)  # change\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n",
    "        self.layer6 = ClassifierModule(2048, [6, 12, 18, 24], [6, 12, 18, 24], num_classes)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n",
    "\n",
    "        \"\"\" \n",
    "        This method is responsible for creating a ResNet block.\n",
    "        Arguments:\n",
    "            - block:\n",
    "                - The class of the residual block to use (e.g., Bottleneck).\n",
    "                - A residual block consists of multiple convolutional layers with a skip connection.\n",
    "            - planes:\n",
    "                - The number of output channels for the block. The block will typically expand this to planes * expansion.\n",
    "                - For example, in Bottleneck class we have expansion = 4. Hence the number of output channels is planes * 4. \n",
    "            - blocks:\n",
    "                - The number of residual layers (or blocks) to include in this layer group.\n",
    "                - For example, in ResNet-101, layer3 has 23 blocks.\n",
    "            - stride:\n",
    "                - Controls the downsampling factor for the first block in the layer. A stride > 1 reduces the spatial resolution.\n",
    "            - dilation:\n",
    "                - Specifies the dilation rate for convolutions in the block.\n",
    "                - Used in layer3 and layer4 to replace downsampling with atrous convolution, maintaining spatial resolution while expanding the receptive field. \n",
    "        \"\"\"\n",
    "        downsample = None\n",
    "        if (stride != 1\n",
    "                or self.inplanes != planes * block.expansion\n",
    "                or dilation == 2\n",
    "                or dilation == 4):\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion, affine=affine_par))\n",
    "        for i in downsample._modules['1'].parameters():\n",
    "            i.requires_grad = False\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(self.inplanes, planes, stride, dilation=dilation, downsample=downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, dilation=dilation))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, H, W = x.size()\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer6(x)\n",
    "\n",
    "        x = torch.nn.functional.interpolate(x, size=(H, W), mode='bilinear')\n",
    "\n",
    "        if self.training == True:\n",
    "            return x\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_1x_lr_params_no_scale(self):\n",
    "        \"\"\"\n",
    "        This generator returns all the parameters of the net except for\n",
    "        the last classification layer. Note that for each batchnorm layer,\n",
    "        requires_grad is set to False in deeplab_resnet.py, therefore this function does not return\n",
    "        any batchnorm parameter\n",
    "        \"\"\"\n",
    "        b = []\n",
    "\n",
    "        b.append(self.conv1)\n",
    "        b.append(self.bn1)\n",
    "        b.append(self.layer1)\n",
    "        b.append(self.layer2)\n",
    "        b.append(self.layer3)\n",
    "        b.append(self.layer4)\n",
    "\n",
    "        for i in range(len(b)):\n",
    "            for j in b[i].modules():\n",
    "                jj = 0\n",
    "                for k in j.parameters():\n",
    "                    jj += 1\n",
    "                    if k.requires_grad:\n",
    "                        yield k\n",
    "\n",
    "    def get_10x_lr_params(self):\n",
    "        \"\"\"\n",
    "        This generator returns all the parameters for the last layer of the net,\n",
    "        which does the classification of pixel into classes\n",
    "        \"\"\"\n",
    "        b = []\n",
    "        if self.multi_level:\n",
    "            b.append(self.layer5.parameters())\n",
    "        b.append(self.layer6.parameters())\n",
    "\n",
    "        for j in range(len(b)):\n",
    "            for i in b[j]:\n",
    "                yield i\n",
    "\n",
    "    def optim_parameters(self, lr):\n",
    "        return [{'params': self.get_1x_lr_params_no_scale(), 'lr': lr},\n",
    "                {'params': self.get_10x_lr_params(), 'lr': 10 * lr}]\n",
    "\n",
    "\n",
    "def get_deeplab_v2(num_classes=19, pretrain=True, pretrain_model_path='DeepLab_resnet_pretrained_imagenet.pth'):\n",
    "    \"\"\" model = ResNetMulti(Bottleneck, [3, 4, 23, 3], num_classes)\n",
    "\n",
    "    # Pretraining loading\n",
    "    if pretrain:\n",
    "        resnet_model = resnet101(pretrained=True)\n",
    "        saved_state_dict = resnet_model.state_dict()\n",
    "\n",
    "        new_params = model.state_dict().copy()\n",
    "        for i in saved_state_dict:\n",
    "            i_parts = i.split('.')\n",
    "            new_params['.'.join(i_parts[1:])] = saved_state_dict[i]\n",
    "        model.load_state_dict(new_params, strict=False)\n",
    "\n",
    "    return model \"\"\"\n",
    "\n",
    "    model = ResNetMulti(Bottleneck, [3, 4, 23, 3], num_classes)\n",
    "\n",
    "    # Pretraining loading from torchvision's ResNet-101\n",
    "    if pretrain:\n",
    "        print('Loading ResNet-101 pretrained weights...')\n",
    "        resnet_model = resnet101(pretrained=True)\n",
    "        saved_state_dict = resnet_model.state_dict()\n",
    "\n",
    "        # Load pretrained weights into DeepLab model\n",
    "        new_params = model.state_dict().copy()\n",
    "        for key in saved_state_dict:\n",
    "            # Remove 'layer' prefixes or other modifications if necessary\n",
    "            modified_key = key.split('.', 1)[-1] if key.startswith('layer') else key\n",
    "            if modified_key in new_params:\n",
    "                new_params[modified_key] = saved_state_dict[key]\n",
    "\n",
    "        model.load_state_dict(new_params, strict=False)\n",
    "        print('Pretrained weights loaded successfully.')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:19.657079Z",
     "iopub.status.busy": "2025-05-12T16:05:19.656783Z",
     "iopub.status.idle": "2025-05-12T16:05:21.370463Z",
     "shell.execute_reply": "2025-05-12T16:05:21.369623Z",
     "shell.execute_reply.started": "2025-05-12T16:05:19.657061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "deeplab_model = get_deeplab_v2(num_classes=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LRO-dropYbt"
   },
   "source": [
    "_init_paths.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:21.371633Z",
     "iopub.status.busy": "2025-05-12T16:05:21.371275Z",
     "iopub.status.idle": "2025-05-12T16:05:21.375869Z",
     "shell.execute_reply": "2025-05-12T16:05:21.375147Z",
     "shell.execute_reply.started": "2025-05-12T16:05:21.371607Z"
    },
    "id": "HZRc88q3pV0d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Copyright (c) Microsoft\n",
    "# Licensed under the MIT License.\n",
    "# Written by Ke Sun (sunk@mail.ustc.edu.cn)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "\n",
    "def add_path(path):\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sj2x9u3_lEmk"
   },
   "source": [
    "model_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:21.377836Z",
     "iopub.status.busy": "2025-05-12T16:05:21.377639Z",
     "iopub.status.idle": "2025-05-12T16:05:21.416289Z",
     "shell.execute_reply": "2025-05-12T16:05:21.415488Z",
     "shell.execute_reply.started": "2025-05-12T16:05:21.377821Z"
    },
    "id": "RkXp_Vq6k_FR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "BatchNorm2d = nn.BatchNorm2d\n",
    "bn_mom = 0.1\n",
    "algc = False\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn1 = BatchNorm2d(planes, momentum=bn_mom)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = BatchNorm2d(planes, momentum=bn_mom)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.no_relu = no_relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "\n",
    "        if self.no_relu:\n",
    "            return out\n",
    "        else:\n",
    "            return self.relu(out)\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 2\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=True):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = BatchNorm2d(planes, momentum=bn_mom)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = BatchNorm2d(planes, momentum=bn_mom)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn3 = BatchNorm2d(planes * self.expansion, momentum=bn_mom)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.no_relu = no_relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        if self.no_relu:\n",
    "            return out\n",
    "        else:\n",
    "            return self.relu(out)\n",
    "\n",
    "class segmenthead(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, interplanes, outplanes, scale_factor=None):\n",
    "        super(segmenthead, self).__init__()\n",
    "        self.bn1 = BatchNorm2d(inplanes, momentum=bn_mom)\n",
    "        self.conv1 = nn.Conv2d(inplanes, interplanes, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = BatchNorm2d(interplanes, momentum=bn_mom)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(interplanes, outplanes, kernel_size=1, padding=0, bias=True)\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(self.relu(self.bn1(x)))\n",
    "        out = self.conv2(self.relu(self.bn2(x)))\n",
    "\n",
    "        if self.scale_factor is not None:\n",
    "            height = x.shape[-2] * self.scale_factor\n",
    "            width = x.shape[-1] * self.scale_factor\n",
    "            out = F.interpolate(out,\n",
    "                        size=[height, width],\n",
    "                        mode='bilinear', align_corners=algc)\n",
    "\n",
    "        return out\n",
    "\n",
    "class DAPPM(nn.Module):\n",
    "    def __init__(self, inplanes, branch_planes, outplanes, BatchNorm=nn.BatchNorm2d):\n",
    "        super(DAPPM, self).__init__()\n",
    "        bn_mom = 0.1\n",
    "        self.scale1 = nn.Sequential(nn.AvgPool2d(kernel_size=5, stride=2, padding=2),\n",
    "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
    "                                    )\n",
    "        self.scale2 = nn.Sequential(nn.AvgPool2d(kernel_size=9, stride=4, padding=4),\n",
    "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
    "                                    )\n",
    "        self.scale3 = nn.Sequential(nn.AvgPool2d(kernel_size=17, stride=8, padding=8),\n",
    "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
    "                                    )\n",
    "        self.scale4 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
    "                                    )\n",
    "        self.scale0 = nn.Sequential(\n",
    "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
    "                                    )\n",
    "        self.process1 = nn.Sequential(\n",
    "                                    BatchNorm(branch_planes, momentum=bn_mom),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n",
    "                                    )\n",
    "        self.process2 = nn.Sequential(\n",
    "                                    BatchNorm(branch_planes, momentum=bn_mom),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n",
    "                                    )\n",
    "        self.process3 = nn.Sequential(\n",
    "                                    BatchNorm(branch_planes, momentum=bn_mom),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n",
    "                                    )\n",
    "        self.process4 = nn.Sequential(\n",
    "                                    BatchNorm(branch_planes, momentum=bn_mom),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n",
    "                                    )\n",
    "        self.compression = nn.Sequential(\n",
    "                                    BatchNorm(branch_planes * 5, momentum=bn_mom),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(branch_planes * 5, outplanes, kernel_size=1, bias=False),\n",
    "                                    )\n",
    "        self.shortcut = nn.Sequential(\n",
    "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=False),\n",
    "                                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        width = x.shape[-1]\n",
    "        height = x.shape[-2]\n",
    "        x_list = []\n",
    "\n",
    "        x_list.append(self.scale0(x))\n",
    "        x_list.append(self.process1((F.interpolate(self.scale1(x),\n",
    "                        size=[height, width],\n",
    "                        mode='bilinear', align_corners=algc)+x_list[0])))\n",
    "        x_list.append((self.process2((F.interpolate(self.scale2(x),\n",
    "                        size=[height, width],\n",
    "                        mode='bilinear', align_corners=algc)+x_list[1]))))\n",
    "        x_list.append(self.process3((F.interpolate(self.scale3(x),\n",
    "                        size=[height, width],\n",
    "                        mode='bilinear', align_corners=algc)+x_list[2])))\n",
    "        x_list.append(self.process4((F.interpolate(self.scale4(x),\n",
    "                        size=[height, width],\n",
    "                        mode='bilinear', align_corners=algc)+x_list[3])))\n",
    "\n",
    "        out = self.compression(torch.cat(x_list, 1)) + self.shortcut(x)\n",
    "        return out\n",
    "\n",
    "class PAPPM(nn.Module):\n",
    "    def __init__(self, inplanes, branch_planes, outplanes, BatchNorm=nn.BatchNorm2d):\n",
    "        super(PAPPM, self).__init__()\n",
    "        bn_mom = 0.1\n",
    "        self.scale1 = nn.Sequential(nn.AvgPool2d(kernel_size=5, stride=2, padding=2),\n",
    "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
    "                                    )\n",
    "        self.scale2 = nn.Sequential(nn.AvgPool2d(kernel_size=9, stride=4, padding=4),\n",
    "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
    "                                    )\n",
    "        self.scale3 = nn.Sequential(nn.AvgPool2d(kernel_size=17, stride=8, padding=8),\n",
    "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
    "                                    )\n",
    "        self.scale4 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
    "                                    )\n",
    "\n",
    "        self.scale0 = nn.Sequential(\n",
    "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
    "                                    )\n",
    "\n",
    "        self.scale_process = nn.Sequential(\n",
    "                                    BatchNorm(branch_planes*4, momentum=bn_mom),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(branch_planes*4, branch_planes*4, kernel_size=3, padding=1, groups=4, bias=False),\n",
    "                                    )\n",
    "\n",
    "\n",
    "        self.compression = nn.Sequential(\n",
    "                                    BatchNorm(branch_planes * 5, momentum=bn_mom),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(branch_planes * 5, outplanes, kernel_size=1, bias=False),\n",
    "                                    )\n",
    "\n",
    "        self.shortcut = nn.Sequential(\n",
    "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=False),\n",
    "                                    )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        width = x.shape[-1]\n",
    "        height = x.shape[-2]\n",
    "        scale_list = []\n",
    "\n",
    "        x_ = self.scale0(x)\n",
    "        scale_list.append(F.interpolate(self.scale1(x), size=[height, width],\n",
    "                        mode='bilinear', align_corners=algc)+x_)\n",
    "        scale_list.append(F.interpolate(self.scale2(x), size=[height, width],\n",
    "                        mode='bilinear', align_corners=algc)+x_)\n",
    "        scale_list.append(F.interpolate(self.scale3(x), size=[height, width],\n",
    "                        mode='bilinear', align_corners=algc)+x_)\n",
    "        scale_list.append(F.interpolate(self.scale4(x), size=[height, width],\n",
    "                        mode='bilinear', align_corners=algc)+x_)\n",
    "\n",
    "        scale_out = self.scale_process(torch.cat(scale_list, 1))\n",
    "\n",
    "        out = self.compression(torch.cat([x_,scale_out], 1)) + self.shortcut(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PagFM(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, after_relu=False, with_channel=False, BatchNorm=nn.BatchNorm2d):\n",
    "        super(PagFM, self).__init__()\n",
    "        self.with_channel = with_channel\n",
    "        self.after_relu = after_relu\n",
    "        self.f_x = nn.Sequential(\n",
    "                                nn.Conv2d(in_channels, mid_channels,\n",
    "                                          kernel_size=1, bias=False),\n",
    "                                BatchNorm(mid_channels)\n",
    "                                )\n",
    "        self.f_y = nn.Sequential(\n",
    "                                nn.Conv2d(in_channels, mid_channels,\n",
    "                                          kernel_size=1, bias=False),\n",
    "                                BatchNorm(mid_channels)\n",
    "                                )\n",
    "        if with_channel:\n",
    "            self.up = nn.Sequential(\n",
    "                                    nn.Conv2d(mid_channels, in_channels,\n",
    "                                              kernel_size=1, bias=False),\n",
    "                                    BatchNorm(in_channels)\n",
    "                                   )\n",
    "        if after_relu:\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        input_size = x.size()\n",
    "        if self.after_relu:\n",
    "            y = self.relu(y)\n",
    "            x = self.relu(x)\n",
    "\n",
    "        y_q = self.f_y(y)\n",
    "        y_q = F.interpolate(y_q, size=[input_size[2], input_size[3]],\n",
    "                            mode='bilinear', align_corners=False)\n",
    "        x_k = self.f_x(x)\n",
    "\n",
    "        if self.with_channel:\n",
    "            sim_map = torch.sigmoid(self.up(x_k * y_q))\n",
    "        else:\n",
    "            sim_map = torch.sigmoid(torch.sum(x_k * y_q, dim=1).unsqueeze(1))\n",
    "\n",
    "        y = F.interpolate(y, size=[input_size[2], input_size[3]],\n",
    "                            mode='bilinear', align_corners=False)\n",
    "        x = (1-sim_map)*x + sim_map*y\n",
    "\n",
    "        return x\n",
    "\n",
    "class Light_Bag(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n",
    "        super(Light_Bag, self).__init__()\n",
    "        self.conv_p = nn.Sequential(\n",
    "                                nn.Conv2d(in_channels, out_channels,\n",
    "                                          kernel_size=1, bias=False),\n",
    "                                BatchNorm(out_channels)\n",
    "                                )\n",
    "        self.conv_i = nn.Sequential(\n",
    "                                nn.Conv2d(in_channels, out_channels,\n",
    "                                          kernel_size=1, bias=False),\n",
    "                                BatchNorm(out_channels)\n",
    "                                )\n",
    "\n",
    "    def forward(self, p, i, d):\n",
    "        edge_att = torch.sigmoid(d)\n",
    "\n",
    "        p_add = self.conv_p((1-edge_att)*i + p)\n",
    "        i_add = self.conv_i(i + edge_att*p)\n",
    "\n",
    "        return p_add + i_add\n",
    "\n",
    "\n",
    "class DDFMv2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n",
    "        super(DDFMv2, self).__init__()\n",
    "        self.conv_p = nn.Sequential(\n",
    "                                BatchNorm(in_channels),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Conv2d(in_channels, out_channels,\n",
    "                                          kernel_size=1, bias=False),\n",
    "                                BatchNorm(out_channels)\n",
    "                                )\n",
    "        self.conv_i = nn.Sequential(\n",
    "                                BatchNorm(in_channels),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Conv2d(in_channels, out_channels,\n",
    "                                          kernel_size=1, bias=False),\n",
    "                                BatchNorm(out_channels)\n",
    "                                )\n",
    "\n",
    "    def forward(self, p, i, d):\n",
    "        edge_att = torch.sigmoid(d)\n",
    "\n",
    "        p_add = self.conv_p((1-edge_att)*i + p)\n",
    "        i_add = self.conv_i(i + edge_att*p)\n",
    "\n",
    "        return p_add + i_add\n",
    "\n",
    "class Bag(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n",
    "        super(Bag, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "                                BatchNorm(in_channels),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Conv2d(in_channels, out_channels,\n",
    "                                          kernel_size=3, padding=1, bias=False)\n",
    "                                )\n",
    "\n",
    "\n",
    "    def forward(self, p, i, d):\n",
    "        edge_att = torch.sigmoid(d)\n",
    "        return self.conv(edge_att*p + (1-edge_att)*i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nct1QBaLlIY-"
   },
   "source": [
    "pidnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:21.417356Z",
     "iopub.status.busy": "2025-05-12T16:05:21.417129Z",
     "iopub.status.idle": "2025-05-12T16:05:21.444589Z",
     "shell.execute_reply": "2025-05-12T16:05:21.443829Z",
     "shell.execute_reply.started": "2025-05-12T16:05:21.417341Z"
    },
    "id": "3jboQ25HlLp2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import logging\n",
    "\n",
    "BatchNorm2d = nn.BatchNorm2d\n",
    "bn_mom = 0.1\n",
    "algc = False\n",
    "\n",
    "class PIDNet(nn.Module):\n",
    "\n",
    "    def __init__(self, m=2, n=3, num_classes=19, planes=64, ppm_planes=96, head_planes=128, augment=True):\n",
    "        super(PIDNet, self).__init__()\n",
    "        self.augment = augment\n",
    "\n",
    "        # I Branch\n",
    "        self.conv1 =  nn.Sequential(\n",
    "                          nn.Conv2d(3,planes,kernel_size=3, stride=2, padding=1),\n",
    "                          BatchNorm2d(planes, momentum=bn_mom),\n",
    "                          nn.ReLU(inplace=True),\n",
    "                          nn.Conv2d(planes,planes,kernel_size=3, stride=2, padding=1),\n",
    "                          BatchNorm2d(planes, momentum=bn_mom),\n",
    "                          nn.ReLU(inplace=True),\n",
    "                      )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(BasicBlock, planes, planes, m)\n",
    "        self.layer2 = self._make_layer(BasicBlock, planes, planes * 2, m, stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock, planes * 2, planes * 4, n, stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlock, planes * 4, planes * 8, n, stride=2)\n",
    "        self.layer5 =  self._make_layer(Bottleneck, planes * 8, planes * 8, 2, stride=2)\n",
    "\n",
    "        # P Branch\n",
    "        self.compression3 = nn.Sequential(\n",
    "                                          nn.Conv2d(planes * 4, planes * 2, kernel_size=1, bias=False),\n",
    "                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n",
    "                                          )\n",
    "\n",
    "        self.compression4 = nn.Sequential(\n",
    "                                          nn.Conv2d(planes * 8, planes * 2, kernel_size=1, bias=False),\n",
    "                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n",
    "                                          )\n",
    "        self.pag3 = PagFM(planes * 2, planes)\n",
    "        self.pag4 = PagFM(planes * 2, planes)\n",
    "\n",
    "        self.layer3_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n",
    "        self.layer4_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n",
    "        self.layer5_ = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n",
    "\n",
    "        # D Branch\n",
    "        if m == 2:\n",
    "            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes)\n",
    "            self.layer4_d = self._make_layer(Bottleneck, planes, planes, 1)\n",
    "            self.diff3 = nn.Sequential(\n",
    "                                        nn.Conv2d(planes * 4, planes, kernel_size=3, padding=1, bias=False),\n",
    "                                        BatchNorm2d(planes, momentum=bn_mom),\n",
    "                                        )\n",
    "            self.diff4 = nn.Sequential(\n",
    "                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n",
    "                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n",
    "                                     )\n",
    "            self.spp = PAPPM(planes * 16, ppm_planes, planes * 4)\n",
    "            self.dfm = Light_Bag(planes * 4, planes * 4)\n",
    "        else:\n",
    "            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n",
    "            self.layer4_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n",
    "            self.diff3 = nn.Sequential(\n",
    "                                        nn.Conv2d(planes * 4, planes * 2, kernel_size=3, padding=1, bias=False),\n",
    "                                        BatchNorm2d(planes * 2, momentum=bn_mom),\n",
    "                                        )\n",
    "            self.diff4 = nn.Sequential(\n",
    "                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n",
    "                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n",
    "                                     )\n",
    "            self.spp = DAPPM(planes * 16, ppm_planes, planes * 4)\n",
    "            self.dfm = Bag(planes * 4, planes * 4)\n",
    "\n",
    "        self.layer5_d = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n",
    "\n",
    "        # Prediction Head\n",
    "        if self.augment:\n",
    "            self.seghead_p = segmenthead(planes * 2, head_planes, num_classes)\n",
    "            self.seghead_d = segmenthead(planes * 2, planes, 1)\n",
    "\n",
    "        self.final_layer = segmenthead(planes * 4, head_planes, num_classes)\n",
    "\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(inplanes, planes, stride, downsample))\n",
    "        inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            if i == (blocks-1):\n",
    "                layers.append(block(inplanes, planes, stride=1, no_relu=True))\n",
    "            else:\n",
    "                layers.append(block(inplanes, planes, stride=1, no_relu=False))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_single_layer(self, block, inplanes, planes, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n",
    "            )\n",
    "\n",
    "        layer = block(inplanes, planes, stride, downsample, no_relu=True)\n",
    "\n",
    "        return layer\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        width_output = x.shape[-1] // 8\n",
    "        height_output = x.shape[-2] // 8\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(self.layer2(self.relu(x)))\n",
    "        x_ = self.layer3_(x)\n",
    "        x_d = self.layer3_d(x)\n",
    "\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x_ = self.pag3(x_, self.compression3(x))\n",
    "        x_d = x_d + F.interpolate(\n",
    "                        self.diff3(x),\n",
    "                        size=[height_output, width_output],\n",
    "                        mode='bilinear', align_corners=algc)\n",
    "        if self.augment:\n",
    "            temp_p = x_\n",
    "\n",
    "        x = self.relu(self.layer4(x))\n",
    "        x_ = self.layer4_(self.relu(x_))\n",
    "        x_d = self.layer4_d(self.relu(x_d))\n",
    "\n",
    "        x_ = self.pag4(x_, self.compression4(x))\n",
    "        x_d = x_d + F.interpolate(\n",
    "                        self.diff4(x),\n",
    "                        size=[height_output, width_output],\n",
    "                        mode='bilinear', align_corners=algc)\n",
    "        if self.augment:\n",
    "            temp_d = x_d\n",
    "\n",
    "        x_ = self.layer5_(self.relu(x_))\n",
    "        x_d = self.layer5_d(self.relu(x_d))\n",
    "        x = F.interpolate(\n",
    "                        self.spp(self.layer5(x)),\n",
    "                        size=[height_output, width_output],\n",
    "                        mode='bilinear', align_corners=algc)\n",
    "\n",
    "        x_ = self.final_layer(self.dfm(x_, x, x_d))\n",
    "\n",
    "        if self.augment:\n",
    "            x_extra_p = self.seghead_p(temp_p)\n",
    "            x_extra_d = self.seghead_d(temp_d)\n",
    "            return [x_extra_p, x_, x_extra_d]\n",
    "        else:\n",
    "            return x_\n",
    "\n",
    "def get_seg_model(cfg, imgnet_pretrained):\n",
    "\n",
    "    if 's' in cfg.MODEL.NAME:\n",
    "        model = PIDNet(m=2, n=3, num_classes=cfg.DATASET.NUM_CLASSES, planes=32, ppm_planes=96, head_planes=128, augment=True)\n",
    "    elif 'm' in cfg.MODEL.NAME:\n",
    "        model = PIDNet(m=2, n=3, num_classes=cfg.DATASET.NUM_CLASSES, planes=64, ppm_planes=96, head_planes=128, augment=True)\n",
    "    else:\n",
    "        model = PIDNet(m=3, n=4, num_classes=cfg.DATASET.NUM_CLASSES, planes=64, ppm_planes=112, head_planes=256, augment=True)\n",
    "\n",
    "    if imgnet_pretrained:\n",
    "        pretrained_state = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')['state_dict']\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_state = {k: v for k, v in pretrained_state.items() if (k in model_dict and v.shape == model_dict[k].shape)}\n",
    "        model_dict.update(pretrained_state)\n",
    "        msg = 'Loaded {} parameters!'.format(len(pretrained_state))\n",
    "        logging.info('Attention!!!')\n",
    "        logging.info(msg)\n",
    "        logging.info('Over!!!')\n",
    "        model.load_state_dict(model_dict, strict = False)\n",
    "    else:\n",
    "        pretrained_dict = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')\n",
    "        if 'state_dict' in pretrained_dict:\n",
    "            pretrained_dict = pretrained_dict['state_dict']\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = {k[6:]: v for k, v in pretrained_dict.items() if (k[6:] in model_dict and v.shape == model_dict[k[6:]].shape)}\n",
    "        msg = 'Loaded {} parameters!'.format(len(pretrained_dict))\n",
    "        logging.info('Attention!!!')\n",
    "        logging.info(msg)\n",
    "        logging.info('Over!!!')\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict, strict = False)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_pred_model(name, num_classes):\n",
    "\n",
    "    if 's' in name:\n",
    "        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=32, ppm_planes=96, head_planes=128, augment=False)\n",
    "    elif 'm' in name:\n",
    "        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=64, ppm_planes=96, head_planes=128, augment=False)\n",
    "    else:\n",
    "        model = PIDNet(m=3, n=4, num_classes=num_classes, planes=64, ppm_planes=112, head_planes=256, augment=False)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TL86sUi4ngtL"
   },
   "source": [
    "base_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:21.445550Z",
     "iopub.status.busy": "2025-05-12T16:05:21.445313Z",
     "iopub.status.idle": "2025-05-12T16:05:22.544947Z",
     "shell.execute_reply": "2025-05-12T16:05:22.544223Z",
     "shell.execute_reply.started": "2025-05-12T16:05:21.445532Z"
    },
    "id": "yxhY6uIngb5e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import albumentations as A\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "y_k_size = 6\n",
    "x_k_size = 6\n",
    "\n",
    "def show_images(x_original, x_augmented, unnormalize = False):\n",
    "\n",
    "    if unnormalize:\n",
    "        # ImageNet mean and std\n",
    "        imagenet_mean = np.array([0.485, 0.456, 0.406])[:, None, None]\n",
    "        imagenet_std = np.array([0.229, 0.224, 0.225])[:, None, None]\n",
    "\n",
    "        # Denormalize using NumPy broadcasting\n",
    "        x_original = x_original * imagenet_std + imagenet_mean\n",
    "        x_augmented = x_augmented * imagenet_std + imagenet_mean\n",
    "\n",
    "        # Clip to [0, 1] in case of overflows\n",
    "        x_original = np.clip(x_original, 0, 1)\n",
    "        x_augmented = np.clip(x_augmented, 0, 1)\n",
    "\n",
    "        # Transpose to HWC for matplotlib\n",
    "        x_original = np.transpose(x_original, (1, 2, 0))\n",
    "        x_augmented = np.transpose(x_augmented, (1, 2, 0))\n",
    "\n",
    "    # Plot\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axs[0].imshow(x_original)\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(x_augmented)\n",
    "    axs[1].set_title(\"Augmented Image\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "class BaseDataset(data.Dataset):\n",
    "    def __init__(self,\n",
    "                 ignore_label=255,\n",
    "                 base_size=2048,\n",
    "                 crop_size=(512, 512),\n",
    "                 scale_factor=16,\n",
    "                 mean=[0.485, 0.456, 0.406],\n",
    "                 std=[0.229, 0.224, 0.225]):\n",
    "\n",
    "        self.base_size = base_size\n",
    "        self.crop_size = crop_size\n",
    "        self.ignore_label = ignore_label\n",
    "\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "        self.files = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def input_transform(self, image, city=False):\n",
    "        if city:\n",
    "            image = image.astype(np.float32)[:, :, ::-1]\n",
    "        else:\n",
    "            image = image.astype(np.float32)\n",
    "        image = image / 255.0\n",
    "        image -= self.mean\n",
    "        image /= self.std\n",
    "        return image\n",
    "\n",
    "    def label_transform(self, label):\n",
    "        return np.array(label).astype(np.uint8)\n",
    "\n",
    "    def pad_image(self, image, h, w, size, padvalue):\n",
    "        pad_h = max(size[0] - h, 0)\n",
    "        pad_w = max(size[1] - w, 0)\n",
    "\n",
    "        # Se non è necessario il padding, restituisci l'immagine originale\n",
    "        if pad_h == 0 and pad_w == 0:\n",
    "            return image\n",
    "\n",
    "        # Verifica il formato dell'immagine (deve essere H, W, C)\n",
    "        if len(image.shape) == 3 and image.shape[0] <= 3:  # Se è in formato (C, H, W)\n",
    "            image = np.transpose(image, (1, 2, 0))  # Converti in (H, W, C)\n",
    "\n",
    "        # Aggiungi il padding\n",
    "        pad_image = cv2.copyMakeBorder(image, 0, pad_h, 0, pad_w, cv2.BORDER_CONSTANT, value=padvalue)\n",
    "\n",
    "        # Ripristina il formato originale (C, H, W) se necessario\n",
    "        if len(image.shape) == 3 and image.shape[2] <= 3:  # Se era in formato (C, H, W)\n",
    "            pad_image = np.transpose(pad_image, (2, 0, 1))  # Converti di nuovo in (C, H, W)\n",
    "\n",
    "        return pad_image\n",
    "\n",
    "    def rand_crop(self, image, label, edge):\n",
    "        # Verifica il formato dell'immagine\n",
    "        if len(image.shape) == 3 and image.shape[0] <= 3:  # Se è in formato (C, H, W)\n",
    "            image = np.transpose(image, (1, 2, 0))  # Converti in (H, W, C)\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "\n",
    "        # Aggiungi padding se necessario\n",
    "        if h < self.crop_size[0] or w < self.crop_size[1]:\n",
    "            image = self.pad_image(image, h, w, self.crop_size, (0.0, 0.0, 0.0))\n",
    "            label = self.pad_image(label, h, w, self.crop_size, (self.ignore_label,))\n",
    "            edge = self.pad_image(edge, h, w, self.crop_size, (0.0,))\n",
    "\n",
    "        # Aggiorna le dimensioni dopo il padding\n",
    "        new_h, new_w = label.shape\n",
    "        if new_h < self.crop_size[0] or new_w < self.crop_size[1]:\n",
    "            raise ValueError(f\"Dimensioni insufficienti per il ritaglio: label={label.shape}, crop_size={self.crop_size}\")\n",
    "\n",
    "        # Calcola le coordinate per il ritaglio casuale\n",
    "        x = random.randint(0, new_w - self.crop_size[1])\n",
    "        y = random.randint(0, new_h - self.crop_size[0])\n",
    "\n",
    "        # Esegui il ritaglio\n",
    "        image = image[y:y+self.crop_size[0], x:x+self.crop_size[1]]\n",
    "        label = label[y:y+self.crop_size[0], x:x+self.crop_size[1]]\n",
    "        edge = edge[y:y+self.crop_size[0], x:x+self.crop_size[1]]\n",
    "\n",
    "        #in questo modo l'iimagine è 512x512x3\n",
    "        #se volessi croppare quella regione\n",
    "        '''\n",
    "        # Estrai la regione da sfocare\n",
    "        cropped_region = image[y:y+crop_size[0], x:x+crop_size[1]]\n",
    "\n",
    "        # Applica il Gaussian Blur alla regione\n",
    "        blurred_region = cv2.GaussianBlur(cropped_region, (15, 15), 0)\n",
    "\n",
    "        # Sostituisci la regione originale con quella sfocata\n",
    "        augmented_image = image.copy()\n",
    "        augmented_image[y:y+crop_size[0], x:x+crop_size[1]] = blurred_region\n",
    "        '''\n",
    "\n",
    "        return image, label, edge\n",
    "\n",
    "    def multi_scale_aug(self, image, label=None, edge=None,\n",
    "                        rand_scale=1, rand_crop=True):\n",
    "        long_size = int(self.base_size * rand_scale + 0.5)\n",
    "        h, w = image.shape[:2]\n",
    "        if h > w:\n",
    "            new_h = long_size\n",
    "            new_w = int(w * long_size / h + 0.5)\n",
    "        else:\n",
    "            new_w = long_size\n",
    "            new_h = int(h * long_size / w + 0.5)\n",
    "\n",
    "        image = cv2.resize(image, (new_w, new_h),\n",
    "                           interpolation=cv2.INTER_LINEAR)\n",
    "        if label is not None:\n",
    "            label = cv2.resize(label, (new_w, new_h),\n",
    "                               interpolation=cv2.INTER_NEAREST)\n",
    "            if edge is not None:\n",
    "                edge = cv2.resize(edge, (new_w, new_h),\n",
    "                                   interpolation=cv2.INTER_NEAREST)\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "        if rand_crop:\n",
    "            image, label, edge = self.rand_crop(image, label, edge)\n",
    "\n",
    "        return image, label, edge\n",
    "\n",
    "\n",
    "    def gen_sample(self, image, label, edge_pad=True, edge_size=4, city=False, transform=None, show=False):\n",
    "\n",
    "\n",
    "        if transform is not None:\n",
    "            # Pass both image and mask\n",
    "            augmented = transform(image=image, mask=label)\n",
    "\n",
    "            if show:\n",
    "                show_images(image, augmented[\"image\"])\n",
    "\n",
    "            # Extract results\n",
    "            image = augmented['image']\n",
    "            label = augmented['mask']\n",
    "\n",
    "\n",
    "\n",
    "        #It' important keeping the edge generation after the data augmentation\n",
    "        edge = cv2.Canny(label, 0.1, 0.2)\n",
    "        kernel = np.ones((edge_size, edge_size), np.uint8)\n",
    "        if edge_pad:\n",
    "            edge = edge[y_k_size:-y_k_size, x_k_size:-x_k_size]\n",
    "            edge = np.pad(edge, ((y_k_size,y_k_size),(x_k_size,x_k_size)), mode='constant')\n",
    "        edge = (cv2.dilate(edge, kernel, iterations=1)>50)*1.0\n",
    "\n",
    "\n",
    "        #trasformazioni di input\n",
    "        image = self.input_transform(image, city=city) #Se city=True, converte l'immagine da RGB in BGR per opencv\n",
    "        label = self.label_transform(label) #converte la label in un array di interi\n",
    "        image = image.transpose((2, 0, 1)) #H,W,C -> C,H,W\n",
    "\n",
    "        return image, label, edge\n",
    "\n",
    "\n",
    "    def inference(self, config, model, image):\n",
    "        size = image.size()\n",
    "        pred = model(image)\n",
    "\n",
    "        if config.MODEL.NUM_OUTPUTS > 1:\n",
    "            pred = pred[config.TEST.OUTPUT_INDEX]\n",
    "\n",
    "\n",
    "        pred = F.interpolate(\n",
    "            input=pred, size=size[-2:],\n",
    "            mode='bilinear', align_corners=config.MODEL.ALIGN_CORNERS\n",
    "        )\n",
    "\n",
    "\n",
    "        return pred.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwgVWGZaqIdl"
   },
   "source": [
    "discriminator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:22.545976Z",
     "iopub.status.busy": "2025-05-12T16:05:22.545671Z",
     "iopub.status.idle": "2025-05-12T16:05:22.552618Z",
     "shell.execute_reply": "2025-05-12T16:05:22.551939Z",
     "shell.execute_reply.started": "2025-05-12T16:05:22.545958Z"
    },
    "id": "GI5TiVbdqLuY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------\n",
    "    #Discriminator based on DCGAN\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FCDiscriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, ndf = 64):\n",
    "        super(FCDiscriminator, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_classes, ndf, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=2, padding=1)\n",
    "        self.classifier = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        #self.up_sample = nn.Upsample(scale_factor=32, mode='bilinear')\n",
    "        #self.sigmoid = nn.Sigmoid() #non uso sigmoid per la stabilità numerica a discapito di avere i logits e non le probabilità\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.classifier(x)\n",
    "        #x = self.up_sample(x)\n",
    "        #x = self.sigmoid(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOPrWJGJndSn"
   },
   "source": [
    "loveda.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:22.553490Z",
     "iopub.status.busy": "2025-05-12T16:05:22.553295Z",
     "iopub.status.idle": "2025-05-12T16:05:22.580721Z",
     "shell.execute_reply": "2025-05-12T16:05:22.580136Z",
     "shell.execute_reply.started": "2025-05-12T16:05:22.553470Z"
    },
    "id": "FMHg7dVtgmKS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "from PIL import Image\n",
    "import torchvision.transforms as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compare_images(image, blurred_image):\n",
    "    # Compute the absolute difference between the images\n",
    "    # Ensure both images are in the same format (H, W, C)\n",
    "    blurred_image = blurred_image.transpose(1, 2, 0)  # Change (C, H, W) to (H, W, C)\n",
    "\n",
    "    # Compute the absolute difference between the images\n",
    "    diff = np.abs(image - blurred_image)\n",
    "\n",
    "    # Plot the images and their difference side by side\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    axs[0].imshow(image)  # Original image\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(blurred_image)  # Blurred image\n",
    "    axs[1].set_title(\"Blurred Image\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    axs[2].imshow(diff)  # Difference image\n",
    "    axs[2].set_title(\"Difference Image\")\n",
    "    axs[2].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return diff\n",
    "\n",
    "\n",
    "def show_images(x_original, x_augmented, unnormalize = False):\n",
    "\n",
    "    if unnormalize:\n",
    "        # ImageNet mean and std\n",
    "        imagenet_mean = np.array([0.485, 0.456, 0.406])[:, None, None]\n",
    "        imagenet_std = np.array([0.229, 0.224, 0.225])[:, None, None]\n",
    "\n",
    "        # Denormalize using NumPy broadcasting\n",
    "        x_original = x_original * imagenet_std + imagenet_mean\n",
    "        x_augmented = x_augmented * imagenet_std + imagenet_mean\n",
    "\n",
    "        # Clip to [0, 1] in case of overflows\n",
    "        x_original = np.clip(x_original, 0, 1)\n",
    "        x_augmented = np.clip(x_augmented, 0, 1)\n",
    "\n",
    "        # Transpose to HWC for matplotlib\n",
    "        x_original = np.transpose(x_original, (1, 2, 0))\n",
    "        x_augmented = np.transpose(x_augmented, (1, 2, 0))\n",
    "\n",
    "    # Plot\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axs[0].imshow(x_original)\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(x_augmented)\n",
    "    axs[1].set_title(\"Augmented Image\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#classe per fare la data augmentation\n",
    "class DataAugmentation:\n",
    "    def __init__(self, config, dataset_instance):\n",
    "        self.enable = config[\"ENABLE\"]\n",
    "        self.probability = config[\"PROBABILITY\"]\n",
    "        self.techniques = config[\"TECHNIQUES\"]\n",
    "        self.dataset = dataset_instance  # Riferimento all'istanza del dataset\n",
    "\n",
    "    def apply(self, image, label, edge):\n",
    "\n",
    "        if not self.enable or random.random() > self.probability: #50% di probabilità di applicare la data augmentation\n",
    "            return image,label,edge #non faccio augmentation\n",
    "\n",
    "        if self.techniques.get(\"HORIZONTAL_FLIP\", False):\n",
    "            image,label,edge = self.horizontal_flip(image, label, edge)\n",
    "\n",
    "        if self.techniques.get(\"GAUSSIAN_BLUR\", False):\n",
    "            image, label, edge = self.gaussian_blur(image, label, edge)\n",
    "\n",
    "        if self.techniques.get(\"MULTIPLY\", False):\n",
    "            image, label, edge = self.multiply(image, label, edge)\n",
    "\n",
    "        if self.techniques.get(\"RANDOM_BRIGHTNESS\", False):\n",
    "            image, label, edge = self.random_brightness(image, label, edge)\n",
    "\n",
    "        if self.techniques.get(\"RANDOM_CROP\", False):\n",
    "            image, label, edge = self.random_crop(image, label, edge)\n",
    "\n",
    "\n",
    "        return image, label, edge\n",
    "\n",
    "\n",
    "\n",
    "    def random_crop(self, image, label, edge):\n",
    "        return self.dataset.rand_crop(image, label, edge)  # Usa l'istanza del dataset\n",
    "\n",
    "\n",
    "\n",
    "    def horizontal_flip(self, image, label, edge):\n",
    "        # Inverti orizzontalmente immagine, label ed edge\n",
    "        flipped_image = image[:, :, ::-1]\n",
    "        flipped_label = label[:, ::-1]\n",
    "        flipped_edge = edge[:, ::-1]\n",
    "        return flipped_image, flipped_label, flipped_edge\n",
    "\n",
    "\n",
    "\n",
    "    def gaussian_blur(self, image, label, edge, kernel_size=5, show = False):\n",
    "        # Applica il Gaussian Blur solo all'immagine\n",
    "        transposed_image = image.transpose(1, 2, 0)  # From (C, H, W) to (H, W, C)\n",
    "\n",
    "        # Apply Gaussian blur\n",
    "        blurred_image = cv2.GaussianBlur(transposed_image, (kernel_size, kernel_size), 0)\n",
    "\n",
    "        # If you want to return it to the PyTorch format (C, H, W)\n",
    "        blurred_image = blurred_image.transpose(2, 0, 1)  # From (H, W, C) to (C, H, W)\n",
    "\n",
    "        if show:\n",
    "            show_images(image, blurred_image)\n",
    "\n",
    "        return blurred_image, label, edge\n",
    "\n",
    "\n",
    "\n",
    "    def multiply(self, image, label, edge, factor_range=(0.8, 1.2), show = False):\n",
    "        # Convert image to float32 to avoid overflow issues\n",
    "        factor = random.uniform(*factor_range)\n",
    "        image = image.astype(np.float32)  # Ensure safe multiplication\n",
    "\n",
    "        # Check if image is normalized (0-1), rescale before multiplication\n",
    "        if image.max() <= 1.0:\n",
    "            image *= 255.0  # Scale to 0-255 range before multiplication\n",
    "\n",
    "        multiplied_image = image * factor\n",
    "\n",
    "        if show:\n",
    "            show_images(image, multiplied_image)\n",
    "\n",
    "        return multiplied_image, label, edge\n",
    "\n",
    "    def random_brightness(self, image, label, edge, brightness_range=(-0.5, 0.5), show = False):\n",
    "        # Modify image brightness\n",
    "        brightness = np.float32(np.random.uniform(*brightness_range))\n",
    "        brightened_image = image + brightness  # Keep within [0,1]\n",
    "\n",
    "        if show:\n",
    "            show_images(image, brightened_image)\n",
    "        return brightened_image, label, edge\n",
    "\n",
    "\n",
    "\n",
    "class LoveDA(BaseDataset):\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 list_path,\n",
    "                 num_classes=7,\n",
    "                 multi_scale=False,\n",
    "                 flip=False,\n",
    "                 ignore_label=0,\n",
    "                 base_size=1024,\n",
    "                 crop_size=(512, 512),\n",
    "                 scale_factor=16, #multi scale usato come data augmentation alredy provided\n",
    "                 enable_augmentation=False,\n",
    "                 augmentation_probability=0.5,\n",
    "                 horizontal_flip=False,\n",
    "                 gaussian_blur=False,\n",
    "                 multiply=False,\n",
    "                 random_brightness=False,\n",
    "                 random_crop=True,\n",
    "                 mean=[0.485, 0.456, 0.406],\n",
    "                 std=[0.229, 0.224, 0.225],\n",
    "                 bd_dilate_size=4,\n",
    "                 pseudo_label=False,\n",
    "                 weighted=True,\n",
    "                 transform=None):\n",
    "\n",
    "        # estende il base_dataset\n",
    "        super(LoveDA, self).__init__(ignore_label, base_size,\n",
    "                                     crop_size, scale_factor, mean, std)\n",
    "\n",
    "        self.root = root\n",
    "        self.list_path = list_path\n",
    "        self.num_classes = num_classes\n",
    "        self.multi_scale = multi_scale\n",
    "        self.flip = flip\n",
    "        self.ignore_label = ignore_label\n",
    "        self.base_size = base_size\n",
    "        self.crop_size = crop_size\n",
    "        self.scale_factor = scale_factor\n",
    "        self.enable_augmentation = enable_augmentation\n",
    "        self.augmentation_probability = augmentation_probability\n",
    "        self.horizontal_flip = horizontal_flip\n",
    "        self.gaussian_blur = gaussian_blur\n",
    "        self.multiply = multiply\n",
    "        self.random_brightness = random_brightness\n",
    "        self.random_crop = random_crop\n",
    "        self.bd_dilate_size = bd_dilate_size\n",
    "\n",
    "        self.img_list = [line.strip().split() for line in open(root + list_path)]\n",
    "        self.files = self.read_files()\n",
    "        self.color_list = [[0, 0, 0], [1, 1, 1], [2, 2, 2],\n",
    "                            [3, 3, 3], [4, 4, 4], [5, 5, 5], [6, 6, 6], [7, 7, 7]]\n",
    "        self.class_weights = None\n",
    "        if weighted:\n",
    "            self.class_weights = torch.tensor([0.000000, 0.116411, 0.266041, 0.607794, 1.511413, 0.745507, 0.712438, 3.040396])\n",
    "        \n",
    "        self.pseudo_label = pseudo_label\n",
    "        self.transform=transform\n",
    "\n",
    "    def read_files(self):\n",
    "        files = []\n",
    "\n",
    "        for item in self.img_list:\n",
    "            image_path, label_path = item\n",
    "            name = os.path.splitext(os.path.basename(label_path))[0]\n",
    "            files.append({\n",
    "                \"img\": image_path,\n",
    "                \"label\": label_path,\n",
    "                \"name\": name\n",
    "            })\n",
    "\n",
    "        return files\n",
    "\n",
    "    # da immagine a label\n",
    "    def color2label(self, color_map):\n",
    "        label = np.ones(color_map.shape[:2]) * self.ignore_label\n",
    "        for i, v in enumerate(self.color_list):\n",
    "            label[(color_map == v).sum(2) == 3] = i\n",
    "\n",
    "        return label.astype(np.uint8)\n",
    "\n",
    "    def convert_label(self, label, inverse=False):\n",
    "        temp = label.copy()\n",
    "        if inverse:\n",
    "            for v, k in self.label_mapping.items():\n",
    "                label[temp == k] = v\n",
    "        else:\n",
    "            for k, v in self.label_mapping.items():\n",
    "                label[temp == k] = v\n",
    "        return label\n",
    "\n",
    "    # da label a immagine\n",
    "    def label2color(self, label):\n",
    "        color_map = np.zeros(label.shape + (3,))\n",
    "        for i, v in enumerate(self.color_list):\n",
    "            color_map[label == i] = self.color_list[i]\n",
    "\n",
    "        return color_map.astype(np.uint8)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.files[index]\n",
    "        name = item[\"name\"]\n",
    "        image = cv2.imread(item[\"img\"], cv2.IMREAD_COLOR)\n",
    "\n",
    "        size = image.shape\n",
    "\n",
    "        label = cv2.imread(item[\"label\"], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "\n",
    "        #edge (H,W)\n",
    "        image, label, edge = self.gen_sample(image, label, edge_pad=False,\n",
    "                                             edge_size=self.bd_dilate_size, city=False, transform=self.transform, show=False) #image diventa (C,H,W)\n",
    "\n",
    "        return image.copy(), label.copy(), edge.copy(), np.array(size), name\n",
    "\n",
    "    def single_scale_inference(self, config, model, image):\n",
    "        pred = self.inference(config, model, image)\n",
    "        return pred\n",
    "\n",
    "    def save_pred(self, preds, sv_path, name):\n",
    "        preds = np.asarray(np.argmax(preds.cpu(), axis=1), dtype=np.uint8)\n",
    "        for i in range(preds.shape[0]):\n",
    "            pred = self.label2color(preds[i])\n",
    "            save_img = Image.fromarray(pred)\n",
    "            save_img.save(os.path.join(sv_path, name[i] + '.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWtVHwfinr5g"
   },
   "source": [
    "criterion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:22.581739Z",
     "iopub.status.busy": "2025-05-12T16:05:22.581529Z",
     "iopub.status.idle": "2025-05-12T16:05:22.602466Z",
     "shell.execute_reply": "2025-05-12T16:05:22.601650Z",
     "shell.execute_reply.started": "2025-05-12T16:05:22.581723Z"
    },
    "id": "h1yOyy_NhYqb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from configs import config\n",
    "\n",
    "\n",
    "class CrossEntropy(nn.Module):\n",
    "    def __init__(self, ignore_label=-1, weight=None):\n",
    "        super(CrossEntropy, self).__init__()\n",
    "        self.ignore_label = ignore_label\n",
    "        self.criterion = nn.CrossEntropyLoss(\n",
    "            weight=weight,\n",
    "            ignore_index=ignore_label\n",
    "        )\n",
    "\n",
    "    def _forward(self, score, target):\n",
    "\n",
    "        loss = self.criterion(score, target)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def forward(self, score, target):\n",
    "\n",
    "        if config.MODEL.NUM_OUTPUTS == 1:\n",
    "            score = [score]\n",
    "\n",
    "        balance_weights = config.LOSS.BALANCE_WEIGHTS\n",
    "        sb_weights = config.LOSS.SB_WEIGHTS\n",
    "        if len(balance_weights) == len(score):\n",
    "            return sum([w * self._forward(x, target) for (w, x) in zip(balance_weights, score)])\n",
    "        elif len(score) == 1:\n",
    "            return sb_weights * self._forward(score[0], target)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"lengths of prediction and target are not identical!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class OhemCrossEntropy(nn.Module):\n",
    "    def __init__(self, ignore_label=-1, thres=0.7,\n",
    "                 min_kept=100000, weight=None):\n",
    "        super(OhemCrossEntropy, self).__init__()\n",
    "        self.thresh = thres\n",
    "        self.min_kept = max(1, min_kept)\n",
    "        self.ignore_label = ignore_label\n",
    "        self.criterion = nn.CrossEntropyLoss(\n",
    "            weight=weight,\n",
    "            ignore_index=ignore_label,\n",
    "            reduction='none'\n",
    "        )\n",
    "\n",
    "    def _ce_forward(self, score, target):\n",
    "\n",
    "\n",
    "        loss = self.criterion(score, target)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _ohem_forward(self, score, target, **kwargs):\n",
    "\n",
    "        pred = F.softmax(score, dim=1)\n",
    "        pixel_losses = self.criterion(score, target).contiguous().view(-1)\n",
    "        mask = target.contiguous().view(-1) != self.ignore_label\n",
    "\n",
    "        tmp_target = target.clone()\n",
    "        tmp_target[tmp_target == self.ignore_label] = 0\n",
    "        pred = pred.gather(1, tmp_target.unsqueeze(1))\n",
    "        pred, ind = pred.contiguous().view(-1,)[mask].contiguous().sort()\n",
    "        min_value = pred[min(self.min_kept, pred.numel() - 1)]\n",
    "        threshold = max(min_value, self.thresh)\n",
    "\n",
    "        pixel_losses = pixel_losses[mask][ind]\n",
    "        pixel_losses = pixel_losses[pred < threshold]\n",
    "        return pixel_losses.mean()\n",
    "\n",
    "    def forward(self, score, target):\n",
    "\n",
    "        if not (isinstance(score, list) or isinstance(score, tuple)):\n",
    "            score = [score]\n",
    "\n",
    "        balance_weights = config.LOSS.BALANCE_WEIGHTS\n",
    "        sb_weights = config.LOSS.SB_WEIGHTS\n",
    "        if len(balance_weights) == len(score):\n",
    "            functions = [self._ce_forward] * \\\n",
    "                (len(balance_weights) - 1) + [self._ohem_forward]\n",
    "            return sum([\n",
    "                w * func(x, target)\n",
    "                for (w, x, func) in zip(balance_weights, score, functions)\n",
    "            ])\n",
    "\n",
    "        elif len(score) == 1:\n",
    "            return sb_weights * self._ohem_forward(score[0], target)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"lengths of prediction and target are not identical!\")\n",
    "\n",
    "\n",
    "def weighted_bce(bd_pre, target):\n",
    "    n, c, h, w = bd_pre.size()\n",
    "    log_p = bd_pre.permute(0,2,3,1).contiguous().view(1, -1)\n",
    "    target_t = target.view(1, -1)\n",
    "\n",
    "    pos_index = (target_t == 1)\n",
    "    neg_index = (target_t == 0)\n",
    "\n",
    "    weight = torch.zeros_like(log_p)\n",
    "    pos_num = pos_index.sum()\n",
    "    neg_num = neg_index.sum()\n",
    "    sum_num = pos_num + neg_num\n",
    "    weight[pos_index] = neg_num * 1.0 / sum_num\n",
    "    weight[neg_index] = pos_num * 1.0 / sum_num\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(log_p, target_t, weight, reduction='mean')\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "class BondaryLoss(nn.Module):\n",
    "    def __init__(self, coeff_bce = 20.0):\n",
    "        super(BondaryLoss, self).__init__()\n",
    "        self.coeff_bce = coeff_bce\n",
    "\n",
    "    def forward(self, bd_pre, bd_gt):\n",
    "\n",
    "        bce_loss = self.coeff_bce * weighted_bce(bd_pre, bd_gt)\n",
    "        loss = bce_loss\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGSIbNfzoHXo"
   },
   "source": [
    "utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:22.604628Z",
     "iopub.status.busy": "2025-05-12T16:05:22.604374Z",
     "iopub.status.idle": "2025-05-12T16:05:22.624416Z",
     "shell.execute_reply": "2025-05-12T16:05:22.623611Z",
     "shell.execute_reply.started": "2025-05-12T16:05:22.604611Z"
    },
    "id": "n8P5mZLNh-Ho",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from configs import default\n",
    "config = default._C.clone()\n",
    "update_config = default.update_config\n",
    "\n",
    "class FullModel(nn.Module):\n",
    "\n",
    "  def __init__(self, model, sem_loss, bd_loss):\n",
    "    super(FullModel, self).__init__()\n",
    "    self.model = model\n",
    "    self.sem_loss = sem_loss\n",
    "    self.bd_loss = bd_loss\n",
    "\n",
    "  def pixel_acc(self, pred, label):\n",
    "    _, preds = torch.max(pred, dim=1)\n",
    "    valid = (label >= 0).long()\n",
    "    acc_sum = torch.sum(valid * (preds == label).long())\n",
    "    pixel_sum = torch.sum(valid)\n",
    "    acc = acc_sum.float() / (pixel_sum.float() + 1e-10)\n",
    "    return acc\n",
    "\n",
    "  def forward(self, inputs, labels, bd_gt, *args, **kwargs):\n",
    "\n",
    "    outputs = self.model(inputs, *args, **kwargs)\n",
    "\n",
    "    h, w = labels.size(1), labels.size(2)\n",
    "    ph, pw = outputs[0].size(2), outputs[0].size(3)\n",
    "    if ph != h or pw != w:\n",
    "        for i in range(len(outputs)):\n",
    "            outputs[i] = F.interpolate(outputs[i], size=(\n",
    "                h, w), mode='bilinear', align_corners=config.MODEL.ALIGN_CORNERS)\n",
    "\n",
    "    acc  = self.pixel_acc(outputs[-2], labels)\n",
    "    loss_s = self.sem_loss(outputs[:-1], labels)\n",
    "    loss_b = self.bd_loss(outputs[-1], bd_gt)\n",
    "\n",
    "    filler = torch.ones_like(labels) * config.TRAIN.IGNORE_LABEL\n",
    "    try:\n",
    "        bd_label = torch.where(torch.sigmoid(outputs[-1][:, 0, :, :]) > 0.8, labels, filler) # 0.7\n",
    "        loss_sb = self.sem_loss([outputs[-2]], bd_label)\n",
    "    except:\n",
    "        print(\"Error in loss computation\")\n",
    "        loss_sb = self.sem_loss([outputs[-2]], labels)\n",
    "    loss = loss_s + loss_b + loss_sb\n",
    "\n",
    "    return torch.unsqueeze(loss,0), outputs[:-1], acc, [loss_s, loss_b] #aoutputs[:-1] è una lista di tensori\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.initialized = False\n",
    "        self.val = None\n",
    "        self.avg = None\n",
    "        self.sum = None\n",
    "        self.count = None\n",
    "\n",
    "    def initialize(self, val, weight):\n",
    "        self.val = val\n",
    "        self.avg = val\n",
    "        self.sum = val * weight\n",
    "        self.count = weight\n",
    "        self.initialized = True\n",
    "\n",
    "    def update(self, val, weight=1):\n",
    "        if not self.initialized:\n",
    "            self.initialize(val, weight)\n",
    "        else:\n",
    "            self.add(val, weight)\n",
    "\n",
    "    def add(self, val, weight):\n",
    "        self.val = val\n",
    "        self.sum += val * weight\n",
    "        self.count += weight\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def value(self):\n",
    "        return self.val\n",
    "\n",
    "    def average(self):\n",
    "        return self.avg\n",
    "\n",
    "def create_logger(cfg, cfg_name, phase='train'):\n",
    "    root_output_dir = Path(cfg.OUTPUT_DIR)\n",
    "\n",
    "    if (\n",
    "        not cfg.TRAIN.AUGMENTATION.TECHNIQUES.HORIZONTAL_FLIP and\n",
    "        not cfg.TRAIN.AUGMENTATION.TECHNIQUES.GAUSSIAN_BLUR and\n",
    "        not cfg.TRAIN.AUGMENTATION.TECHNIQUES.RANDOM_CROP and\n",
    "        not cfg.TRAIN.AUGMENTATION.TECHNIQUES.GAUSSIAN_NOISE and\n",
    "        not cfg.TRAIN.AUGMENTATION.TECHNIQUES.COLOR_JITTER\n",
    "        ):\n",
    "        folder_name = \"no_aug\"\n",
    "    else:\n",
    "        folder_name = \"aug\"\n",
    "\n",
    "    if cfg.TRAIN.DACS.ENABLE:\n",
    "        folder_name = \"dacs\"\n",
    "\n",
    "    if cfg.TRAIN.GAN.ENABLE:\n",
    "        folder_name = \"gan\"\n",
    "\n",
    "    if cfg.TRAIN.AUGMENTATION.ENABLE:\n",
    "        folder_name+= \"_hf\" if cfg.TRAIN.AUGMENTATION.TECHNIQUES.HORIZONTAL_FLIP else \"\"\n",
    "        folder_name+= \"_gb\" if cfg.TRAIN.AUGMENTATION.TECHNIQUES.GAUSSIAN_BLUR else \"\"\n",
    "        folder_name+= \"_rc\" if cfg.TRAIN.AUGMENTATION.TECHNIQUES.RANDOM_CROP else \"\"\n",
    "        folder_name+= \"_cj\" if cfg.TRAIN.AUGMENTATION.TECHNIQUES.COLOR_JITTER else \"\"\n",
    "        folder_name+= \"_gn\" if cfg.TRAIN.AUGMENTATION.TECHNIQUES.GAUSSIAN_NOISE else \"\"\n",
    "\n",
    "    # set up logger\n",
    "    if not root_output_dir.exists():\n",
    "        print('=> creating {}'.format(root_output_dir))\n",
    "        root_output_dir.mkdir()\n",
    "\n",
    "    dataset = cfg.DATASET.DATASET\n",
    "    model = cfg.MODEL.NAME\n",
    "    cfg_name = os.path.basename(cfg_name).split('.')[0]\n",
    "\n",
    "    final_output_dir = root_output_dir / dataset / cfg_name / folder_name\n",
    "\n",
    "    print('=> creating {}'.format(final_output_dir))\n",
    "    final_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    time_str = time.strftime('%Y-%m-%d-%H-%M')\n",
    "    log_file = '{}_{}_{}.log'.format(cfg_name, time_str, phase)\n",
    "    final_log_file = final_output_dir / log_file\n",
    "    head = '%(asctime)-15s %(message)s'\n",
    "    logging.basicConfig(filename=str(final_log_file),\n",
    "                        format=head)\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    console = logging.StreamHandler()\n",
    "    logging.getLogger('').addHandler(console)\n",
    "\n",
    "    tensorboard_log_dir = Path(cfg.LOG_DIR) / dataset / model / \\\n",
    "            (cfg_name + '_' + time_str)\n",
    "    print('=> creating {}'.format(tensorboard_log_dir))\n",
    "    tensorboard_log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return logger, str(final_output_dir), str(tensorboard_log_dir)\n",
    "\n",
    "def get_confusion_matrix(label, pred, size, num_class, ignore=-1):\n",
    "    \"\"\"\n",
    "    Calcute the confusion matrix by given label and pred\n",
    "    \"\"\"\n",
    "    output = pred.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    seg_pred = np.asarray(np.argmax(output, axis=3), dtype=np.uint8)\n",
    "    seg_gt = np.asarray(\n",
    "    label.cpu().numpy()[:, :size[-2], :size[-1]], dtype=int)\n",
    "\n",
    "    ignore_index = seg_gt != ignore\n",
    "    seg_gt = seg_gt[ignore_index]\n",
    "    seg_pred = seg_pred[ignore_index]\n",
    "\n",
    "    index = (seg_gt * num_class + seg_pred).astype('int32')\n",
    "    label_count = np.bincount(index)\n",
    "    confusion_matrix = np.zeros((num_class, num_class))\n",
    "\n",
    "    for i_label in range(num_class):\n",
    "        for i_pred in range(num_class):\n",
    "            cur_index = i_label * num_class + i_pred\n",
    "            if cur_index < len(label_count):\n",
    "                confusion_matrix[i_label,\n",
    "                                 i_pred] = label_count[cur_index]\n",
    "    return confusion_matrix\n",
    "\n",
    "def adjust_learning_rate(optimizer, base_lr, max_iters,\n",
    "        cur_iters, power=0.9, nbb_mult=10):\n",
    "    lr = base_lr*((1-float(cur_iters)/max_iters)**(power))\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    if len(optimizer.param_groups) == 2:\n",
    "        optimizer.param_groups[1]['lr'] = lr * nbb_mult\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOsR_t09oO3W"
   },
   "source": [
    "classmix.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:22.625556Z",
     "iopub.status.busy": "2025-05-12T16:05:22.625252Z",
     "iopub.status.idle": "2025-05-12T16:05:22.647157Z",
     "shell.execute_reply": "2025-05-12T16:05:22.646313Z",
     "shell.execute_reply.started": "2025-05-12T16:05:22.625514Z"
    },
    "id": "ZTvJD1AJiIdu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def show_mixed_visualization(x_s, y_s, x_t, y_t, x_mixed, y_mixed, bd_mixed, ignore_label=0):\n",
    "    \"\"\"\n",
    "    Displays source, target, mixed images and labels alongside the edge map, \n",
    "    with a legend whose colors exactly match the label images.\n",
    "    \n",
    "    Args:\n",
    "        x_s, x_t, x_mixed: Tensors [1,C,H,W] pre-normalized.\n",
    "        y_s, y_t, y_mixed: Tensors [1,H,W] integer labels.\n",
    "        bd_mixed: Tensor [1,H,W] edge/boundary map.\n",
    "        ignore_label: integer value to ignore in edge_map_vis.\n",
    "        class_names: dict mapping label int -> class name.\n",
    "    \"\"\"\n",
    "    class_names = {\n",
    "        0: \"no-data\",\n",
    "        1: \"background\",\n",
    "        2: \"building\",\n",
    "        3: \"road\",\n",
    "        4: \"water\",\n",
    "        5: \"barren\",\n",
    "        6: \"forest\",\n",
    "        7: \"agriculture\",\n",
    "    }\n",
    "    # Unpack first batch element and move to CPU / numpy\n",
    "    x_s = x_s[0].cpu().clone()\n",
    "    x_t = x_t[0].cpu().clone()\n",
    "    x_mixed = x_mixed[0].cpu().clone()\n",
    "    y_s = y_s[0].cpu().numpy()\n",
    "    y_t = y_t[0].cpu().numpy()\n",
    "    y_mixed = y_mixed[0].cpu().numpy()\n",
    "    edge_map = bd_mixed[0].cpu().numpy()\n",
    "\n",
    "    # Prepare edge‐map visualization\n",
    "    edge_vis = (edge_map != ignore_label).astype(np.uint8) * edge_map\n",
    "    edge_vis = (edge_vis > 0).astype(np.uint8) * 255\n",
    "\n",
    "    # Un-normalize images (assumes ImageNet mean/std)\n",
    "    imagenet_mean = [0.485, 0.456, 0.406]\n",
    "    imagenet_std  = [0.229, 0.224, 0.225]\n",
    "    for t, m, s in zip(x_s, imagenet_mean, imagenet_std):\n",
    "        t.mul_(s).add_(m)\n",
    "    for t, m, s in zip(x_t, imagenet_mean, imagenet_std):\n",
    "        t.mul_(s).add_(m)\n",
    "    for t, m, s in zip(x_mixed, imagenet_mean, imagenet_std):\n",
    "        t.mul_(s).add_(m)\n",
    "\n",
    "    # H×W×C for imshow\n",
    "    x_s = np.transpose(x_s.numpy(),     (1, 2, 0))\n",
    "    x_t = np.transpose(x_t.numpy(),     (1, 2, 0))\n",
    "    x_mixed = np.transpose(x_mixed.numpy(), (1, 2, 0))\n",
    "\n",
    "    # Create a discrete ListedColormap with exactly len(class_names) colors\n",
    "    num_classes = len(class_names)\n",
    "    cmap = plt.cm.get_cmap(\"tab20\", num_classes)\n",
    "\n",
    "    fig, axs = plt.subplots(2, 4, figsize=(16, 6))\n",
    "\n",
    "    # Row 1: images + edge map\n",
    "    axs[0, 0].imshow(x_s);      axs[0, 0].set_title(\"Source Image\");  axs[0, 0].axis(\"off\")\n",
    "    axs[0, 1].imshow(x_t);      axs[0, 1].set_title(\"Target Image\");  axs[0, 1].axis(\"off\")\n",
    "    axs[0, 2].imshow(x_mixed);  axs[0, 2].set_title(\"Mixed Image\");   axs[0, 2].axis(\"off\")\n",
    "    axs[0, 3].imshow(edge_vis, cmap='gray', vmin=0, vmax=255)\n",
    "    axs[0, 3].set_title(\"Edge Map\"); axs[0, 3].axis(\"off\")\n",
    "\n",
    "    # Row 2: labels using the same cmap with explicit vmin/vmax\n",
    "    axs[1, 0].imshow(y_s,      cmap=cmap, vmin=0, vmax=num_classes-1)\n",
    "    axs[1, 0].set_title(\"Source Label\");  axs[1, 0].axis(\"off\")\n",
    "    axs[1, 1].imshow(y_t,      cmap=cmap, vmin=0, vmax=num_classes-1)\n",
    "    axs[1, 1].set_title(\"Pseudo Label\");  axs[1, 1].axis(\"off\")\n",
    "    axs[1, 2].imshow(y_mixed,  cmap=cmap, vmin=0, vmax=num_classes-1)\n",
    "    axs[1, 2].set_title(\"Mixed Label\");   axs[1, 2].axis(\"off\")\n",
    "\n",
    "    # Legend: one patch per class, using the exact same cmap\n",
    "    handles = [\n",
    "        mpatches.Patch(color=cmap(i), label=class_names[i])\n",
    "        for i in sorted(class_names)\n",
    "    ]\n",
    "    axs[1, 3].legend(handles=handles, loc=\"center\", ncol=2, frameon=False, fontsize=\"small\")\n",
    "    axs[1, 3].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "def classmix(x1, y1, x2, y2, verbose=False, deterministic=False):\n",
    "    classes = y1.unique().tolist()\n",
    "    # Remove class 1 if it exists, because background occupies too much of the image\n",
    "    classes = [c for c in classes if c not in (0, 1)]\n",
    "    selected = random.sample(classes, len(classes) // 2)\n",
    "    if deterministic:\n",
    "        selected = classes[:len(classes) // 2]\n",
    "    mask = torch.zeros_like(y1, dtype=torch.bool)\n",
    "    if verbose:\n",
    "        print(f\"Classes from the source domain: {selected}\")\n",
    "    for c in selected:\n",
    "        mask |= (y1 == c) # c\n",
    "    mask = mask.unsqueeze(1)\n",
    "    x_mix = torch.where(mask, x1, x2)\n",
    "    y_mix = torch.where(mask.squeeze(1), y1, y2)\n",
    "    return x_mix, y_mix, mask.squeeze(1)\n",
    "\n",
    "\n",
    "\n",
    "def generate_confident_edge_map(y_mixed, source_mask, confidence_mask,\n",
    "                                edge_size=4, edge_pad=False, ignore_label=255):\n",
    "    \"\"\"\n",
    "    Build boundary map from y_mixed but only at:\n",
    "     - all source pixels (source_mask==1)\n",
    "     - target pixels with confidence_mask==True\n",
    "    Everything else is set to ignore_label.\n",
    "    \"\"\"\n",
    "    B, H, W = y_mixed.shape\n",
    "    edge_maps = []\n",
    "    kernel = np.ones((edge_size, edge_size), np.uint8)\n",
    "    pad = edge_size  # or whatever padding you want\n",
    "\n",
    "    for i in range(B):\n",
    "        lbl = y_mixed[i].cpu().numpy().astype(np.uint8)\n",
    "        src = source_mask[i].cpu().numpy().astype(bool)\n",
    "        conf = confidence_mask[i].cpu().numpy().astype(bool)\n",
    "\n",
    "        # only keep labels where we trust them\n",
    "        keep = src | conf\n",
    "        lbl_masked = np.where(keep, lbl, 0).astype(np.uint8)\n",
    "\n",
    "        # detect edges\n",
    "        edge = cv2.Canny(lbl_masked, 0.1, 0.2)\n",
    "        if edge_pad:\n",
    "            edge = edge[pad:-pad, pad:-pad]\n",
    "            edge = np.pad(edge, ((pad, pad), (pad, pad)), mode='constant')\n",
    "\n",
    "        # dilate + threshold\n",
    "        edge = (cv2.dilate(edge, kernel, iterations=1) > 50).astype(np.uint8)\n",
    "\n",
    "        # ignore everything outside our keep region\n",
    "        edge[~keep] = ignore_label\n",
    "\n",
    "        edge_maps.append(torch.from_numpy(edge).long().unsqueeze(0))\n",
    "\n",
    "    return torch.cat(edge_maps, dim=0).to(y_mixed.device)\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Train segmentation network')\n",
    "    parser.add_argument('--cfg', default=\"configs/loveda/pidnet_small_loveda.yaml\", type=str)\n",
    "    parser.add_argument('--seed', type=int, default=304)\n",
    "    parser.add_argument('opts', default=None, nargs=argparse.REMAINDER)\n",
    "    args = parser.parse_args()\n",
    "    update_config(config, args)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5a9CgnuKo-6G"
   },
   "source": [
    "function.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:22.648247Z",
     "iopub.status.busy": "2025-05-12T16:05:22.648005Z",
     "iopub.status.idle": "2025-05-12T16:05:22.701499Z",
     "shell.execute_reply": "2025-05-12T16:05:22.700591Z",
     "shell.execute_reply.started": "2025-05-12T16:05:22.648229Z"
    },
    "id": "wzdMHgT9htZC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the transform once (outside the training loop ideally)\n",
    "color_jitter = A.Compose([\n",
    "    A.ColorJitter(\n",
    "        p=0.5\n",
    "    ),\n",
    "])\n",
    "\n",
    "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "def apply_color_jitter_tensor(img_tensor):\n",
    "    # Move mean and std to same device as img_tensor\n",
    "    mean = IMAGENET_MEAN.to(img_tensor.device)\n",
    "    std = IMAGENET_STD.to(img_tensor.device)\n",
    "\n",
    "    # Denormalize\n",
    "    img_tensor = img_tensor.clone()\n",
    "    img_tensor = img_tensor * std + mean\n",
    "\n",
    "    img_np = img_tensor.cpu().numpy()  # (C, H, W)\n",
    "    img_np = np.transpose(img_np, (1, 2, 0))  # (H, W, C)\n",
    "    img_np = np.clip(img_np * 255.0, 0, 255).astype(np.uint8)\n",
    "\n",
    "    augmented = color_jitter(image=img_np)\n",
    "    img_aug = augmented['image'].astype(np.float32) / 255.0\n",
    "    img_aug = np.transpose(img_aug, (2, 0, 1))  # back to (C, H, W)\n",
    "\n",
    "    # Re-normalize (on CPU, then move back to GPU)\n",
    "    img_aug = torch.tensor(img_aug, dtype=torch.float)\n",
    "    img_aug = (img_aug - IMAGENET_MEAN) / IMAGENET_STD  # these are still on CPU\n",
    "    return img_aug.to(img_tensor.device)\n",
    "\n",
    "def update_ema_variables(ema_model, model, alpha_teacher, iteration):\n",
    "    # Use the \"true\" average until the exponential average is more correct\n",
    "    alpha_teacher = min(1 - 1 / (iteration + 1), alpha_teacher)\n",
    "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "        #ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n",
    "        ema_param.data[:] = alpha_teacher * ema_param[:].data[:] + (1 - alpha_teacher) * param[:].data[:]\n",
    "    return ema_model\n",
    "\n",
    "\n",
    "def train(config, epoch, num_epoch, epoch_iters, base_lr,\n",
    "          num_iters, trainloader, optimizer, model, writer_dict, deeplab_model = None, ema_model=None, targetloader=None):\n",
    "    # Training\n",
    "    \n",
    "    model.train()\n",
    "    show_dacs = False\n",
    "    batch_time = AverageMeter()\n",
    "    ave_loss = AverageMeter()\n",
    "    ave_acc  = AverageMeter()\n",
    "    avg_sem_loss = AverageMeter()\n",
    "    avg_bce_loss = AverageMeter()\n",
    "    tic = time.time()\n",
    "    cur_iters = epoch*epoch_iters\n",
    "    writer = writer_dict['writer']\n",
    "    global_steps = writer_dict['train_global_steps']\n",
    "    \n",
    "    if targetloader is not None:\n",
    "        target_iter = iter(targetloader)\n",
    "    lambda_weight = 0\n",
    "    for i_iter, batch in enumerate(trainloader, 0):\n",
    "        if config.TRAIN.DACS.ENABLE and epoch > -1:\n",
    "            # DACS\n",
    "\n",
    "            # === SOURCE BATCH ==\n",
    "            x_s, y_s, bd_s, _, _ = batch\n",
    "            \n",
    "            x_s, y_s, bd_s = x_s.cuda(), y_s.long().cuda(), bd_s.float().cuda()\n",
    "\n",
    "            # === TARGET BATCH ===\n",
    "            try:\n",
    "                x_t, y_t, _, _, _ = next(target_iter)\n",
    "            except StopIteration:\n",
    "                target_iter = iter(targetloader)\n",
    "                x_t, y_t, _, _, _ = next(target_iter)\n",
    "            x_t = x_t.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if deeplab_model is not None:\n",
    "                    x_t_resized = torch.nn.functional.interpolate(x_t, size=(224, 224), mode='bilinear', align_corners=config.MODEL.ALIGN_CORNERS)\n",
    "                    logits_t = deeplab_model(x_t_resized)\n",
    "                    \n",
    "                elif ema_model is not None:\n",
    "                    logits_t = ema_model.module.model(x_t)[-2]\n",
    "    \n",
    "                else:\n",
    "                    logits_t = model.module.model(x_t)[-2]\n",
    "                    \n",
    "                logits_t = torch.nn.functional.interpolate(\n",
    "                    logits_t, size=x_t.shape[2:], mode='bilinear', align_corners=config.MODEL.ALIGN_CORNERS\n",
    "                )\n",
    "                \n",
    "                pseudo_t = torch.argmax(logits_t, dim=1)\n",
    "                if deeplab_model is not None:\n",
    "                    pseudo_t += 1\n",
    "                conf_t = torch.softmax(logits_t, dim=1).max(dim=1)[0]\n",
    "\n",
    "\n",
    "            # === CONFIDENCE USAGE ==\n",
    "\n",
    "            # Confidence_mask tells the position of the pixel whose pseudo-labels have\n",
    "            # a confidence higher than  the threshold\n",
    "            confidence_mask = conf_t > config.TRAIN.DACS.THRESHOLD \n",
    "\n",
    "\n",
    "            \n",
    "            # Where confidence mask is True put the pixel from pseudo_t, otherwise insert the ignore label (0)\n",
    "            pseudo_t_filtered = torch.where(\n",
    "               confidence_mask,\n",
    "               pseudo_t,\n",
    "               torch.tensor(config.TRAIN.IGNORE_LABEL, device=pseudo_t.device)\n",
    "            )\n",
    "            \n",
    "            # Apply classmix\n",
    "            x_mixed, y_mixed, source_mask = classmix(x_s, y_s, x_t, pseudo_t_filtered)\n",
    "\n",
    "            \n",
    "            # Generate edges from the mixed image\n",
    "            bd_mixed = generate_confident_edge_map(\n",
    "                y_mixed,                       # mixed labels with ignore where conf low\n",
    "                source_mask,                   # source==1, target==0\n",
    "                confidence_mask,               # True where pseudo-label valid\n",
    "                edge_size=3, edge_pad=True,\n",
    "                ignore_label=config.TRAIN.IGNORE_LABEL\n",
    "            )\n",
    "\n",
    "            \n",
    "\n",
    "            # Source images augmentation\n",
    "            x_s.cpu()\n",
    "\n",
    "            # Apply color jitter to batch\n",
    "            x_s = torch.stack([\n",
    "                apply_color_jitter_tensor(img) for img in x_s\n",
    "            ])\n",
    "            \n",
    "            x_s.cuda()\n",
    "\n",
    "            # Mixed images augmentation\n",
    "            # Apply to batch\n",
    "            x_mixed = torch.stack([\n",
    "                apply_color_jitter_tensor(img) for img in x_mixed\n",
    "            ])\n",
    "            \n",
    "            if show_dacs:\n",
    "                show_mixed_visualization(x_s, y_s, x_t, pseudo_t_filtered, x_mixed, y_mixed, bd_mixed)\n",
    "            \n",
    "            x_mixed = x_mixed.cuda()\n",
    "            y_mixed = y_mixed.long().cuda()\n",
    "            bd_mixed = bd_mixed.float().cuda()\n",
    "\n",
    "\n",
    "            \n",
    "            # === FORWARD PASSES ===\n",
    "            losses_src, _, acc_src, loss_list_src = model(x_s, y_s, bd_s)\n",
    "            losses_mix, _, acc_mix, loss_list_mix = model(x_mixed, y_mixed, bd_mixed)\n",
    "            \n",
    "            loss_src = losses_src.mean()\n",
    "            loss_mix = losses_mix.mean()\n",
    "            \n",
    "            # === COMBINE LOSSES ===\n",
    "            # lambda_weight = confidence_mask.float().mean().item()\n",
    "            lambda_weight = (epoch+1) * 0.01\n",
    "            loss = loss_src + lambda_weight * loss_mix # .mean()\n",
    "            acc = (acc_src + acc_mix) / 2\n",
    "            \n",
    "            sem_loss = loss_list_src[0] + lambda_weight * loss_list_mix[0]\n",
    "            bce_loss = loss_list_src[1] + lambda_weight * loss_list_mix[1]\n",
    "        else:\n",
    "            images, labels, bd_gts, _, _ = batch\n",
    "            images = images.cuda()\n",
    "            labels = labels.long().cuda()\n",
    "            bd_gts = bd_gts.float().cuda()\n",
    "\n",
    "\n",
    "            losses, _, acc, loss_list = model(images, labels, bd_gts)\n",
    "            loss = losses.mean()\n",
    "            acc  = acc.mean()\n",
    "            sem_loss = loss_list[0]\n",
    "            bce_loss = loss_list[1]\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if config.TRAIN.DACS.ENABLE and ema_model is not None and epoch > -1:\n",
    "            alpha_teacher = 0.99\n",
    "            ema_model = update_ema_variables(ema_model = ema_model, model = model, alpha_teacher=alpha_teacher, iteration=i_iter+cur_iters)\n",
    "        \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - tic)\n",
    "        tic = time.time()\n",
    "\n",
    "        # update average loss\n",
    "        ave_loss.update(loss.item())\n",
    "        ave_acc.update(acc.item())\n",
    "        avg_sem_loss.update(sem_loss.mean().item())\n",
    "        avg_bce_loss.update(bce_loss.mean().item())\n",
    "\n",
    "        lr = adjust_learning_rate(optimizer,\n",
    "                                  base_lr,\n",
    "                                  num_iters,\n",
    "                                  i_iter+cur_iters)\n",
    "\n",
    "        if i_iter % config.PRINT_FREQ == 0:\n",
    "            msg = 'Epoch: [{}/{}] Iter:[{}/{}], Time: {:.2f}, ' \\\n",
    "                  'lr: {}, Loss: {:.6f}, Lambda: {:.2f}, Acc:{:.6f}, Semantic loss: {:.6f}, BCE loss: {:.6f}, SB loss: {:.6f}' .format(\n",
    "                      epoch, num_epoch, i_iter, epoch_iters,\n",
    "                      batch_time.average(), [x['lr'] for x in optimizer.param_groups], ave_loss.average(), lambda_weight,\n",
    "                      ave_acc.average(), avg_sem_loss.average(), avg_bce_loss.average(),ave_loss.average()-avg_sem_loss.average()-avg_bce_loss.average())\n",
    "            logging.info(msg)\n",
    "\n",
    "    writer.add_scalar('train_loss', ave_loss.average(), global_steps)\n",
    "    writer_dict['train_global_steps'] = global_steps + 1\n",
    "\n",
    "    # Ritorna la loss media per l'epoca\n",
    "    return ave_loss.average()\n",
    "\n",
    "def validate(config, testloader, model, writer_dict):\n",
    "    model.eval()\n",
    "    ave_loss = AverageMeter()\n",
    "    nums = config.MODEL.NUM_OUTPUTS\n",
    "    confusion_matrix = np.zeros(\n",
    "        (config.DATASET.NUM_CLASSES, config.DATASET.NUM_CLASSES, nums))\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testloader):\n",
    "            image, label, bd_gts, _, _ = batch\n",
    "            size = label.size()\n",
    "            image = image.cuda()\n",
    "            label = label.long().cuda()\n",
    "            bd_gts = bd_gts.float().cuda()\n",
    "\n",
    "            losses, pred, _, _ = model(image, label, bd_gts)\n",
    "            if not isinstance(pred, (list, tuple)):\n",
    "                pred = [pred]\n",
    "            for i, x in enumerate(pred):\n",
    "                x = F.interpolate(\n",
    "                    input=x, size=size[-2:],\n",
    "                    mode='bilinear', align_corners=config.MODEL.ALIGN_CORNERS\n",
    "                )\n",
    "\n",
    "                confusion_matrix[..., i] += get_confusion_matrix(\n",
    "                    label,\n",
    "                    x,\n",
    "                    size,\n",
    "                    config.DATASET.NUM_CLASSES,\n",
    "                    config.TRAIN.IGNORE_LABEL\n",
    "                )\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                print(idx)\n",
    "\n",
    "            loss = losses.mean()\n",
    "            ave_loss.update(loss.item())\n",
    "\n",
    "    for i in range(nums):\n",
    "        pos = confusion_matrix[..., i].sum(1)\n",
    "        res = confusion_matrix[..., i].sum(0)\n",
    "        tp = np.diag(confusion_matrix[..., i])\n",
    "        IoU_array = (tp / np.maximum(1.0, pos + res - tp))\n",
    "        mean_IoU = IoU_array.mean()\n",
    "\n",
    "        logging.info('{} {} {}'.format(i, IoU_array, mean_IoU))\n",
    "\n",
    "    writer = writer_dict['writer']\n",
    "    global_steps = writer_dict['valid_global_steps']\n",
    "    writer.add_scalar('valid_loss', ave_loss.average(), global_steps)\n",
    "    writer.add_scalar('valid_mIoU', mean_IoU, global_steps)\n",
    "    writer_dict['valid_global_steps'] = global_steps + 1\n",
    "    return ave_loss.average(), mean_IoU, IoU_array\n",
    "\n",
    "\n",
    "def testval(config, test_dataset, testloader, model,\n",
    "            sv_dir='./', sv_pred=False):\n",
    "    model.eval()\n",
    "    confusion_matrix = np.zeros((config.DATASET.NUM_CLASSES, config.DATASET.NUM_CLASSES))\n",
    "    with torch.no_grad():\n",
    "        for index, batch in enumerate(tqdm(testloader)):\n",
    "            image, label, _, _, name = batch\n",
    "            size = label.size()\n",
    "            pred = test_dataset.single_scale_inference(config, model, image.cuda())\n",
    "\n",
    "            if pred.size()[-2] != size[-2] or pred.size()[-1] != size[-1]:\n",
    "                pred = F.interpolate(\n",
    "                    pred, size[-2:],\n",
    "                    mode='bilinear', align_corners=config.MODEL.ALIGN_CORNERS\n",
    "                )\n",
    "\n",
    "            confusion_matrix += get_confusion_matrix(\n",
    "                label,\n",
    "                pred,\n",
    "                size,\n",
    "                config.DATASET.NUM_CLASSES,\n",
    "                config.TRAIN.IGNORE_LABEL)\n",
    "\n",
    "            if sv_pred:\n",
    "                sv_path = os.path.join(sv_dir, 'val_results')\n",
    "                if not os.path.exists(sv_path):\n",
    "                    os.mkdir(sv_path)\n",
    "                test_dataset.save_pred(pred, sv_path, name)\n",
    "\n",
    "            if index % 100 == 0:\n",
    "                logging.info('processing: %d images' % index)\n",
    "                pos = confusion_matrix.sum(1)\n",
    "                res = confusion_matrix.sum(0)\n",
    "                tp = np.diag(confusion_matrix)\n",
    "                IoU_array = (tp / np.maximum(1.0, pos + res - tp))\n",
    "                mean_IoU = IoU_array.mean()\n",
    "                logging.info('mIoU: %.4f' % (mean_IoU))\n",
    "\n",
    "    pos = confusion_matrix.sum(1)\n",
    "    res = confusion_matrix.sum(0)\n",
    "    tp = np.diag(confusion_matrix)\n",
    "    pixel_acc = tp.sum()/pos.sum()\n",
    "    mean_acc = (tp/np.maximum(1.0, pos)).mean()\n",
    "    IoU_array = (tp / np.maximum(1.0, pos + res - tp))\n",
    "    mean_IoU = IoU_array.mean()\n",
    "\n",
    "    return mean_IoU, IoU_array, pixel_acc, mean_acc\n",
    "\n",
    "\n",
    "def test(config, test_dataset, testloader, model,\n",
    "         sv_dir='./', sv_pred=True):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(tqdm(testloader)):\n",
    "            image, size, name = batch\n",
    "            size = size[0]\n",
    "            pred = test_dataset.single_scale_inference(\n",
    "                config,\n",
    "                model,\n",
    "                image.cuda())\n",
    "\n",
    "            if pred.size()[-2] != size[0] or pred.size()[-1] != size[1]:\n",
    "                pred = F.interpolate(\n",
    "                    pred, size[-2:],\n",
    "                    mode='bilinear', align_corners=config.MODEL.ALIGN_CORNERS\n",
    "                )\n",
    "\n",
    "            if sv_pred:\n",
    "                sv_path = os.path.join(sv_dir,'test_results')\n",
    "                if not os.path.exists(sv_path):\n",
    "                    os.mkdir(sv_path)\n",
    "                test_dataset.save_pred(pred, sv_path, name)\n",
    "\n",
    "def train_adv(config, epoch, num_epoch, epoch_iters, base_lr,\n",
    "          num_iters, trainloader, targetloader, optimizer_G, optimizer_D,\n",
    "          model, discriminator, writer_dict, lambda_adv=0.001, iter_size=4):\n",
    "\n",
    "    # Training mode\n",
    "    model.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    ave_loss = AverageMeter()\n",
    "    ave_acc = AverageMeter()\n",
    "    avg_sem_loss = AverageMeter()\n",
    "    avg_bce_loss = AverageMeter()\n",
    "\n",
    "    tic = time.time()\n",
    "    cur_iters = epoch * epoch_iters\n",
    "    writer = writer_dict['writer']\n",
    "    global_steps = writer_dict['train_global_steps']\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    for i_iter, (batch_source, batch_target) in enumerate(zip(trainloader, targetloader)):\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        cumulative_loss_G = 0.0\n",
    "        cumulative_loss_D = 0.0\n",
    "\n",
    "        for sub_iter in range(iter_size): #to make the batch size bigger\n",
    "\n",
    "            #Train G\n",
    "            # don't accumulate grads in D\n",
    "            for param in discriminator.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "            images_source, labels, bd_gts, _, _ = batch_source\n",
    "            images_target, _, _, _, _ = batch_target\n",
    "\n",
    "            images_source = images_source.to(device)\n",
    "            images_target = images_target.to(device)\n",
    "            labels = labels.long().to(device)\n",
    "            bd_gts = bd_gts.float().to(device)\n",
    "\n",
    "            # Checks\n",
    "            #print(f\"Labels dtype: {labels.dtype}, shape: {labels.shape}, unique values: {torch.unique(labels)}\")\n",
    "            #assert labels.dtype == torch.long, \"Labels devono essere di tipo torch.LongTensor\"\n",
    "            #assert labels.min() >= 0, f\"Labels contengono valori negativi: {labels.min()}\"\n",
    "            #assert labels.max() < 8, f\"Labels contengono valori >= n_classes: {labels.max()}\"\n",
    "\n",
    "\n",
    "\n",
    "            # 1. Forward seg net per dominio sorgente (supervisionato)\n",
    "            loss_seg1, output_source, _, _ = model(images_source, labels, bd_gts)\n",
    "            loss_seg1 = loss_seg1.squeeze().mean()\n",
    "            loss_seg1 = loss_seg1 / iter_size\n",
    "            cumulative_loss_G += loss_seg1.item()\n",
    "            loss_seg1.backward()\n",
    "\n",
    "            # Forward pass per il dominio target (adversarial)\n",
    "            output_target = model(images_target, labels, bd_gts)[1]\n",
    "            fake_preds = discriminator(F.softmax(output_target[-2], dim=1))\n",
    "            loss_adv = nn.BCEWithLogitsLoss()(fake_preds, torch.ones_like(fake_preds))\n",
    "            loss_adv = (loss_adv * lambda_adv) / iter_size  # Normalizza la loss\n",
    "            cumulative_loss_G += loss_adv.item()\n",
    "            loss_adv.backward()\n",
    "\n",
    "\n",
    "            output_source = [t.detach() for t in output_source]\n",
    "            output_target = [t.detach() for t in output_target]\n",
    "\n",
    "            for param in discriminator.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "            # Aggiorna il discriminatore\n",
    "            real_preds = discriminator(F.softmax(output_source[-2], dim=1)) #TODO: check -2\n",
    "            fake_preds = discriminator(F.softmax(output_target[-2], dim=1))\n",
    "            loss_D_real = nn.BCEWithLogitsLoss()(real_preds, torch.ones_like(real_preds))\n",
    "            loss_D_fake = nn.BCEWithLogitsLoss()(fake_preds, torch.zeros_like(fake_preds))\n",
    "            loss_D = (loss_D_real + loss_D_fake) / 2\n",
    "            loss_D = loss_D / iter_size  # Normalizza la loss\n",
    "            cumulative_loss_D += loss_D.item()\n",
    "            loss_D.backward()\n",
    "\n",
    "        # Aggiorna i pesi dopo tutte le sub-iterazioni\n",
    "        optimizer_G.step()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Log\n",
    "        batch_time.update(time.time() - tic)\n",
    "        tic = time.time()\n",
    "\n",
    "        ave_loss.update(cumulative_loss_G)\n",
    "        ave_acc.update(0)\n",
    "        avg_sem_loss.update(0)\n",
    "        avg_bce_loss.update(0)\n",
    "\n",
    "        lr = adjust_learning_rate(optimizer_G, base_lr, num_iters, i_iter + cur_iters)\n",
    "\n",
    "        if i_iter % config.PRINT_FREQ == 0:\n",
    "            msg = ('Epoch: [{}/{}] Iter:[{}/{}], Time: {:.2f}, lr: {}, '\n",
    "                   'Loss_G: {:.6f}, Loss_D: {:.6f}, Acc: {:.6f}, Semantic Loss: {:.6f}, BCE Loss: {:.6f}').format(\n",
    "                      epoch, num_epoch, i_iter, epoch_iters, batch_time.average(),\n",
    "                      [x['lr'] for x in optimizer_G.param_groups], ave_loss.average(), cumulative_loss_D,\n",
    "                      ave_acc.average(), avg_sem_loss.average(), avg_bce_loss.average()\n",
    "                  )\n",
    "            logging.info(msg)\n",
    "\n",
    "    writer.add_scalar('train_loss_G', ave_loss.average(), global_steps)\n",
    "    writer.add_scalar('train_loss_D', cumulative_loss_D, global_steps)\n",
    "    writer_dict['train_global_steps'] = global_steps + 1\n",
    "\n",
    "def train_adv_multi(config, epoch, num_epoch, epoch_iters, base_lr,\n",
    "                    num_iters, trainloader, targetloader, optimizer_G,\n",
    "                    optimizer_D1, optimizer_D2, model, discriminator1, discriminator2,\n",
    "                    writer_dict, lambda_adv1=0.001, lambda_adv2=0.0005, iter_size=4):\n",
    "\n",
    "    # Training mode\n",
    "    model.train()\n",
    "    discriminator1.train()\n",
    "    discriminator2.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    ave_loss = AverageMeter()\n",
    "    ave_acc = AverageMeter()\n",
    "    avg_sem_loss = AverageMeter()\n",
    "    avg_bce_loss = AverageMeter()\n",
    "\n",
    "    tic = time.time()\n",
    "    cur_iters = epoch * epoch_iters\n",
    "    writer = writer_dict['writer']\n",
    "    global_steps = writer_dict['train_global_steps']\n",
    "\n",
    "    # Iteratore per il targetloader\n",
    "    target_iter = iter(targetloader)\n",
    "\n",
    "    for i_iter, batch_source in enumerate(trainloader):\n",
    "        optimizer_G.zero_grad()\n",
    "        optimizer_D1.zero_grad()\n",
    "        optimizer_D2.zero_grad()\n",
    "\n",
    "        cumulative_loss_G = 0.0\n",
    "        cumulative_loss_D1 = 0.0\n",
    "        cumulative_loss_D2 = 0.0\n",
    "\n",
    "        for sub_iter in range(iter_size):  # Accumula i gradienti su più sotto-iterazioni\n",
    "            # Ottieni il batch target\n",
    "            try:\n",
    "                batch_target = next(target_iter)\n",
    "            except StopIteration:\n",
    "                target_iter = iter(targetloader)\n",
    "                batch_target = next(target_iter)\n",
    "\n",
    "            # Estrai i dati dal batch\n",
    "            images_source, labels, bd_gts, _, _ = batch_source\n",
    "            images_target, _, _, _, _ = batch_target\n",
    "\n",
    "            images_source = images_source.cuda()\n",
    "            images_target = images_target.cuda()\n",
    "            labels = labels.long().cuda()\n",
    "            bd_gts = bd_gts.float().cuda()\n",
    "\n",
    "            # ------------------ TRAINING DEL GENERATORE ------------------\n",
    "            # Forward pass della rete di segmentazione (Supervisionato su sorgente)\n",
    "            losses, output_source_final, output_source_intermediate, acc, loss_list = model(images_source, labels, bd_gts)\n",
    "            loss_seg = losses.mean()\n",
    "\n",
    "            # Forward pass della rete di segmentazione su target (Non supervisionato)\n",
    "            output_target_final, output_target_intermediate = model(images_target)\n",
    "\n",
    "            # Calcola la loss adversarial per il generatore\n",
    "            fake_preds1 = discriminator1(output_target_final)\n",
    "            fake_preds2 = discriminator2(output_target_intermediate)\n",
    "\n",
    "            loss_adv1 = nn.BCEWithLogitsLoss()(fake_preds1, torch.ones_like(fake_preds1))\n",
    "            loss_adv2 = nn.BCEWithLogitsLoss()(fake_preds2, torch.ones_like(fake_preds2))\n",
    "\n",
    "            loss_G = (loss_seg + lambda_adv1 * loss_adv1 + lambda_adv2 * loss_adv2) / iter_size\n",
    "            cumulative_loss_G += loss_G.item()\n",
    "            loss_G.backward()\n",
    "\n",
    "            # ------------------ TRAINING DEL DISCRIMINATORE 1 ------------------\n",
    "            real_preds1 = discriminator1(output_source_final.detach())\n",
    "            fake_preds1 = discriminator1(output_target_final.detach())\n",
    "\n",
    "            loss_D1_real = nn.BCEWithLogitsLoss()(real_preds1, torch.ones_like(real_preds1))\n",
    "            loss_D1_fake = nn.BCEWithLogitsLoss()(fake_preds1, torch.zeros_like(fake_preds1))\n",
    "            loss_D1 = (loss_D1_real + loss_D1_fake) / 2 / iter_size\n",
    "            cumulative_loss_D1 += loss_D1.item()\n",
    "            loss_D1.backward()\n",
    "\n",
    "            # ------------------ TRAINING DEL DISCRIMINATORE 2 ------------------\n",
    "            real_preds2 = discriminator2(output_source_intermediate.detach())\n",
    "            fake_preds2 = discriminator2(output_target_intermediate.detach())\n",
    "\n",
    "            loss_D2_real = nn.BCEWithLogitsLoss()(real_preds2, torch.ones_like(real_preds2))\n",
    "            loss_D2_fake = nn.BCEWithLogitsLoss()(fake_preds2, torch.zeros_like(fake_preds2))\n",
    "            loss_D2 = (loss_D2_real + loss_D2_fake) / 2 / iter_size\n",
    "            cumulative_loss_D2 += loss_D2.item()\n",
    "            loss_D2.backward()\n",
    "\n",
    "        # Aggiorna i pesi dopo tutte le sotto-iterazioni\n",
    "        optimizer_G.step()\n",
    "        optimizer_D1.step()\n",
    "        optimizer_D2.step()\n",
    "\n",
    "        # Log\n",
    "        batch_time.update(time.time() - tic)\n",
    "        tic = time.time()\n",
    "\n",
    "        ave_loss.update(cumulative_loss_G)\n",
    "        ave_acc.update(acc.mean().item())\n",
    "        avg_sem_loss.update(loss_list[0].mean().item())\n",
    "        avg_bce_loss.update(loss_list[1].mean().item())\n",
    "\n",
    "        lr = adjust_learning_rate(optimizer_G, base_lr, num_iters, i_iter + cur_iters)\n",
    "\n",
    "        if i_iter % config.PRINT_FREQ == 0:\n",
    "            msg = ('Epoch: [{}/{}] Iter:[{}/{}], Time: {:.2f}, lr: {}, '\n",
    "                   'Loss_G: {:.6f}, Loss_D1: {:.6f}, Loss_D2: {:.6f}, Acc: {:.6f}, Semantic Loss: {:.6f}, BCE Loss: {:.6f}').format(\n",
    "                      epoch, num_epoch, i_iter, epoch_iters, batch_time.average(),\n",
    "                      [x['lr'] for x in optimizer_G.param_groups], ave_loss.average(), cumulative_loss_D1,\n",
    "                      cumulative_loss_D2, ave_acc.average(), avg_sem_loss.average(), avg_bce_loss.average()\n",
    "                  )\n",
    "            logging.info(msg)\n",
    "\n",
    "    writer.add_scalar('train_loss_G', ave_loss.average(), global_steps)\n",
    "    writer.add_scalar('train_loss_D1', cumulative_loss_D1, global_steps)\n",
    "    writer.add_scalar('train_loss_D2', cumulative_loss_D2, global_steps)\n",
    "    writer_dict['train_global_steps'] = global_steps + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n",
    "Here the training loop is executed, then the validation phase is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:22.702790Z",
     "iopub.status.busy": "2025-05-12T16:05:22.702517Z",
     "iopub.status.idle": "2025-05-12T18:59:50.181629Z",
     "shell.execute_reply": "2025-05-12T18:59:50.180828Z",
     "shell.execute_reply.started": "2025-05-12T16:05:22.702769Z"
    },
    "id": "fhmkK1Xpg8Uk",
    "outputId": "070b97ec-3dfc-4042-d058-9ead37ba2cad",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from re import L\n",
    "# ------------------------------------------------------------------------------\n",
    "# Modified based on https://github.com/HRNet/HRNet-Semantic-Segmentation\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pprint\n",
    "import albumentations as A\n",
    "import logging\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, Image, display\n",
    "import sys\n",
    "\n",
    "os.environ[\"NO_ALBUMENTATIONS_UPDATE\"] = \"1\"\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Train segmentation network')\n",
    "\n",
    "    parser.add_argument('--cfg',\n",
    "                        help='experiment configure file name',\n",
    "                        default=\"configs/loveda/pidnet_small_loveda.yaml\", #file di configurazione da usare\n",
    "                        type=str)\n",
    "    parser.add_argument('--seed', type=int, default=304)\n",
    "    parser.add_argument('--opts',\n",
    "                        help=\"Modify config options using the command-line\",\n",
    "                        default=[],\n",
    "                        nargs=argparse.REMAINDER)\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    update_config(config, args) #aggiorna config con tutti i parametri trovati nel file di configurazione\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_pidnet_model(config):\n",
    "    class_weights_pidnet = torch.tensor([0.000000, 0.116411, 0.266041, 0.607794, 1.511413, 0.745507, 0.712438, 3.040396])\n",
    "    # criterion\n",
    "    if config.LOSS.USE_OHEM:\n",
    "        sem_criterion = OhemCrossEntropy(ignore_label=config.TRAIN.IGNORE_LABEL,\n",
    "                                        thres=config.LOSS.OHEMTHRES,\n",
    "                                        min_kept=config.LOSS.OHEMKEEP,\n",
    "                                        weight=class_weights_pidnet)\n",
    "    else:\n",
    "        sem_criterion = CrossEntropy(ignore_label=config.TRAIN.IGNORE_LABEL,\n",
    "                                     weight=class_weights_pidnet)\n",
    "\n",
    "    imgnet = 'imagenet' in config.MODEL.PRETRAINED\n",
    "    model = get_seg_model(config, imgnet_pretrained=imgnet)\n",
    "    bd_criterion = BondaryLoss()\n",
    "    \n",
    "    model = FullModel(model, sem_criterion, bd_criterion)\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_ema_model(model, config, gpus):\n",
    "    \"\"\"\n",
    "    The name ema_model literally stands for Exponential Moving Average model. \n",
    "    In the DACS code you’ll see that after each gradient update to your student network, they do:\n",
    "    \n",
    "    #for each pair of parameters (ema_param, param)\n",
    "    ema_param.data = alpha * ema_param.data + (1 - alpha) * param.data\n",
    "        \n",
    "    that’s exactly the EMA update rule. By keeping a separate copy of the network whose weights \n",
    "    are the exponential moving average of the student’s weights, you get a “smoothed,” more stable\n",
    "    model to generate pseudo-labels from. Hence the second network is called ema_model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ema_model = get_pidnet_model(config)\n",
    "    \n",
    "    for param in ema_model.parameters():\n",
    "        param.detach_()\n",
    "    mp = list(model.parameters())\n",
    "    mcp = list(ema_model.parameters())\n",
    "    n = len(mp)\n",
    "    for i in range(0, n):\n",
    "        mcp[i].data[:] = mp[i].data[:].clone()\n",
    "    ema_model = nn.DataParallel(ema_model, device_ids=gpus).cuda()\n",
    "    return ema_model\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    if args.seed > 0:\n",
    "        import random\n",
    "        print('Seeding with', args.seed)\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "\n",
    "    logger, final_output_dir, tb_log_dir = create_logger(config, args.cfg, 'train')\n",
    "    logger.info(pprint.pformat(args))\n",
    "    logger.info(config)\n",
    "\n",
    "    writer_dict = {\n",
    "        'writer': SummaryWriter(tb_log_dir),\n",
    "        'train_global_steps': 0,\n",
    "        'valid_global_steps': 0,\n",
    "    }\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using CUDA\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # cudnn related setting\n",
    "        cudnn.benchmark = config.CUDNN.BENCHMARK\n",
    "        cudnn.deterministic = config.CUDNN.DETERMINISTIC\n",
    "        cudnn.enabled = config.CUDNN.ENABLED\n",
    "        gpus = list(config.GPUS)\n",
    "        if torch.cuda.device_count() != len(gpus):\n",
    "            print(\"The gpu numbers do not match!\")\n",
    "            return 0\n",
    "    gpus = list(config.GPUS)\n",
    "\n",
    "    imgnet = 'imagenet' in config.MODEL.PRETRAINED\n",
    "    model = get_seg_model(config, imgnet_pretrained=imgnet)\n",
    "    \n",
    "    batch_size = config.TRAIN.BATCH_SIZE_PER_GPU * len(gpus)\n",
    "    # prepare data\n",
    "    #crop_size = (config.TRAIN.IMAGE_SIZE[1], config.TRAIN.IMAGE_SIZE[0])\n",
    "    crop_size = (512, 512)\n",
    "\n",
    "    train_trasform = None\n",
    "\n",
    "    if config.TRAIN.AUGMENTATION.ENABLE:\n",
    "        list_augmentations = []\n",
    "        if config.TRAIN.AUGMENTATION.TECHNIQUES.RANDOM_CROP:\n",
    "            list_augmentations.append(A.RandomResizedCrop(1024, 1024, p=0.5))\n",
    "        if config.TRAIN.AUGMENTATION.TECHNIQUES.HORIZONTAL_FLIP:\n",
    "            list_augmentations.append(A.HorizontalFlip(p=0.5))\n",
    "        if config.TRAIN.AUGMENTATION.TECHNIQUES.COLOR_JITTER:\n",
    "            list_augmentations.append(A.ColorJitter(p=0.5))\n",
    "        if config.TRAIN.AUGMENTATION.TECHNIQUES.GAUSSIAN_BLUR:\n",
    "            list_augmentations.append(A.GaussianBlur(p=0.5))\n",
    "        if config.TRAIN.AUGMENTATION.TECHNIQUES.GAUSSIAN_NOISE:\n",
    "            list_augmentations.append(A.GaussNoise(std_range=(0.2, 0.3), p=0.5))\n",
    "        if len(list_augmentations) != 0:\n",
    "            train_trasform = A.Compose(list_augmentations)\n",
    "\n",
    "    #The eval() function evaluates the specified expression, if the expression is a legal Python statement, it will be executed.\n",
    "    train_dataset = LoveDA(\n",
    "                        root=config.DATASET.ROOT,\n",
    "                        list_path=config.DATASET.TRAIN_SET,\n",
    "                        num_classes=config.DATASET.NUM_CLASSES,\n",
    "                        multi_scale=config.TRAIN.MULTI_SCALE,\n",
    "                        flip=config.TRAIN.FLIP,\n",
    "                        enable_augmentation=True,\n",
    "                        ignore_label=config.TRAIN.IGNORE_LABEL,\n",
    "                        base_size=config.TRAIN.BASE_SIZE,\n",
    "                        crop_size=crop_size,\n",
    "                        scale_factor=config.TRAIN.SCALE_FACTOR,\n",
    "                        horizontal_flip=config.TRAIN.AUGMENTATION.TECHNIQUES.HORIZONTAL_FLIP,\n",
    "                        gaussian_blur=config.TRAIN.AUGMENTATION.TECHNIQUES.GAUSSIAN_BLUR,\n",
    "                        transform=None)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=config.TRAIN.SHUFFLE,\n",
    "        num_workers=config.WORKERS,\n",
    "        pin_memory=False,\n",
    "        drop_last=True)\n",
    "\n",
    "\n",
    "    targetloader = None\n",
    "    if config.TRAIN.DACS.ENABLE or config.TRAIN.GAN.ENABLE:\n",
    "        target_dataset = LoveDA(\n",
    "        root=config.DATASET.ROOT,\n",
    "        list_path=config.DATASET.TARGET_SET,\n",
    "        num_classes=config.DATASET.NUM_CLASSES,\n",
    "        multi_scale=config.TRAIN.MULTI_SCALE,\n",
    "        flip=config.TRAIN.FLIP,\n",
    "        enable_augmentation=True,\n",
    "        ignore_label=config.TRAIN.IGNORE_LABEL,\n",
    "        base_size=config.TRAIN.BASE_SIZE,\n",
    "        crop_size=crop_size,\n",
    "        scale_factor=config.TRAIN.SCALE_FACTOR,\n",
    "        horizontal_flip=config.TRAIN.AUGMENTATION.TECHNIQUES.HORIZONTAL_FLIP,\n",
    "        gaussian_blur=config.TRAIN.AUGMENTATION.TECHNIQUES.GAUSSIAN_BLUR,\n",
    "        random_crop=config.TRAIN.AUGMENTATION.TECHNIQUES.RANDOM_CROP,\n",
    "        transform=None)\n",
    "\n",
    "        targetloader = torch.utils.data.DataLoader(\n",
    "            target_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=config.TRAIN.SHUFFLE,\n",
    "            num_workers=config.WORKERS,\n",
    "            pin_memory=False,\n",
    "            drop_last=True)\n",
    "\n",
    "\n",
    "    test_size = (config.TEST.IMAGE_SIZE[1], config.TEST.IMAGE_SIZE[0])\n",
    "    test_dataset = LoveDA(\n",
    "                        root=config.DATASET.ROOT,\n",
    "                        list_path=config.DATASET.TEST_SET,\n",
    "                        num_classes=config.DATASET.NUM_CLASSES,\n",
    "                        multi_scale=False,\n",
    "                        flip=False,\n",
    "                        ignore_label=config.TRAIN.IGNORE_LABEL,\n",
    "                        base_size=config.TEST.BASE_SIZE,\n",
    "                        crop_size=test_size,\n",
    "                        weighted=False)\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.TEST.BATCH_SIZE_PER_GPU * len(gpus),\n",
    "        shuffle=False,\n",
    "        num_workers=config.WORKERS,\n",
    "        pin_memory=False)\n",
    "\n",
    "    # criterion\n",
    "    if config.LOSS.USE_OHEM:\n",
    "        sem_criterion = OhemCrossEntropy(ignore_label=config.TRAIN.IGNORE_LABEL,\n",
    "                                        thres=config.LOSS.OHEMTHRES,\n",
    "                                        min_kept=config.LOSS.OHEMKEEP,\n",
    "                                        weight=train_dataset.class_weights)\n",
    "    else:\n",
    "        sem_criterion = CrossEntropy(ignore_label=config.TRAIN.IGNORE_LABEL,\n",
    "                                    weight=train_dataset.class_weights)\n",
    "\n",
    "    bd_criterion = BondaryLoss()\n",
    "\n",
    "    \n",
    "    model = FullModel(model, sem_criterion, bd_criterion)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = nn.DataParallel(model, device_ids=gpus).cuda() #per noi inutile\n",
    "    else:\n",
    "        model = model.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    if config.TRAIN.OPTIMIZER == 'sgd':\n",
    "        params_dict = dict(model.named_parameters())\n",
    "        params = [{'params': list(params_dict.values()), 'lr': config.TRAIN.LR}]\n",
    "\n",
    "        optimizer = torch.optim.SGD(params,\n",
    "                                lr=config.TRAIN.LR,\n",
    "                                momentum=config.TRAIN.MOMENTUM,\n",
    "                                weight_decay=config.TRAIN.WD,\n",
    "                                nesterov=config.TRAIN.NESTEROV,\n",
    "                                )\n",
    "    else:\n",
    "        raise ValueError('Only Support SGD optimizer')\n",
    "\n",
    "    epoch_iters = int(train_dataset.__len__() / config.TRAIN.BATCH_SIZE_PER_GPU / len(gpus))\n",
    "\n",
    "    best_mIoU = 0\n",
    "    last_epoch = 0\n",
    "    flag_rm = config.TRAIN.RESUME\n",
    "    if config.TRAIN.RESUME:\n",
    "        model_state_file = os.path.join(final_output_dir, 'checkpoint.pth.tar')\n",
    "        if os.path.isfile(model_state_file):\n",
    "            print('-'*60)\n",
    "            checkpoint = torch.load(model_state_file, map_location={'cuda:0': 'cpu'})\n",
    "            best_mIoU = checkpoint['best_mIoU']\n",
    "            last_epoch = checkpoint['epoch']\n",
    "            dct = checkpoint['state_dict']\n",
    "\n",
    "            model.module.model.load_state_dict({k.replace('model.', ''): v for k, v in dct.items() if k.startswith('model.')})\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            logger.info(\"=> loaded checkpoint (epoch {})\".format(checkpoint['epoch']))\n",
    "\n",
    "    start = timeit.default_timer()\n",
    "    end_epoch = config.TRAIN.END_EPOCH\n",
    "    num_iters = config.TRAIN.END_EPOCH * epoch_iters\n",
    "    real_end = 120+1 if 'camvid' in config.DATASET.TRAIN_SET else end_epoch\n",
    "\n",
    "    # grafici\n",
    "    #plt.ion()  # Modalità interattiva\n",
    "    #fig, ax = plt.subplots(2, 1, figsize=(10, 8))  # Due grafici: uno per le loss, uno per la mean IoU\n",
    "    train_loss_history = []\n",
    "    eval_loss_history = []\n",
    "    mean_iou_history = []\n",
    "\n",
    "    activate_ema_model = True\n",
    "    ema_model = None\n",
    "    deeplab_model = None\n",
    "    # deeplab_model.load_state_dict(torch.load(\"/kaggle/input/deeplab-pretrained/deeplabv2_loveda_best.pth\"))\n",
    "    # deeplab_model.to(device)\n",
    "    # deeplab_model.eval()\n",
    "    for epoch in range(last_epoch, real_end):\n",
    "\n",
    "        # --- EMA model creation---\n",
    "        if config.TRAIN.DACS.ENABLE and activate_ema_model and epoch > -1:\n",
    "            ema_model = create_ema_model(model, config, gpus)\n",
    "            activate_ema_model = False\n",
    "        # --- EMA model creation---\n",
    "    \n",
    "\n",
    "        current_trainloader = trainloader\n",
    "        if current_trainloader.sampler is not None and hasattr(current_trainloader.sampler, 'set_epoch'):\n",
    "            current_trainloader.sampler.set_epoch(epoch)\n",
    "\n",
    "        if config.TRAIN.GAN.ENABLE:\n",
    "\n",
    "            discriminator = FCDiscriminator(num_classes=8).to(device)\n",
    "            #optimizer_G = optim.SGD(model.parameters(), lr=2.5e-4, momentum=0.9, weight_decay=1e-4) paper infos, but our net is different\n",
    "            optimizer_G = optimizer\n",
    "            optimizer_D = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.9, 0.99)) #given by the paper\n",
    "\n",
    "            if config.TRAIN.GAN.MULTI_LEVEL:\n",
    "                train_adv_multi(config, epoch, config.TRAIN.END_EPOCH, epoch_iters, config.TRAIN.LR, num_iters, trainloader, targetloader, optimizer_G, optimizer_D, model, discriminator,discriminator, writer_dict)\n",
    "            else:\n",
    "                train_adv(config, epoch, config.TRAIN.END_EPOCH, epoch_iters, config.TRAIN.LR, num_iters, trainloader, targetloader, optimizer_G, optimizer_D, model, discriminator, writer_dict)\n",
    "        \n",
    "        elif config.TRAIN.DACS.ENABLE:\n",
    "            train_loss = train(config, epoch, config.TRAIN.END_EPOCH,\n",
    "                  epoch_iters, config.TRAIN.LR, num_iters,\n",
    "                  trainloader, optimizer, model, writer_dict, deeplab_model=deeplab_model, ema_model=ema_model, targetloader=targetloader)\n",
    "        else:\n",
    "            \n",
    "            train_loss = train(config, epoch, config.TRAIN.END_EPOCH,\n",
    "                  epoch_iters, config.TRAIN.LR, num_iters,\n",
    "                  trainloader, optimizer, model, writer_dict, targetloader=targetloader)\n",
    "\n",
    "        train_loss_history.append(train_loss)\n",
    "\n",
    "        if flag_rm == 1 or (epoch % 5 == 0 and epoch < real_end - 100) or (epoch >= real_end - 100):\n",
    "            valid_loss, mean_IoU, IoU_array = validate(config, testloader, model, writer_dict)\n",
    "            eval_loss_history.append(valid_loss)\n",
    "            mean_iou_history.append(mean_IoU)\n",
    "\n",
    "        if flag_rm == 1:\n",
    "            flag_rm = 0\n",
    "        logger.info('=> saving checkpoint to {}'.format(\n",
    "            final_output_dir + 'checkpoint.pth.tar'))\n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'best_mIoU': best_mIoU,\n",
    "            'state_dict': model.module.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, os.path.join(final_output_dir,'checkpoint.pth.tar'))\n",
    "        if mean_IoU > best_mIoU:\n",
    "            best_mIoU = mean_IoU\n",
    "            torch.save(model.module.state_dict(),\n",
    "                    os.path.join(final_output_dir, 'best.pt'))\n",
    "        msg = 'Loss: {:.3f}, MeanIU: {: 4.4f}, Best_mIoU: {: 4.4f}'.format(\n",
    "                    valid_loss, mean_IoU, best_mIoU)\n",
    "        logging.info(msg)\n",
    "        logging.info(IoU_array)\n",
    "\n",
    "\n",
    "\n",
    "    torch.save(model.module.state_dict(),\n",
    "            os.path.join(final_output_dir, 'final_state.pt'))\n",
    "\n",
    "    writer_dict['writer'].close()\n",
    "    end = timeit.default_timer()\n",
    "    logger.info('Hours: %d' % int((end-start)/3600))\n",
    "    logger.info('Done')\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T19:00:53.459304Z",
     "iopub.status.busy": "2025-05-12T19:00:53.459019Z",
     "iopub.status.idle": "2025-05-12T19:00:59.671883Z",
     "shell.execute_reply": "2025-05-12T19:00:59.671129Z",
     "shell.execute_reply.started": "2025-05-12T19:00:53.459282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Crea un file ZIP della cartella /kaggle/working/\n",
    "shutil.make_archive('/kaggle/working/log', 'zip', '/kaggle/working/log')\n",
    "shutil.make_archive('/kaggle/working/output', 'zip', '/kaggle/working/output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T19:01:22.845965Z",
     "iopub.status.busy": "2025-05-12T19:01:22.845667Z",
     "iopub.status.idle": "2025-05-12T19:01:22.851662Z",
     "shell.execute_reply": "2025-05-12T19:01:22.851017Z",
     "shell.execute_reply.started": "2025-05-12T19:01:22.845943Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "FileLink('/kaggle/working/output.zip')\n",
    "FileLink('/kaggle/working/log.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7153631,
     "sourceId": 11422581,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7153698,
     "sourceId": 11422827,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7154101,
     "sourceId": 11423306,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7390048,
     "sourceId": 11771064,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7292060,
     "sourceId": 11780531,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
