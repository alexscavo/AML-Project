{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11422827,"sourceType":"datasetVersion","datasetId":7153698},{"sourceId":11423306,"sourceType":"datasetVersion","datasetId":7154101}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q albumentations==1.4.18","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:55:30.488940Z","iopub.execute_input":"2025-05-28T07:55:30.489663Z","iopub.status.idle":"2025-05-28T07:55:35.541663Z","shell.execute_reply.started":"2025-05-28T07:55:30.489631Z","shell.execute_reply":"2025-05-28T07:55:35.540951Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.0/224.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install yacs\n!pip install tensorboardX","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:55:35.543294Z","iopub.execute_input":"2025-05-28T07:55:35.543538Z","iopub.status.idle":"2025-05-28T07:55:41.917643Z","shell.execute_reply.started":"2025-05-28T07:55:35.543515Z","shell.execute_reply":"2025-05-28T07:55:41.916853Z"}},"outputs":[{"name":"stdout","text":"Collecting yacs\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from yacs) (6.0.2)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nInstalling collected packages: yacs\nSuccessfully installed yacs-0.1.8\nCollecting tensorboardX\n  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (25.0)\nRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (3.20.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->tensorboardX) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->tensorboardX) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->tensorboardX) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->tensorboardX) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->tensorboardX) (2024.2.0)\nDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tensorboardX\nSuccessfully installed tensorboardX-2.6.2.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import shutil\nshutil.copytree('/kaggle/input/pidnet-pretrained/', '/kaggle/working/pretrained_model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:55:41.919214Z","iopub.execute_input":"2025-05-28T07:55:41.919523Z","iopub.status.idle":"2025-05-28T07:55:42.515709Z","shell.execute_reply.started":"2025-05-28T07:55:41.919496Z","shell.execute_reply":"2025-05-28T07:55:42.514992Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/pretrained_model'"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import os\n\ndef create_lst_file(image_dir, label_dir, output_lst):\n    # List and sort files numerically\n    images = sorted(os.listdir(image_dir), key=lambda x: int(os.path.splitext(x)[0]))\n    labels = sorted(os.listdir(label_dir), key=lambda x: int(os.path.splitext(x)[0]))\n\n    os.makedirs(os.path.dirname(output_lst), exist_ok=True)  # Ensure the directory exists\n\n    with open(output_lst, 'w') as f:\n        for img, lbl in zip(images, labels):\n            # Generate full paths and normalize to use forward slashes\n            img_path = os.path.join(image_dir, img).replace(\"\\\\\", \"/\")\n            lbl_path = os.path.join(label_dir, lbl).replace(\"\\\\\", \"/\")\n            # Write formatted line with consistent spacing\n            f.write(f\"{img_path} {lbl_path}\\n\")\n\n# Paths to the LoveDA dataset directories\ntrain_image_dir = \"/kaggle/input/loveda-splits/Train/Train/Urban/images_png\"\ntrain_label_dir = \"/kaggle/input/loveda-splits/Train/Train/Urban/masks_png\"\n\n\ntarget_image_dir = \"/kaggle/input/loveda-splits/Train/Train/Rural/images_png\"\ntarget_label_dir = \"/kaggle/input/loveda-splits/Train/Train/Rural/masks_png\"\n\nval_image_dir = \"/kaggle/input/loveda-splits/Val/Val/Rural/images_png\"\nval_label_dir = \"/kaggle/input/loveda-splits/Val/Val/Rural/masks_png\"\n\n\ntrain_lst_path = \"list/urban/train.lst\"\ntarget_lst_path = \"list/rural/train.lst\"\nval_lst_path = \"list/rural/val.lst\"\n\n# Create .lst files\ncreate_lst_file(train_image_dir, train_label_dir, train_lst_path)\ncreate_lst_file(target_image_dir, target_label_dir, target_lst_path)\ncreate_lst_file(val_image_dir, val_label_dir, val_lst_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:55:42.517407Z","iopub.execute_input":"2025-05-28T07:55:42.517601Z","iopub.status.idle":"2025-05-28T07:55:42.644454Z","shell.execute_reply.started":"2025-05-28T07:55:42.517585Z","shell.execute_reply":"2025-05-28T07:55:42.643964Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ------------------------------------------------------------------------------\n# Copyright (c) Microsoft\n# Licensed under the MIT License.\n# Written by Ke Sun (sunk@mail.ustc.edu.cn)\n# ------------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path as osp\nimport sys\n\n\ndef add_path(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:55:42.645055Z","iopub.execute_input":"2025-05-28T07:55:42.645215Z","iopub.status.idle":"2025-05-28T07:55:42.649183Z","shell.execute_reply.started":"2025-05-28T07:55:42.645201Z","shell.execute_reply":"2025-05-28T07:55:42.648676Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nBatchNorm2d = nn.BatchNorm2d\nbn_mom = 0.1\nalgc = False\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=False):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn1 = BatchNorm2d(planes, momentum=bn_mom)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               padding=1, bias=False)\n        self.bn2 = BatchNorm2d(planes, momentum=bn_mom)\n        self.downsample = downsample\n        self.stride = stride\n        self.no_relu = no_relu\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n\n        if self.no_relu:\n            return out\n        else:\n            return self.relu(out)\n\nclass Bottleneck(nn.Module):\n    expansion = 2\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=True):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm2d(planes, momentum=bn_mom)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = BatchNorm2d(planes, momentum=bn_mom)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n                               bias=False)\n        self.bn3 = BatchNorm2d(planes * self.expansion, momentum=bn_mom)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.no_relu = no_relu\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        if self.no_relu:\n            return out\n        else:\n            return self.relu(out)\n\nclass segmenthead(nn.Module):\n\n    def __init__(self, inplanes, interplanes, outplanes, scale_factor=None):\n        super(segmenthead, self).__init__()\n        self.bn1 = BatchNorm2d(inplanes, momentum=bn_mom)\n        self.conv1 = nn.Conv2d(inplanes, interplanes, kernel_size=3, padding=1, bias=False)\n        self.bn2 = BatchNorm2d(interplanes, momentum=bn_mom)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(interplanes, outplanes, kernel_size=1, padding=0, bias=True)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n\n        x = self.conv1(self.relu(self.bn1(x)))\n        out = self.conv2(self.relu(self.bn2(x)))\n\n        if self.scale_factor is not None:\n            height = x.shape[-2] * self.scale_factor\n            width = x.shape[-1] * self.scale_factor\n            out = F.interpolate(out,\n                        size=[height, width],\n                        mode='bilinear', align_corners=algc)\n\n        return out\n\nclass DAPPM(nn.Module):\n    def __init__(self, inplanes, branch_planes, outplanes, BatchNorm=nn.BatchNorm2d):\n        super(DAPPM, self).__init__()\n        bn_mom = 0.1\n        self.scale1 = nn.Sequential(nn.AvgPool2d(kernel_size=5, stride=2, padding=2),\n                                    BatchNorm(inplanes, momentum=bn_mom),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n                                    )\n        self.scale2 = nn.Sequential(nn.AvgPool2d(kernel_size=9, stride=4, padding=4),\n                                    BatchNorm(inplanes, momentum=bn_mom),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n                                    )\n        self.scale3 = nn.Sequential(nn.AvgPool2d(kernel_size=17, stride=8, padding=8),\n                                    BatchNorm(inplanes, momentum=bn_mom),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n                                    )\n        self.scale4 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n                                    BatchNorm(inplanes, momentum=bn_mom),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n                                    )\n        self.scale0 = nn.Sequential(\n                                    BatchNorm(inplanes, momentum=bn_mom),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n                                    )\n        self.process1 = nn.Sequential(\n                                    BatchNorm(branch_planes, momentum=bn_mom),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n                                    )\n        self.process2 = nn.Sequential(\n                                    BatchNorm(branch_planes, momentum=bn_mom),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n                                    )\n        self.process3 = nn.Sequential(\n                                    BatchNorm(branch_planes, momentum=bn_mom),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n                                    )\n        self.process4 = nn.Sequential(\n                                    BatchNorm(branch_planes, momentum=bn_mom),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n                                    )\n        self.compression = nn.Sequential(\n                                    BatchNorm(branch_planes * 5, momentum=bn_mom),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(branch_planes * 5, outplanes, kernel_size=1, bias=False),\n                                    )\n        self.shortcut = nn.Sequential(\n                                    BatchNorm(inplanes, momentum=bn_mom),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=False),\n                                    )\n\n    def forward(self, x):\n        width = x.shape[-1]\n        height = x.shape[-2]\n        x_list = []\n\n        x_list.append(self.scale0(x))\n        x_list.append(self.process1((F.interpolate(self.scale1(x),\n                        size=[height, width],\n                        mode='bilinear', align_corners=algc)+x_list[0])))\n        x_list.append((self.process2((F.interpolate(self.scale2(x),\n                        size=[height, width],\n                        mode='bilinear', align_corners=algc)+x_list[1]))))\n        x_list.append(self.process3((F.interpolate(self.scale3(x),\n                        size=[height, width],\n                        mode='bilinear', align_corners=algc)+x_list[2])))\n        x_list.append(self.process4((F.interpolate(self.scale4(x),\n                        size=[height, width],\n                        mode='bilinear', align_corners=algc)+x_list[3])))\n\n        out = self.compression(torch.cat(x_list, 1)) + self.shortcut(x)\n        return out\n\nclass PAPPM(nn.Module):\n    def __init__(self, inplanes, branch_planes, outplanes, BatchNorm=nn.BatchNorm2d):\n        super(PAPPM, self).__init__()\n        bn_mom = 0.1\n        self.scale1 = nn.Sequential(nn.AvgPool2d(kernel_size=5, stride=2, padding=2),\n                                    BatchNorm(inplanes, momentum=bn_mom),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n                                    )\n        self.scale2 = nn.Sequential(nn.AvgPool2d(kernel_size=9, stride=4, padding=4),\n                                    BatchNorm(inplanes, momentum=bn_mom),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n                                    )\n        self.scale3 = nn.Sequential(nn.AvgPool2d(kernel_size=17, stride=8, padding=8),\n                                    BatchNorm(inplanes, momentum=bn_mom),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n                                    )\n        self.scale4 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n                                    BatchNorm(inplanes, momentum=bn_mom),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n                                    )\n\n        self.scale0 = nn.Sequential(\n                                    BatchNorm(inplanes, momentum=bn_mom),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n                                    )\n\n        self.scale_process = nn.Sequential(\n                                    BatchNorm(branch_planes*4, momentum=bn_mom),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(branch_planes*4, branch_planes*4, kernel_size=3, padding=1, groups=4, bias=False),\n                                    )\n\n\n        self.compression = nn.Sequential(\n                                    BatchNorm(branch_planes * 5, momentum=bn_mom),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(branch_planes * 5, outplanes, kernel_size=1, bias=False),\n                                    )\n\n        self.shortcut = nn.Sequential(\n                                    BatchNorm(inplanes, momentum=bn_mom),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=False),\n                                    )\n\n\n    def forward(self, x):\n        width = x.shape[-1]\n        height = x.shape[-2]\n        scale_list = []\n\n        x_ = self.scale0(x)\n        scale_list.append(F.interpolate(self.scale1(x), size=[height, width],\n                        mode='bilinear', align_corners=algc)+x_)\n        scale_list.append(F.interpolate(self.scale2(x), size=[height, width],\n                        mode='bilinear', align_corners=algc)+x_)\n        scale_list.append(F.interpolate(self.scale3(x), size=[height, width],\n                        mode='bilinear', align_corners=algc)+x_)\n        scale_list.append(F.interpolate(self.scale4(x), size=[height, width],\n                        mode='bilinear', align_corners=algc)+x_)\n\n        scale_out = self.scale_process(torch.cat(scale_list, 1))\n\n        out = self.compression(torch.cat([x_,scale_out], 1)) + self.shortcut(x)\n        return out\n\n\nclass PagFM(nn.Module):\n    def __init__(self, in_channels, mid_channels, after_relu=False, with_channel=False, BatchNorm=nn.BatchNorm2d):\n        super(PagFM, self).__init__()\n        self.with_channel = with_channel\n        self.after_relu = after_relu\n        self.f_x = nn.Sequential(\n                                nn.Conv2d(in_channels, mid_channels,\n                                          kernel_size=1, bias=False),\n                                BatchNorm(mid_channels)\n                                )\n        self.f_y = nn.Sequential(\n                                nn.Conv2d(in_channels, mid_channels,\n                                          kernel_size=1, bias=False),\n                                BatchNorm(mid_channels)\n                                )\n        if with_channel:\n            self.up = nn.Sequential(\n                                    nn.Conv2d(mid_channels, in_channels,\n                                              kernel_size=1, bias=False),\n                                    BatchNorm(in_channels)\n                                   )\n        if after_relu:\n            self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x, y):\n        input_size = x.size()\n        if self.after_relu:\n            y = self.relu(y)\n            x = self.relu(x)\n\n        y_q = self.f_y(y)\n        y_q = F.interpolate(y_q, size=[input_size[2], input_size[3]],\n                            mode='bilinear', align_corners=False)\n        x_k = self.f_x(x)\n\n        if self.with_channel:\n            sim_map = torch.sigmoid(self.up(x_k * y_q))\n        else:\n            sim_map = torch.sigmoid(torch.sum(x_k * y_q, dim=1).unsqueeze(1))\n\n        y = F.interpolate(y, size=[input_size[2], input_size[3]],\n                            mode='bilinear', align_corners=False)\n        x = (1-sim_map)*x + sim_map*y\n\n        return x\n\nclass Light_Bag(nn.Module):\n    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n        super(Light_Bag, self).__init__()\n        self.conv_p = nn.Sequential(\n                                nn.Conv2d(in_channels, out_channels,\n                                          kernel_size=1, bias=False),\n                                BatchNorm(out_channels)\n                                )\n        self.conv_i = nn.Sequential(\n                                nn.Conv2d(in_channels, out_channels,\n                                          kernel_size=1, bias=False),\n                                BatchNorm(out_channels)\n                                )\n\n    def forward(self, p, i, d):\n        edge_att = torch.sigmoid(d)\n\n        p_add = self.conv_p((1-edge_att)*i + p)\n        i_add = self.conv_i(i + edge_att*p)\n\n        return p_add + i_add\n\n\nclass DDFMv2(nn.Module):\n    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n        super(DDFMv2, self).__init__()\n        self.conv_p = nn.Sequential(\n                                BatchNorm(in_channels),\n                                nn.ReLU(inplace=True),\n                                nn.Conv2d(in_channels, out_channels,\n                                          kernel_size=1, bias=False),\n                                BatchNorm(out_channels)\n                                )\n        self.conv_i = nn.Sequential(\n                                BatchNorm(in_channels),\n                                nn.ReLU(inplace=True),\n                                nn.Conv2d(in_channels, out_channels,\n                                          kernel_size=1, bias=False),\n                                BatchNorm(out_channels)\n                                )\n\n    def forward(self, p, i, d):\n        edge_att = torch.sigmoid(d)\n\n        p_add = self.conv_p((1-edge_att)*i + p)\n        i_add = self.conv_i(i + edge_att*p)\n\n        return p_add + i_add\n\nclass Bag(nn.Module):\n    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n        super(Bag, self).__init__()\n\n        self.conv = nn.Sequential(\n                                BatchNorm(in_channels),\n                                nn.ReLU(inplace=True),\n                                nn.Conv2d(in_channels, out_channels,\n                                          kernel_size=3, padding=1, bias=False)\n                                )\n\n\n    def forward(self, p, i, d):\n        edge_att = torch.sigmoid(d)\n        return self.conv(edge_att*p + (1-edge_att)*i)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:55:42.649944Z","iopub.execute_input":"2025-05-28T07:55:42.650102Z","iopub.status.idle":"2025-05-28T07:55:46.526158Z","shell.execute_reply.started":"2025-05-28T07:55:42.650089Z","shell.execute_reply":"2025-05-28T07:55:46.525647Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport time\nimport logging\n\nBatchNorm2d = nn.BatchNorm2d\nbn_mom = 0.1\nalgc = False\n\nclass PIDNet(nn.Module):\n\n    def __init__(self, m=2, n=3, num_classes=19, planes=64, ppm_planes=96, head_planes=128, augment=True):\n        super(PIDNet, self).__init__()\n        self.augment = augment\n\n        # I Branch\n        self.conv1 =  nn.Sequential(\n                          nn.Conv2d(3,planes,kernel_size=3, stride=2, padding=1),\n                          BatchNorm2d(planes, momentum=bn_mom),\n                          nn.ReLU(inplace=True),\n                          nn.Conv2d(planes,planes,kernel_size=3, stride=2, padding=1),\n                          BatchNorm2d(planes, momentum=bn_mom),\n                          nn.ReLU(inplace=True),\n                      )\n\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(BasicBlock, planes, planes, m)\n        self.layer2 = self._make_layer(BasicBlock, planes, planes * 2, m, stride=2)\n        self.layer3 = self._make_layer(BasicBlock, planes * 2, planes * 4, n, stride=2)\n        self.layer4 = self._make_layer(BasicBlock, planes * 4, planes * 8, n, stride=2)\n        self.layer5 =  self._make_layer(Bottleneck, planes * 8, planes * 8, 2, stride=2)\n\n        # P Branch\n        self.compression3 = nn.Sequential(\n                                          nn.Conv2d(planes * 4, planes * 2, kernel_size=1, bias=False),\n                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n                                          )\n\n        self.compression4 = nn.Sequential(\n                                          nn.Conv2d(planes * 8, planes * 2, kernel_size=1, bias=False),\n                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n                                          )\n        self.pag3 = PagFM(planes * 2, planes)\n        self.pag4 = PagFM(planes * 2, planes)\n\n        self.layer3_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n        self.layer4_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n        self.layer5_ = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n\n        # D Branch\n        if m == 2:\n            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes)\n            self.layer4_d = self._make_layer(Bottleneck, planes, planes, 1)\n            self.diff3 = nn.Sequential(\n                                        nn.Conv2d(planes * 4, planes, kernel_size=3, padding=1, bias=False),\n                                        BatchNorm2d(planes, momentum=bn_mom),\n                                        )\n            self.diff4 = nn.Sequential(\n                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n                                     )\n            self.spp = PAPPM(planes * 16, ppm_planes, planes * 4)\n            self.dfm = Light_Bag(planes * 4, planes * 4)\n        else:\n            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n            self.layer4_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n            self.diff3 = nn.Sequential(\n                                        nn.Conv2d(planes * 4, planes * 2, kernel_size=3, padding=1, bias=False),\n                                        BatchNorm2d(planes * 2, momentum=bn_mom),\n                                        )\n            self.diff4 = nn.Sequential(\n                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n                                     )\n            self.spp = DAPPM(planes * 16, ppm_planes, planes * 4)\n            self.dfm = Bag(planes * 4, planes * 4)\n\n        self.layer5_d = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n\n        # Prediction Head\n        if self.augment:\n            self.seghead_p = segmenthead(planes * 2, head_planes, num_classes)\n            self.seghead_d = segmenthead(planes * 2, planes, 1)\n\n        self.final_layer = segmenthead(planes * 4, head_planes, num_classes)\n\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n\n    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n            )\n\n        layers = []\n        layers.append(block(inplanes, planes, stride, downsample))\n        inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            if i == (blocks-1):\n                layers.append(block(inplanes, planes, stride=1, no_relu=True))\n            else:\n                layers.append(block(inplanes, planes, stride=1, no_relu=False))\n\n        return nn.Sequential(*layers)\n\n    def _make_single_layer(self, block, inplanes, planes, stride=1):\n        downsample = None\n        if stride != 1 or inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n            )\n\n        layer = block(inplanes, planes, stride, downsample, no_relu=True)\n\n        return layer\n\n    def forward(self, x):\n\n        width_output = x.shape[-1] // 8\n        height_output = x.shape[-2] // 8\n\n        x = self.conv1(x)\n        x = self.layer1(x)\n        x = self.relu(self.layer2(self.relu(x)))\n        x_ = self.layer3_(x)\n        x_d = self.layer3_d(x)\n\n        x = self.relu(self.layer3(x))\n        x_ = self.pag3(x_, self.compression3(x))\n        x_d = x_d + F.interpolate(\n                        self.diff3(x),\n                        size=[height_output, width_output],\n                        mode='bilinear', align_corners=algc)\n        if self.augment:\n            temp_p = x_\n\n        x = self.relu(self.layer4(x))\n        x_ = self.layer4_(self.relu(x_))\n        x_d = self.layer4_d(self.relu(x_d))\n\n        x_ = self.pag4(x_, self.compression4(x))\n        x_d = x_d + F.interpolate(\n                        self.diff4(x),\n                        size=[height_output, width_output],\n                        mode='bilinear', align_corners=algc)\n        if self.augment:\n            temp_d = x_d\n\n        x_ = self.layer5_(self.relu(x_))\n        x_d = self.layer5_d(self.relu(x_d))\n        x = F.interpolate(\n                        self.spp(self.layer5(x)),\n                        size=[height_output, width_output],\n                        mode='bilinear', align_corners=algc)\n\n        x_ = self.final_layer(self.dfm(x_, x, x_d))\n\n        if self.augment:\n            x_extra_p = self.seghead_p(temp_p)\n            x_extra_d = self.seghead_d(temp_d)\n            return [x_extra_p, x_, x_extra_d]\n        else:\n            return x_\n\ndef get_seg_model():\n\n    model = PIDNet(m=2, n=3, num_classes=8, planes=32, ppm_planes=96, head_planes=128, augment=True)\n    \n    \n    pretrained_state = torch.load('/kaggle/working/pretrained_model/PIDNet_S_ImageNet.pth.tar', map_location='cpu')['state_dict']\n    model_dict = model.state_dict()\n    pretrained_state = {k: v for k, v in pretrained_state.items() if (k in model_dict and v.shape == model_dict[k].shape)}\n    model_dict.update(pretrained_state)\n    msg = 'Loaded {} parameters!'.format(len(pretrained_state))\n    logging.info('Attention!!!')\n    logging.info(msg)\n    logging.info('Over!!!')\n    model.load_state_dict(model_dict, strict = False)\n\n    return model\n\ndef get_pred_model(name, num_classes):\n\n    if 's' in name:\n        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=32, ppm_planes=96, head_planes=128, augment=False)\n    elif 'm' in name:\n        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=64, ppm_planes=96, head_planes=128, augment=False)\n    else:\n        model = PIDNet(m=3, n=4, num_classes=num_classes, planes=64, ppm_planes=112, head_planes=256, augment=False)\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:55:46.526873Z","iopub.execute_input":"2025-05-28T07:55:46.527135Z","iopub.status.idle":"2025-05-28T07:55:46.550341Z","shell.execute_reply.started":"2025-05-28T07:55:46.527118Z","shell.execute_reply":"2025-05-28T07:55:46.549790Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport random\nimport albumentations as A\nfrom torch.nn import functional as F\nfrom torch.utils import data\nimport matplotlib.pyplot as plt\ny_k_size = 6\nx_k_size = 6\n\ndef show_images(x_original, x_augmented, unnormalize = False):\n\n    if unnormalize:\n        # ImageNet mean and std\n        imagenet_mean = np.array([0.485, 0.456, 0.406])[:, None, None]\n        imagenet_std = np.array([0.229, 0.224, 0.225])[:, None, None]\n\n        # Denormalize using NumPy broadcasting\n        x_original = x_original * imagenet_std + imagenet_mean\n        x_augmented = x_augmented * imagenet_std + imagenet_mean\n\n        # Clip to [0, 1] in case of overflows\n        x_original = np.clip(x_original, 0, 1)\n        x_augmented = np.clip(x_augmented, 0, 1)\n\n        # Transpose to HWC for matplotlib\n        x_original = np.transpose(x_original, (1, 2, 0))\n        x_augmented = np.transpose(x_augmented, (1, 2, 0))\n\n    # Plot\n    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n    axs[0].imshow(x_original)\n    axs[0].set_title(\"Original Image\")\n    axs[0].axis(\"off\")\n\n    axs[1].imshow(x_augmented)\n    axs[1].set_title(\"Augmented Image\")\n    axs[1].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\nclass BaseDataset(data.Dataset):\n    def __init__(self,\n                 ignore_label=255,\n                 base_size=2048,\n                 crop_size=(512, 512),\n                 scale_factor=16,\n                 mean=[0.485, 0.456, 0.406],\n                 std=[0.229, 0.224, 0.225]):\n\n        self.base_size = base_size\n        self.crop_size = crop_size\n        self.ignore_label = ignore_label\n\n        self.mean = mean\n        self.std = std\n        self.scale_factor = scale_factor\n\n        self.files = []\n\n    def __len__(self):\n        return len(self.files)\n\n    def input_transform(self, image, city=False):\n        if city:\n            image = image.astype(np.float32)[:, :, ::-1]\n        else:\n            image = image.astype(np.float32)\n        image = image / 255.0\n        image -= self.mean\n        image /= self.std\n        return image\n\n    def label_transform(self, label):\n        return np.array(label).astype(np.uint8)\n\n    def pad_image(self, image, h, w, size, padvalue):\n        pad_h = max(size[0] - h, 0)\n        pad_w = max(size[1] - w, 0)\n\n        # Se non è necessario il padding, restituisci l'immagine originale\n        if pad_h == 0 and pad_w == 0:\n            return image\n\n        # Verifica il formato dell'immagine (deve essere H, W, C)\n        if len(image.shape) == 3 and image.shape[0] <= 3:  # Se è in formato (C, H, W)\n            image = np.transpose(image, (1, 2, 0))  # Converti in (H, W, C)\n\n        # Aggiungi il padding\n        pad_image = cv2.copyMakeBorder(image, 0, pad_h, 0, pad_w, cv2.BORDER_CONSTANT, value=padvalue)\n\n        # Ripristina il formato originale (C, H, W) se necessario\n        if len(image.shape) == 3 and image.shape[2] <= 3:  # Se era in formato (C, H, W)\n            pad_image = np.transpose(pad_image, (2, 0, 1))  # Converti di nuovo in (C, H, W)\n\n        return pad_image\n\n    def rand_crop(self, image, label, edge):\n        # Verifica il formato dell'immagine\n        if len(image.shape) == 3 and image.shape[0] <= 3:  # Se è in formato (C, H, W)\n            image = np.transpose(image, (1, 2, 0))  # Converti in (H, W, C)\n\n        h, w = image.shape[:2]\n\n        # Aggiungi padding se necessario\n        if h < self.crop_size[0] or w < self.crop_size[1]:\n            image = self.pad_image(image, h, w, self.crop_size, (0.0, 0.0, 0.0))\n            label = self.pad_image(label, h, w, self.crop_size, (self.ignore_label,))\n            edge = self.pad_image(edge, h, w, self.crop_size, (0.0,))\n\n        # Aggiorna le dimensioni dopo il padding\n        new_h, new_w = label.shape\n        if new_h < self.crop_size[0] or new_w < self.crop_size[1]:\n            raise ValueError(f\"Dimensioni insufficienti per il ritaglio: label={label.shape}, crop_size={self.crop_size}\")\n\n        # Calcola le coordinate per il ritaglio casuale\n        x = random.randint(0, new_w - self.crop_size[1])\n        y = random.randint(0, new_h - self.crop_size[0])\n\n        # Esegui il ritaglio\n        image = image[y:y+self.crop_size[0], x:x+self.crop_size[1]]\n        label = label[y:y+self.crop_size[0], x:x+self.crop_size[1]]\n        edge = edge[y:y+self.crop_size[0], x:x+self.crop_size[1]]\n\n        #in questo modo l'iimagine è 512x512x3\n        #se volessi croppare quella regione\n        '''\n        # Estrai la regione da sfocare\n        cropped_region = image[y:y+crop_size[0], x:x+crop_size[1]]\n\n        # Applica il Gaussian Blur alla regione\n        blurred_region = cv2.GaussianBlur(cropped_region, (15, 15), 0)\n\n        # Sostituisci la regione originale con quella sfocata\n        augmented_image = image.copy()\n        augmented_image[y:y+crop_size[0], x:x+crop_size[1]] = blurred_region\n        '''\n\n        return image, label, edge\n\n    def multi_scale_aug(self, image, label=None, edge=None,\n                        rand_scale=1, rand_crop=True):\n        long_size = int(self.base_size * rand_scale + 0.5)\n        h, w = image.shape[:2]\n        if h > w:\n            new_h = long_size\n            new_w = int(w * long_size / h + 0.5)\n        else:\n            new_w = long_size\n            new_h = int(h * long_size / w + 0.5)\n\n        image = cv2.resize(image, (new_w, new_h),\n                           interpolation=cv2.INTER_LINEAR)\n        if label is not None:\n            label = cv2.resize(label, (new_w, new_h),\n                               interpolation=cv2.INTER_NEAREST)\n            if edge is not None:\n                edge = cv2.resize(edge, (new_w, new_h),\n                                   interpolation=cv2.INTER_NEAREST)\n        else:\n            return image\n\n        if rand_crop:\n            image, label, edge = self.rand_crop(image, label, edge)\n\n        return image, label, edge\n\n\n    def gen_sample(self, image, label, edge_pad=True, edge_size=4, city=False, transform=None, show=False):\n\n\n        if transform is not None:\n            # Pass both image and mask\n            augmented = transform(image=image, mask=label)\n\n            if show:\n                show_images(image, augmented[\"image\"])\n\n            # Extract results\n            image = augmented['image']\n            label = augmented['mask']\n\n\n\n        #It' important keeping the edge generation after the data augmentation\n        edge = cv2.Canny(label, 0.1, 0.2)\n        kernel = np.ones((edge_size, edge_size), np.uint8)\n        if edge_pad:\n            edge = edge[y_k_size:-y_k_size, x_k_size:-x_k_size]\n            edge = np.pad(edge, ((y_k_size,y_k_size),(x_k_size,x_k_size)), mode='constant')\n        edge = (cv2.dilate(edge, kernel, iterations=1)>50)*1.0\n\n\n        #trasformazioni di input\n        image = self.input_transform(image, city=city) #Se city=True, converte l'immagine da RGB in BGR per opencv\n        label = self.label_transform(label) #converte la label in un array di interi\n        image = image.transpose((2, 0, 1)) #H,W,C -> C,H,W\n\n        return image, label, edge\n\n\n    def inference(self, model, image):\n        size = image.size()\n        pred = model(image)\n\n        \n        pred = pred[1]\n\n\n        pred = F.interpolate(\n            input=pred, size=size[-2:],\n            mode='bilinear', align_corners=True\n        )\n\n\n        return pred.exp()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:55:46.551318Z","iopub.execute_input":"2025-05-28T07:55:46.551538Z","iopub.status.idle":"2025-05-28T07:55:48.121260Z","shell.execute_reply.started":"2025-05-28T07:55:46.551522Z","shell.execute_reply":"2025-05-28T07:55:48.120291Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import cv2\nimport os\nimport numpy as np\nimport torch\nimport random\nimport logging\nfrom PIL import Image\nimport torchvision.transforms as tf\nimport matplotlib.pyplot as plt\n\ndef show_images(x_original, x_augmented, unnormalize = False):\n\n    if unnormalize:\n        # ImageNet mean and std\n        imagenet_mean = np.array([0.485, 0.456, 0.406])[:, None, None]\n        imagenet_std = np.array([0.229, 0.224, 0.225])[:, None, None]\n\n        # Denormalize using NumPy broadcasting\n        x_original = x_original * imagenet_std + imagenet_mean\n        x_augmented = x_augmented * imagenet_std + imagenet_mean\n\n        # Clip to [0, 1] in case of overflows\n        x_original = np.clip(x_original, 0, 1)\n        x_augmented = np.clip(x_augmented, 0, 1)\n\n        # Transpose to HWC for matplotlib\n        x_original = np.transpose(x_original, (1, 2, 0))\n        x_augmented = np.transpose(x_augmented, (1, 2, 0))\n\n    # Plot\n    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n    axs[0].imshow(x_original)\n    axs[0].set_title(\"Original Image\")\n    axs[0].axis(\"off\")\n\n    axs[1].imshow(x_augmented)\n    axs[1].set_title(\"Augmented Image\")\n    axs[1].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\n\n\nclass LoveDA(BaseDataset):\n    def __init__(self,\n                 root,\n                 list_path,\n                 num_classes=7,\n                 flip=False,\n                 ignore_label=0,\n                 crop_size=(512, 512),\n                 scale_factor=16, #multi scale usato come data augmentation alredy provided\n                 mean=[0.485, 0.456, 0.406],\n                 std=[0.229, 0.224, 0.225],\n                 bd_dilate_size=4,\n                 weighted=True,\n                 transform=None):\n\n        # estende il base_dataset\n        super(LoveDA, self).__init__(ignore_label, crop_size, scale_factor, mean, std)\n\n        self.root = root\n        self.list_path = list_path\n        self.num_classes = num_classes\n        self.flip = flip\n        self.ignore_label = ignore_label\n        self.scale_factor = scale_factor\n        self.bd_dilate_size = bd_dilate_size\n\n        self.img_list = [line.strip().split() for line in open(root + list_path)]\n        self.files = self.read_files()\n        self.color_list = [[0, 0, 0], [1, 1, 1], [2, 2, 2],\n                            [3, 3, 3], [4, 4, 4], [5, 5, 5], [6, 6, 6], [7, 7, 7]]\n        self.class_weights = None\n        if weighted:\n            self.class_weights = torch.tensor([0.000000, 0.116411, 0.266041, 0.607794, 1.511413, 0.745507, 0.712438, 3.040396])\n        \n        self.transform=transform\n\n    def read_files(self):\n        files = []\n\n        for item in self.img_list:\n            image_path, label_path = item\n            name = os.path.splitext(os.path.basename(label_path))[0]\n            files.append({\n                \"img\": image_path,\n                \"label\": label_path,\n                \"name\": name\n            })\n\n        return files\n\n    # da immagine a label\n    def color2label(self, color_map):\n        label = np.ones(color_map.shape[:2]) * self.ignore_label\n        for i, v in enumerate(self.color_list):\n            label[(color_map == v).sum(2) == 3] = i\n\n        return label.astype(np.uint8)\n\n    def convert_label(self, label, inverse=False):\n        temp = label.copy()\n        if inverse:\n            for v, k in self.label_mapping.items():\n                label[temp == k] = v\n        else:\n            for k, v in self.label_mapping.items():\n                label[temp == k] = v\n        return label\n\n    # da label a immagine\n    def label2color(self, label):\n        color_map = np.zeros(label.shape + (3,))\n        for i, v in enumerate(self.color_list):\n            color_map[label == i] = self.color_list[i]\n\n        return color_map.astype(np.uint8)\n\n    def __getitem__(self, index):\n        item = self.files[index]\n        name = item[\"name\"]\n        image = cv2.imread(item[\"img\"], cv2.IMREAD_COLOR)\n\n        size = image.shape\n\n        label = cv2.imread(item[\"label\"], cv2.IMREAD_GRAYSCALE)\n\n\n\n        #edge (H,W)\n        image, label, edge = self.gen_sample(image, label, edge_pad=False,\n                                             edge_size=self.bd_dilate_size, city=False, transform=self.transform, show=False) #image diventa (C,H,W)\n\n        return image.copy(), label.copy(), edge.copy(), np.array(size), name\n\n    def single_scale_inference(self, model, image):\n        pred = self.inference(model, image)\n        return pred\n\n    def save_pred(self, preds, sv_path, name):\n        preds = np.asarray(np.argmax(preds.cpu(), axis=1), dtype=np.uint8)\n        for i in range(preds.shape[0]):\n            pred = self.label2color(preds[i])\n            save_img = Image.fromarray(pred)\n            save_img.save(os.path.join(sv_path, name[i] + '.png'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:55:48.122042Z","iopub.execute_input":"2025-05-28T07:55:48.122476Z","iopub.status.idle":"2025-05-28T07:55:51.614549Z","shell.execute_reply.started":"2025-05-28T07:55:48.122448Z","shell.execute_reply":"2025-05-28T07:55:51.613984Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n\n\nclass CrossEntropy(nn.Module):\n    def __init__(self, ignore_label=-1, weight=None):\n        super(CrossEntropy, self).__init__()\n        self.ignore_label = ignore_label\n        self.criterion = nn.CrossEntropyLoss(\n            weight=weight,\n            ignore_index=ignore_label\n        )\n\n    def _forward(self, score, target):\n\n        loss = self.criterion(score, target)\n\n        return loss\n\n    def forward(self, score, target):\n\n        balance_weights = [0.4, 1.0]\n        sb_weights = 1.0\n        if len(balance_weights) == len(score):\n            return sum([w * self._forward(x, target) for (w, x) in zip(balance_weights, score)])\n        elif len(score) == 1:\n            return sb_weights * self._forward(score[0], target)\n\n        else:\n            raise ValueError(\"lengths of prediction and target are not identical!\")\n\n\n\n\nclass OhemCrossEntropy(nn.Module):\n    def __init__(self, ignore_label=-1, thres=0.7,\n                 min_kept=100000, weight=None):\n        super(OhemCrossEntropy, self).__init__()\n        self.thresh = thres\n        self.min_kept = max(1, min_kept)\n        self.ignore_label = ignore_label\n        self.criterion = nn.CrossEntropyLoss(\n            weight=weight,\n            ignore_index=ignore_label,\n            reduction='none'\n        )\n\n    def _ce_forward(self, score, target):\n\n\n        loss = self.criterion(score, target)\n\n        return loss\n\n    def _ohem_forward(self, score, target, **kwargs):\n\n        pred = F.softmax(score, dim=1)\n        pixel_losses = self.criterion(score, target).contiguous().view(-1)\n        mask = target.contiguous().view(-1) != self.ignore_label\n\n        tmp_target = target.clone()\n        tmp_target[tmp_target == self.ignore_label] = 0\n        pred = pred.gather(1, tmp_target.unsqueeze(1))\n        pred, ind = pred.contiguous().view(-1,)[mask].contiguous().sort()\n        min_value = pred[min(self.min_kept, pred.numel() - 1)]\n        threshold = max(min_value, self.thresh)\n\n        pixel_losses = pixel_losses[mask][ind]\n        pixel_losses = pixel_losses[pred < threshold]\n        return pixel_losses.mean()\n\n    def forward(self, score, target):\n\n        if not (isinstance(score, list) or isinstance(score, tuple)):\n            score = [score]\n\n        balance_weights = [0.4, 1.0]\n        sb_weights = 1.0\n        if len(balance_weights) == len(score):\n            functions = [self._ce_forward] * \\\n                (len(balance_weights) - 1) + [self._ohem_forward]\n            return sum([\n                w * func(x, target)\n                for (w, x, func) in zip(balance_weights, score, functions)\n            ])\n\n        elif len(score) == 1:\n            return sb_weights * self._ohem_forward(score[0], target)\n\n        else:\n            raise ValueError(\"lengths of prediction and target are not identical!\")\n\n\ndef weighted_bce(bd_pre, target):\n    n, c, h, w = bd_pre.size()\n    log_p = bd_pre.permute(0,2,3,1).contiguous().view(1, -1)\n    target_t = target.view(1, -1)\n\n    pos_index = (target_t == 1)\n    neg_index = (target_t == 0)\n\n    weight = torch.zeros_like(log_p)\n    pos_num = pos_index.sum()\n    neg_num = neg_index.sum()\n    sum_num = pos_num + neg_num\n    weight[pos_index] = neg_num * 1.0 / sum_num\n    weight[neg_index] = pos_num * 1.0 / sum_num\n\n    loss = F.binary_cross_entropy_with_logits(log_p, target_t, weight, reduction='mean')\n\n    return loss\n\n\nclass BondaryLoss(nn.Module):\n    def __init__(self, coeff_bce = 20.0):\n        super(BondaryLoss, self).__init__()\n        self.coeff_bce = coeff_bce\n\n    def forward(self, bd_pre, bd_gt):\n\n        bce_loss = self.coeff_bce * weighted_bce(bd_pre, bd_gt)\n        loss = bce_loss\n\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:55:51.616673Z","iopub.execute_input":"2025-05-28T07:55:51.616995Z","iopub.status.idle":"2025-05-28T07:55:51.630451Z","shell.execute_reply.started":"2025-05-28T07:55:51.616978Z","shell.execute_reply":"2025-05-28T07:55:51.629754Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport logging\nimport time\nfrom pathlib import Path\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FullModel(nn.Module):\n\n  def __init__(self, model, sem_loss, bd_loss):\n    super(FullModel, self).__init__()\n    self.model = model\n    self.sem_loss = sem_loss\n    self.bd_loss = bd_loss\n\n  def pixel_acc(self, pred, label):\n    _, preds = torch.max(pred, dim=1)\n    valid = (label >= 0).long()\n    acc_sum = torch.sum(valid * (preds == label).long())\n    pixel_sum = torch.sum(valid)\n    acc = acc_sum.float() / (pixel_sum.float() + 1e-10)\n    return acc\n      \n  \n\n\n  def forward(self, inputs, labels, bd_gt, *args, **kwargs):\n    \n    outputs = self.model(inputs, *args, **kwargs)\n\n    h, w = labels.size(1), labels.size(2)\n    ph, pw = outputs[0].size(2), outputs[0].size(3)\n    if ph != h or pw != w:\n        for i in range(len(outputs)):\n            outputs[i] = F.interpolate(outputs[i], size=(\n                h, w), mode='bilinear', align_corners=True)\n\n    acc  = self.pixel_acc(outputs[-2], labels)\n    loss_s = self.sem_loss(outputs[:-1], labels)\n    loss_b = self.bd_loss(outputs[-1], bd_gt)\n\n    filler = torch.ones_like(labels) * 0\n    try:\n        bd_label = torch.where(torch.sigmoid(outputs[-1][:, 0, :, :]) > 0.8, labels, filler) # 0.7\n        loss_sb = self.sem_loss([outputs[-2]], bd_label)\n    except:\n        print(\"Error in loss computation\")\n        loss_sb = self.sem_loss([outputs[-2]], labels)\n    loss = loss_s + loss_b + loss_sb\n\n    return torch.unsqueeze(loss,0), outputs[:-1], acc, [loss_s, loss_b] #outputs[:-1] è una lista di tensori\n\n'''def forward(self, inputs, labels, bd_gt, *args, **kwargs):\n    # ——— cast targets to the right dtype\n    labels = labels.long()\n    # for BCEWithLogitsLoss we need float targets with a channel dim\n    bd_gt = bd_gt.float().unsqueeze(1)  \n\n    outputs = self.model(inputs, *args, **kwargs)\n\n    # ——— resize outputs to match labels\n    h, w = labels.size(1), labels.size(2)\n    if outputs[0].size(2)!=h or outputs[0].size(3)!=w:\n        for i in range(len(outputs)):\n            outputs[i] = F.interpolate(outputs[i],\n                                       size=(h, w),\n                                       mode='bilinear',\n                                       align_corners=True)\n\n    # ——— pixel accuracy on the penultimate head\n    acc = self.pixel_acc(outputs[-2], labels)\n\n    # ——— multi-level segmentation loss\n    loss_s = sum(self.sem_loss(out, labels) for out in outputs[:-1])\n\n    # ——— boundary loss (now shapes match: [B,1,H,W] vs [B,1,H,W])\n    loss_b = self.bd_loss(outputs[-1], bd_gt)\n\n    # ——— selective semantic loss over high-confidence boundary regions\n    filler = torch.zeros_like(labels)\n    try:\n        mask = torch.sigmoid(outputs[-1][:, 0, ...]) > 0.8\n        bd_label = torch.where(mask, labels, filler)\n        loss_sb = self.sem_loss(outputs[-2], bd_label)\n    except Exception as e:\n        loss_sb = self.sem_loss(outputs[-2], labels)\n\n    total_loss = loss_s + loss_b + loss_sb\n\n    return (\n        total_loss.unsqueeze(0),\n        outputs[:-1],\n        acc,\n        [loss_s, loss_b, loss_sb]\n    )'''\n\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.initialized = False\n        self.val = None\n        self.avg = None\n        self.sum = None\n        self.count = None\n\n    def initialize(self, val, weight):\n        self.val = val\n        self.avg = val\n        self.sum = val * weight\n        self.count = weight\n        self.initialized = True\n\n    def update(self, val, weight=1):\n        if not self.initialized:\n            self.initialize(val, weight)\n        else:\n            self.add(val, weight)\n\n    def add(self, val, weight):\n        self.val = val\n        self.sum += val * weight\n        self.count += weight\n        self.avg = self.sum / self.count\n\n    def value(self):\n        return self.val\n\n    def average(self):\n        return self.avg\n\ndef create_logger(cfg, cfg_name, phase='train'):\n    root_output_dir = Path('/kaggle/working/output')\n\n    \n    folder_name = \"gan\"\n\n    # set up logger\n    if not root_output_dir.exists():\n        print('=> creating {}'.format(root_output_dir))\n        root_output_dir.mkdir()\n\n    dataset = 'loveda'\n    model = 'pidnet_small'\n    cfg_name = os.path.basename('log').split('.')[0]\n\n    final_output_dir = root_output_dir / dataset / cfg_name / folder_name\n\n    print('=> creating {}'.format(final_output_dir))\n    final_output_dir.mkdir(parents=True, exist_ok=True)\n\n    time_str = time.strftime('%Y-%m-%d-%H-%M')\n    log_file = '{}_{}_{}.log'.format('log', time_str, phase)\n    final_log_file = final_output_dir / log_file\n    head = '%(asctime)-15s %(message)s'\n    logging.basicConfig(filename=str(final_log_file),\n                        format=head)\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n    console = logging.StreamHandler()\n    logging.getLogger('').addHandler(console)\n\n    tensorboard_log_dir = Path('log') / dataset / model / \\\n            ('log' + '_' + time_str)\n    print('=> creating {}'.format(tensorboard_log_dir))\n    tensorboard_log_dir.mkdir(parents=True, exist_ok=True)\n\n    return logger, str(final_output_dir), str(tensorboard_log_dir)\n\ndef get_confusion_matrix(label, pred, size, num_class, ignore=-1):\n    \"\"\"\n    Calcute the confusion matrix by given label and pred\n    \"\"\"\n    output = pred.cpu().numpy().transpose(0, 2, 3, 1)\n    seg_pred = np.asarray(np.argmax(output, axis=3), dtype=np.uint8)\n    seg_gt = np.asarray(\n    label.cpu().numpy()[:, :size[-2], :size[-1]], dtype=int)\n\n    ignore_index = seg_gt != ignore\n    seg_gt = seg_gt[ignore_index]\n    seg_pred = seg_pred[ignore_index]\n\n    index = (seg_gt * num_class + seg_pred).astype('int32')\n    label_count = np.bincount(index)\n    confusion_matrix = np.zeros((num_class, num_class))\n\n    for i_label in range(num_class):\n        for i_pred in range(num_class):\n            cur_index = i_label * num_class + i_pred\n            if cur_index < len(label_count):\n                confusion_matrix[i_label,\n                                 i_pred] = label_count[cur_index]\n    return confusion_matrix\n\ndef adjust_learning_rate(optimizer, base_lr, max_iters,\n        cur_iters, power=0.9, nbb_mult=10):\n    lr = base_lr*((1-float(cur_iters)/max_iters)**(power))\n    optimizer.param_groups[0]['lr'] = lr\n    if len(optimizer.param_groups) == 2:\n        optimizer.param_groups[1]['lr'] = lr * nbb_mult\n    return lr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:55:51.631177Z","iopub.execute_input":"2025-05-28T07:55:51.631393Z","iopub.status.idle":"2025-05-28T07:55:51.660963Z","shell.execute_reply.started":"2025-05-28T07:55:51.631379Z","shell.execute_reply":"2025-05-28T07:55:51.660390Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def validate(testloader, model, num_classes=8, ignore_label=0, align_corners=False):\n    model.eval()\n    # we're going to collect two sets of preds\n    confusion_matrix = np.zeros((num_classes, num_classes, 2), dtype=np.float64)\n    with torch.no_grad():\n        for idx, batch in enumerate(testloader):\n            image, label, bd_gts, _, _ = batch\n            size = label.size()\n            image  = image.cuda()\n            label  = label.long().cuda()\n            bd_gts = bd_gts.float().cuda()\n\n            losses, preds, _, _ = model(image, label, bd_gts)\n            if not isinstance(preds, (list, tuple)):\n                preds = [preds]\n\n            for i, x in enumerate(preds):\n                x = F.interpolate(\n                    x,\n                    size=size[-2:],\n                    mode='bilinear',\n                    align_corners=align_corners\n                )\n                cm = get_confusion_matrix(\n                    label,\n                    x,\n                    size,\n                    num_classes,\n                    ignore=ignore_label\n                )\n                confusion_matrix[..., i] += cm\n            if idx % 10 == 0:\n                print(idx)\n\n    # compute IoUs for each head\n    mean_ious = []\n    for i in range(confusion_matrix.shape[-1]):\n        cm = confusion_matrix[..., i]\n        pos = cm.sum(1)\n        res = cm.sum(0)\n        tp  = np.diag(cm)\n        ious = tp / np.maximum(1.0, pos + res - tp)\n        # drop class 0\n        mean_ious.append(ious[1:].mean())\n\n    # return the *second* head's mIoU by default (you can adjust)\n    return mean_ious[1], ious[1:]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:55:51.661571Z","iopub.execute_input":"2025-05-28T07:55:51.661763Z","iopub.status.idle":"2025-05-28T07:55:51.679820Z","shell.execute_reply.started":"2025-05-28T07:55:51.661749Z","shell.execute_reply":"2025-05-28T07:55:51.679090Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import os\nimport itertools\nimport time\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torch.multiprocessing as mp\nimport gc\nimport albumentations as A\n# evita fork: usa spawn (più sicuro per DataLoader)\n# --- Discriminator definition for multi-level adaptation ---\n\"\"\"class Discriminator(nn.Module):\n    def __init__(self, in_channels):\n        super(Discriminator, self).__init__()\n        if in_channels == 1:\n            self.conv = nn.Sequential(\n                nn.Conv2d(1, 1, kernel_size=4, stride=2, padding=1),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(1, 1, kernel_size=4, stride=2, padding=1),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(1, 1, kernel_size=4, stride=2, padding=1),\n            )\n        else: \n            self.conv = nn.Sequential(\n                nn.Conv2d(in_channels, in_channels // 2, kernel_size=4, stride=2, padding=1, bias=False),\n                nn.BatchNorm2d(in_channels // 2),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(in_channels // 2, in_channels // 4, kernel_size=4, stride=2, padding=1, bias=False),\n                nn.BatchNorm2d(in_channels // 4),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(in_channels // 4, 1, kernel_size=4, stride=2, padding=1)\n            )\n\n    def forward(self, x):\n        return self.conv(x)\"\"\"\nclass Discriminator(nn.Module):\n    def __init__(self, in_channels, ndf=64):\n        super(Discriminator, self).__init__()\n        # 5 conv layers: in_channels→ndf→ndf*2→ndf*4→ndf*8→1\n        self.conv1      = nn.Conv2d(in_channels,    ndf,     kernel_size=4, stride=2, padding=1)\n        self.conv2      = nn.Conv2d(ndf,            ndf*2,   kernel_size=4, stride=2, padding=1)\n        self.conv3      = nn.Conv2d(ndf*2,          ndf*4,   kernel_size=4, stride=2, padding=1)\n        self.conv4      = nn.Conv2d(ndf*4,          ndf*8,   kernel_size=4, stride=2, padding=1)\n        self.classifier = nn.Conv2d(ndf*8,          1,       kernel_size=4, stride=2, padding=1)\n        \n        # non-linearity after every conv except the last\n        self.lrelu      = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n        \n        # up-sample by 2⁵=32 to restore input spatial size\n        self.up_sample  = nn.Upsample(scale_factor=32, mode='bilinear', align_corners=False)\n\n    def forward(self, x):\n        x = self.lrelu(self.conv1(x))\n        x = self.lrelu(self.conv2(x))\n        x = self.lrelu(self.conv3(x))\n        x = self.lrelu(self.conv4(x))\n        x = self.classifier(x)\n        x = self.up_sample(x)\n        return x\n# --- Utility: compute IoU from confusion matrix ---\ndef compute_iou(conf_matrix, ignore_index=0):\n    \"\"\"\n    Compute per-class IoU for *all* classes, but set the ignored one to NaN.\n    Returns a list of length N; IoU[ignore_index] == np.nan.\n    \"\"\"\n    num_classes = conf_matrix.shape[0]\n    ious = [float('nan')] * num_classes\n    for cls in range(num_classes):\n        if cls == ignore_index:\n            continue\n        tp = conf_matrix[cls, cls]\n        fp = conf_matrix[:, cls].sum() - tp\n        fn = conf_matrix[cls, :].sum() - tp\n        denom = tp + fp + fn\n        ious[cls] = tp / denom if denom > 0 else float('nan')\n    return ious\n\n\n\n\n# --- Main training loop ---\ndef train_domain_adaptation(\n    root, list_src, list_tgt, list_val,\n    num_classes=8, ignore_label=0,\n    batch_size=2, num_epochs=20,\n    base_lr=1e-2, \n    device='cuda'\n):\n    train_trasform = A.Compose([A.ColorJitter(p=0.5)])\n    # Datasets and loaders\n    src_dataset = LoveDA(root, list_src, num_classes=num_classes, ignore_label=ignore_label, transform=train_trasform)\n    tgt_dataset = LoveDA(root, list_tgt, num_classes=num_classes, ignore_label=ignore_label, transform=train_trasform)\n    val_dataset = LoveDA(root, list_val, num_classes=num_classes, ignore_label=ignore_label)\n\n    src_loader = DataLoader(src_dataset, batch_size=batch_size, pin_memory=False, shuffle=True, num_workers=4, drop_last=True)\n    tgt_loader = DataLoader(tgt_dataset, batch_size=batch_size, pin_memory=False, shuffle=True, num_workers=4, drop_last=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, pin_memory=False, shuffle=False, num_workers=4)\n\n\n    target_iter = iter(tgt_loader)\n    # Models\n    seg_model = get_seg_model()\n    seg_model = seg_model.cuda()\n\n    ouput_branches = ['P', 'I']\n    \n    # FullModel wraps semantic and boundary losses\n    # Assuming sem_loss and bd_loss are defined elsewhere \n    class_weights_loveda = torch.tensor([0.000000, 0.116411, 0.266041, 0.607794, 1.511413, 0.745507, 0.712438, 3.040396])\n    \n    # sem_loss = nn.CrossEntropyLoss(ignore_index=ignore_label, weight=class_weights_loveda)\n    sem_loss = OhemCrossEntropy(ignore_label=ignore_label, thres=0.9, min_kept=131072, weight=class_weights_loveda)\n    bd_loss = BondaryLoss()\n    full_model = FullModel(seg_model, sem_loss, bd_loss).cuda()\n\n    n_discriminators = 3\n    \n    # Discriminators for two adaptation levels\n    # Level 1: intermediate head output channels (P branch)\n    if 'P' in ouput_branches:\n        d1 = Discriminator(in_channels=num_classes).cuda()\n\n    if 'I' in ouput_branches: \n        # Level 2: main output (I branch combined with P and D)\n        d2 = Discriminator(in_channels=num_classes).cuda()\n    if 'D' in ouput_branches:\n        # Level 3: intermediate head output channels (D branch)\n        d3 = Discriminator(in_channels=1).cuda()\n        \n\n    # Optimizers\n    params_dict = dict(full_model.named_parameters())\n    params = [{'params': list(params_dict.values()), 'lr': base_lr}]\n    optimizer_seg = torch.optim.SGD(params,\n                                lr=base_lr,\n                                momentum=0.7,\n                                weight_decay=0.0005,\n                                )\n    if 'P' in ouput_branches:\n        optimizer_d1 = optim.Adam(d1.parameters(), lr=1e-4, betas=(0.9, 0.99))\n        base_lr_d1 = optimizer_d1.param_groups[0]['lr']\n    if 'I' in ouput_branches:\n        optimizer_d2 = optim.Adam(d2.parameters(), lr=1e-4, betas=(0.9, 0.99))\n        base_lr_d2 = optimizer_d2.param_groups[0]['lr']\n        \n    if 'D' in ouput_branches:\n        optimizer_d3 = optim.Adam(d3.parameters(), lr=1e-4, betas=(0.9, 0.99))\n        base_lr_d3 = optimizer_d3.param_groups[0]['lr']\n        \n\n    # Adversarial labels\n    src_label = 1.0\n    tgt_label = 0.0\n\n    \n    lambda_adv1=0.0002\n    lambda_adv2=0.001\n    lambda_adv3=0.00002\n    # Training\n    max_iters = num_epochs * len(src_loader) # corresponds to num_epoch * num_batches\n    cur_iter = 0 # incremented by 1 at each iteration, by the end of the training it will be equal to max_iters\n    best_mIoU = 0.0\n    for epoch in range(num_epochs):\n        seg_model.train()\n        if 'P' in ouput_branches:\n            d1.train()\n            meter_loss_d1 = AverageMeter()\n            meter_adv1  = AverageMeter()\n            \n        if 'I' in ouput_branches:\n            d2.train()\n            meter_adv2  = AverageMeter()\n            meter_loss_d2 = AverageMeter()\n        \n        if 'D' in ouput_branches:    \n            d3.train()\n            meter_adv3  = AverageMeter()\n            meter_loss_d3 = AverageMeter()\n            \n        meter_seg   = AverageMeter()\n        meter_bd    = AverageMeter()\n        meter_acc   = AverageMeter()\n        pbar = tqdm(src_loader, total=len(src_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n        for imgs_s, labels_s, edges_s, _, _ in pbar:\n            imgs_s = imgs_s.cuda(); labels_s = labels_s.long().cuda(); edges_s = edges_s.float().cuda()\n            try:\n                imgs_t, _, _, _, _ = next(target_iter)\n            except StopIteration:\n                target_iter = iter(tgt_loader)\n                imgs_t, _, _, _, _ = next(target_iter)\n            imgs_t = imgs_t.cuda()\n\n            # ----- Train Discriminators -----\n            with torch.no_grad():\n                out_s1, out_s2, out_s3 = seg_model(imgs_s)\n                out_t1, out_t2, out_t3 = seg_model(imgs_t)\n            if 'P' in ouput_branches:\n                prob_s1 = F.softmax(out_s1.detach(), dim=1)\n                prob_t1 = F.softmax(out_t1.detach(), dim=1)\n            if 'I' in ouput_branches:\n                prob_s2 = F.softmax(out_s2.detach(), dim=1)\n                prob_t2 = F.softmax(out_t2.detach(), dim=1)\n            if 'D' in ouput_branches:\n                prob_s3 = torch.sigmoid(out_s3.detach())\n                prob_t3 = torch.sigmoid(out_t3.detach())\n\n            # D1\n            if 'P' in ouput_branches:\n                optimizer_d1.zero_grad()\n                pred_d_s1 = d1(prob_s1)\n                pred_d_t1 = d1(prob_t1)\n                loss_d1 = 0.5 * (F.binary_cross_entropy_with_logits(pred_d_s1, torch.full_like(pred_d_s1, src_label)) +\n                                  F.binary_cross_entropy_with_logits(pred_d_t1, torch.full_like(pred_d_t1, tgt_label)))\n                loss_d1.backward()\n                optimizer_d1.step()\n                meter_loss_d1.update(loss_d1.item())\n\n            # D2\n            if 'I' in ouput_branches:\n                optimizer_d2.zero_grad()\n                pred_d_s2 = d2(prob_s2)\n                pred_d_t2 = d2(prob_t2)\n                loss_d2 = 0.5 * (F.binary_cross_entropy_with_logits(pred_d_s2, torch.full_like(pred_d_s2, src_label)) +\n                                  F.binary_cross_entropy_with_logits(pred_d_t2, torch.full_like(pred_d_t2, tgt_label)))\n                loss_d2.backward()\n                optimizer_d2.step()\n                meter_loss_d2.update(loss_d2.item())\n                \n               \n            # D3\n            if 'D' in ouput_branches:\n                optimizer_d3.zero_grad()\n                pred_d_s3 = d3(prob_s3)\n                pred_d_t3 = d3(prob_t3)\n                loss_d3 = 0.5 * (F.binary_cross_entropy_with_logits(pred_d_s3, torch.full_like(pred_d_s3, src_label)) +\n                                  F.binary_cross_entropy_with_logits(pred_d_t3, torch.full_like(pred_d_t3, tgt_label)))\n                loss_d3.backward()\n                optimizer_d3.step()\n                meter_loss_d3.update(loss_d3.item())\n                \n            # ----- Train Segmentation Network -----\n            optimizer_seg.zero_grad()\n            loss_seg, seg_outputs, acc, loss_components = full_model(imgs_s, labels_s, edges_s)\n            \n            # Adversarial on target\n            ot1, ot2, ot3 = seg_model(imgs_t)\n\n            if 'P' in ouput_branches:\n                prob_t1 = F.softmax(ot1, dim=1)\n                pred_d_t1_for_seg = d1(prob_t1)\n                loss_adv1 = F.binary_cross_entropy_with_logits(pred_d_t1_for_seg, torch.full_like(pred_d_t1_for_seg, src_label)) # predictions from target, but source ground truth\n                \n            if 'I' in ouput_branches:\n                prob_t2 = F.softmax(ot2, dim=1)\n                pred_d_t2_for_seg = d2(prob_t2)\n                loss_adv2 = F.binary_cross_entropy_with_logits(pred_d_t2_for_seg, torch.full_like(pred_d_t2_for_seg, src_label)) # predictions from target, but source ground truth\n                \n            if 'D' in ouput_branches:\n                prob_t3 = torch.sigmoid(ot3)\n                pred_d_t3_for_seg = d3(prob_t3)\n                loss_adv3 = F.binary_cross_entropy_with_logits(pred_d_t3_for_seg, torch.full_like(pred_d_t3_for_seg, src_label)) # predictions from target, but source ground truth\n            \n            loss_total = loss_seg.mean()\n            \n            if 'P' in ouput_branches:\n                loss_total += lambda_adv1 * loss_adv1 \n            if 'I' in ouput_branches:\n                loss_total += lambda_adv2 * loss_adv2\n            if 'D' in ouput_branches:\n                loss_total += lambda_adv3 * loss_adv3\n            \n            loss_total.backward()\n            optimizer_seg.step()\n\n            meter_seg.update( loss_components[0].mean().item() )\n            meter_bd .update( loss_components[1].mean().item() )\n\n            if 'P' in ouput_branches:\n                meter_adv1.update( loss_adv1.item() )\n            if 'I' in ouput_branches:\n                meter_adv2.update( loss_adv2.item() )\n            if 'D' in ouput_branches:\n                meter_adv3.update( loss_adv3.item() )\n            \n            meter_acc.update( acc.item() )\n\n            # Update learning rate\n            cur_iter += 1\n            adjust_learning_rate(optimizer_seg, base_lr, max_iters, cur_iter)\n            if 'P' in ouput_branches:\n                adjust_learning_rate(optimizer_d1, base_lr_d1, max_iters, cur_iter)\n            if 'I' in ouput_branches:\n                adjust_learning_rate(optimizer_d2, base_lr_d2, max_iters, cur_iter)\n            if 'D' in ouput_branches:\n                adjust_learning_rate(optimizer_d3, base_lr_d3, max_iters, cur_iter)\n\n            display_dict = {\n                    'seg_loss': loss_components[0].mean().item(),\n                    'bd_loss': loss_components[1].mean().item(),\n                    'acc': acc.item()\n            }\n            if 'P' in ouput_branches:\n                display_dict['adv1'] = loss_adv1.item()\n                display_dict['loss_d1'] = loss_d1.item()\n                \n            if 'I' in ouput_branches:\n                display_dict['adv2'] = loss_adv2.item()\n                display_dict['loss_d2'] = loss_d2.item()\n                \n            if 'D' in ouput_branches:\n                display_dict['adv3'] = loss_adv3.item()\n                display_dict['loss_d3'] = loss_d3.item()\n\n            # Display losses\n            pbar.set_postfix(display_dict)\n\n        log_str = (\n            f\"Epoch {epoch+1} Train Avg — \"\n            f\"seg_loss: {meter_seg.average():.4f}, \"\n            f\"bd_loss: {meter_bd.average():.4f}, \"\n            f\"acc: {meter_acc.average():.4f}, \"\n        )\n\n        \n        if 'P' in ouput_branches:\n            log_str += f\"adv1: {meter_adv1.average():.4f}, \"\n            log_str += f\"loss_d1: {meter_loss_d1.average():.4f}, \"    \n                    \n        if 'I' in ouput_branches:\n            log_str += f\"adv2: {meter_adv2.average():.4f}, \"\n            log_str += f\"loss_d2: {meter_loss_d2.average():.4f}, \"      \n        \n        if 'D' in ouput_branches:\n            log_str += f\"adv3: {meter_adv3.average():.4f}, \" \n            log_str += f\"loss_d3: {meter_loss_d3.average():.4f}, \" \n        \n        print(log_str)\n        \n        torch.cuda.empty_cache()       # se usi GPU\n        gc.collect()                   # raccogli il garbage Python\n\n\n       # ——— (3) Validation on the FULL model ———\n        print(f\"\\nEpoch {epoch+1} Validation (full_model):\")\n        mIoU, cls_ious = validate(\n            val_loader,\n            full_model,\n            num_classes=num_classes,\n            ignore_label=ignore_label,\n            align_corners=True \n        )\n        print(f\"  ==> full_model mIoU: {mIoU:.4f}\")\n        for idx, iou in enumerate(cls_ious, start=1):\n            print(f\"    Class {idx:2d}: IoU = {iou:.4f}\")\n\n        final_output_dir = os.path.join('/kaggle/working', 'best')\n        os.makedirs(final_output_dir, exist_ok=True)\n        \n        if mIoU > best_mIoU:\n            best_mIoU = mIoU\n            torch.save(full_model.state_dict(), os.path.join(final_output_dir, 'best_full_model.pt'))\n\n        \n\nif __name__ == '__main__':\n    # Example usage\n    train_domain_adaptation(\n        root='/kaggle/working/list/',\n        list_src='urban/train.lst',\n        list_tgt='rural/train.lst',\n        list_val='rural/val.lst',\n        num_classes=8,\n        ignore_label=0,\n        batch_size=6,\n        num_epochs=20,\n        base_lr=1e-2,\n        device='cuda'\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:55:51.680452Z","iopub.execute_input":"2025-05-28T07:55:51.680622Z","iopub.status.idle":"2025-05-28T10:17:01.663901Z","shell.execute_reply.started":"2025-05-28T07:55:51.680608Z","shell.execute_reply":"2025-05-28T10:17:01.662876Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/20: 100%|██████████| 192/192 [03:23<00:00,  1.06s/it, seg_loss=0.617, bd_loss=1.31, acc=0.295, adv1=0.756, loss_d1=0.664, adv2=0.86, loss_d2=0.654] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Train Avg — seg_loss: 1.1733, bd_loss: 1.7715, acc: 0.3003, adv1: 0.8384, loss_d1: 0.6455, adv2: 0.7641, loss_d2: 0.6738, \n\nEpoch 1 Validation (full_model):\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n  ==> full_model mIoU: 0.2109\n    Class  1: IoU = 0.2658\n    Class  2: IoU = 0.2348\n    Class  3: IoU = 0.1590\n    Class  4: IoU = 0.2814\n    Class  5: IoU = 0.0207\n    Class  6: IoU = 0.1379\n    Class  7: IoU = 0.3767\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 100%|██████████| 192/192 [03:14<00:00,  1.01s/it, seg_loss=0.831, bd_loss=0.974, acc=0.524, adv1=1.38, loss_d1=0.675, adv2=1.03, loss_d2=0.562] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Train Avg — seg_loss: 0.6900, bd_loss: 1.4783, acc: 0.4034, adv1: 1.2862, loss_d1: 0.5492, adv2: 0.9830, loss_d2: 0.6099, \n\nEpoch 2 Validation (full_model):\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n  ==> full_model mIoU: 0.2121\n    Class  1: IoU = 0.1943\n    Class  2: IoU = 0.2337\n    Class  3: IoU = 0.1878\n    Class  4: IoU = 0.3322\n    Class  5: IoU = 0.0528\n    Class  6: IoU = 0.1086\n    Class  7: IoU = 0.3754\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20: 100%|██████████| 192/192 [03:05<00:00,  1.03it/s, seg_loss=0.463, bd_loss=1.54, acc=0.416, adv1=0.8, loss_d1=0.85, adv2=0.697, loss_d2=0.722]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Train Avg — seg_loss: 0.6124, bd_loss: 1.4491, acc: 0.4406, adv1: 1.5202, loss_d1: 0.4951, adv2: 1.2966, loss_d2: 0.5450, \n\nEpoch 3 Validation (full_model):\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n  ==> full_model mIoU: 0.2289\n    Class  1: IoU = 0.2237\n    Class  2: IoU = 0.2385\n    Class  3: IoU = 0.2267\n    Class  4: IoU = 0.2911\n    Class  5: IoU = 0.0517\n    Class  6: IoU = 0.1705\n    Class  7: IoU = 0.3999\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20: 100%|██████████| 192/192 [03:13<00:00,  1.01s/it, seg_loss=0.498, bd_loss=1.65, acc=0.458, adv1=2.02, loss_d1=0.5, adv2=1.14, loss_d2=0.492]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Train Avg — seg_loss: 0.5810, bd_loss: 1.4379, acc: 0.4648, adv1: 1.9377, loss_d1: 0.4117, adv2: 1.5419, loss_d2: 0.5015, \n\nEpoch 4 Validation (full_model):\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n  ==> full_model mIoU: 0.2367\n    Class  1: IoU = 0.1761\n    Class  2: IoU = 0.3315\n    Class  3: IoU = 0.2217\n    Class  4: IoU = 0.3720\n    Class  5: IoU = 0.0471\n    Class  6: IoU = 0.1482\n    Class  7: IoU = 0.3603\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/20: 100%|██████████| 192/192 [03:23<00:00,  1.06s/it, seg_loss=0.358, bd_loss=1.28, acc=0.439, adv1=1.72, loss_d1=0.222, adv2=1.04, loss_d2=0.41]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Train Avg — seg_loss: 0.5510, bd_loss: 1.4240, acc: 0.4771, adv1: 2.4616, loss_d1: 0.3512, adv2: 1.8124, loss_d2: 0.4529, \n\nEpoch 5 Validation (full_model):\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n  ==> full_model mIoU: 0.2343\n    Class  1: IoU = 0.1672\n    Class  2: IoU = 0.3216\n    Class  3: IoU = 0.2100\n    Class  4: IoU = 0.3764\n    Class  5: IoU = 0.0422\n    Class  6: IoU = 0.1625\n    Class  7: IoU = 0.3602\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/20: 100%|██████████| 192/192 [03:17<00:00,  1.03s/it, seg_loss=0.56, bd_loss=0.76, acc=0.411, adv1=1.8, loss_d1=1.02, adv2=1.72, loss_d2=0.988]    \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Train Avg — seg_loss: 0.5186, bd_loss: 1.4181, acc: 0.4942, adv1: 2.5548, loss_d1: 0.3232, adv2: 2.0137, loss_d2: 0.4214, \n\nEpoch 6 Validation (full_model):\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n  ==> full_model mIoU: 0.2538\n    Class  1: IoU = 0.2491\n    Class  2: IoU = 0.3253\n    Class  3: IoU = 0.2277\n    Class  4: IoU = 0.3848\n    Class  5: IoU = 0.0844\n    Class  6: IoU = 0.0986\n    Class  7: IoU = 0.4064\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/20: 100%|██████████| 192/192 [03:14<00:00,  1.02s/it, seg_loss=0.497, bd_loss=1.08, acc=0.582, adv1=3.52, loss_d1=0.228, adv2=2.78, loss_d2=0.571] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Train Avg — seg_loss: 0.5014, bd_loss: 1.4096, acc: 0.5064, adv1: 2.7900, loss_d1: 0.2969, adv2: 2.2454, loss_d2: 0.3794, \n\nEpoch 7 Validation (full_model):\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n  ==> full_model mIoU: 0.2450\n    Class  1: IoU = 0.3177\n    Class  2: IoU = 0.2250\n    Class  3: IoU = 0.2206\n    Class  4: IoU = 0.3818\n    Class  5: IoU = 0.0693\n    Class  6: IoU = 0.1220\n    Class  7: IoU = 0.3783\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/20: 100%|██████████| 192/192 [03:17<00:00,  1.03s/it, seg_loss=0.564, bd_loss=1.79, acc=0.518, adv1=4.19, loss_d1=0.37, adv2=1.79, loss_d2=0.501]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 Train Avg — seg_loss: 0.4876, bd_loss: 1.4009, acc: 0.5152, adv1: 2.8978, loss_d1: 0.2911, adv2: 2.1794, loss_d2: 0.3781, \n\nEpoch 8 Validation (full_model):\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n  ==> full_model mIoU: 0.2541\n    Class  1: IoU = 0.3021\n    Class  2: IoU = 0.2678\n    Class  3: IoU = 0.2423\n    Class  4: IoU = 0.4178\n    Class  5: IoU = 0.0586\n    Class  6: IoU = 0.1336\n    Class  7: IoU = 0.3568\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/20: 100%|██████████| 192/192 [03:10<00:00,  1.01it/s, seg_loss=0.423, bd_loss=1.29, acc=0.448, adv1=2.7, loss_d1=0.158, adv2=3.47, loss_d2=0.202]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 Train Avg — seg_loss: 0.4810, bd_loss: 1.3986, acc: 0.5161, adv1: 3.0413, loss_d1: 0.2685, adv2: 2.2998, loss_d2: 0.3616, \n\nEpoch 9 Validation (full_model):\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n  ==> full_model mIoU: 0.2685\n    Class  1: IoU = 0.3156\n    Class  2: IoU = 0.2915\n    Class  3: IoU = 0.2486\n    Class  4: IoU = 0.4553\n    Class  5: IoU = 0.0723\n    Class  6: IoU = 0.1348\n    Class  7: IoU = 0.3617\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/20: 100%|██████████| 192/192 [03:14<00:00,  1.01s/it, seg_loss=0.353, bd_loss=1.41, acc=0.592, adv1=2.52, loss_d1=0.359, adv2=2.35, loss_d2=0.282] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 Train Avg — seg_loss: 0.4641, bd_loss: 1.3914, acc: 0.5368, adv1: 3.3034, loss_d1: 0.2436, adv2: 2.3823, loss_d2: 0.3445, \n\nEpoch 10 Validation (full_model):\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n  ==> full_model mIoU: 0.2846\n    Class  1: IoU = 0.2501\n    Class  2: IoU = 0.3308\n    Class  3: IoU = 0.2805\n    Class  4: IoU = 0.4743\n    Class  5: IoU = 0.0666\n    Class  6: IoU = 0.1512\n    Class  7: IoU = 0.4388\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/20: 100%|██████████| 192/192 [03:14<00:00,  1.01s/it, seg_loss=0.336, bd_loss=0.979, acc=0.554, adv1=4.39, loss_d1=0.348, adv2=1.88, loss_d2=0.485] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 Train Avg — seg_loss: 0.4489, bd_loss: 1.3868, acc: 0.5414, adv1: 3.3107, loss_d1: 0.2456, adv2: 2.2427, loss_d2: 0.3507, \n\nEpoch 11 Validation (full_model):\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n  ==> full_model mIoU: 0.2774\n    Class  1: IoU = 0.3857\n    Class  2: IoU = 0.2260\n    Class  3: IoU = 0.2647\n    Class  4: IoU = 0.4363\n    Class  5: IoU = 0.0932\n    Class  6: IoU = 0.1472\n    Class  7: IoU = 0.3889\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/20: 100%|██████████| 192/192 [03:14<00:00,  1.01s/it, seg_loss=0.39, bd_loss=1.74, acc=0.547, adv1=2.7, loss_d1=0.31, adv2=1.8, loss_d2=0.379]     \n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 Train Avg — seg_loss: 0.4489, bd_loss: 1.3842, acc: 0.5418, adv1: 3.4644, loss_d1: 0.2242, adv2: 2.2590, loss_d2: 0.3478, \n\nEpoch 12 Validation (full_model):\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n  ==> full_model mIoU: 0.2533\n    Class  1: IoU = 0.3162\n    Class  2: IoU = 0.2677\n    Class  3: IoU = 0.2464\n    Class  4: IoU = 0.3676\n    Class  5: IoU = 0.0521\n    Class  6: IoU = 0.1274\n    Class  7: IoU = 0.3960\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/20: 100%|██████████| 192/192 [03:14<00:00,  1.01s/it, seg_loss=0.431, bd_loss=1.11, acc=0.533, adv1=3, loss_d1=0.146, adv2=2.3, loss_d2=0.481]     \n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 Train Avg — seg_loss: 0.4341, bd_loss: 1.3826, acc: 0.5534, adv1: 3.4318, loss_d1: 0.2353, adv2: 2.4742, loss_d2: 0.3180, \n\nEpoch 13 Validation (full_model):\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n  ==> full_model mIoU: 0.2624\n    Class  1: IoU = 0.3305\n    Class  2: IoU = 0.2325\n    Class  3: IoU = 0.2829\n    Class  4: IoU = 0.4072\n    Class  5: IoU = 0.0795\n    Class  6: IoU = 0.1203\n    Class  7: IoU = 0.3836\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/20: 100%|██████████| 192/192 [03:19<00:00,  1.04s/it, seg_loss=0.476, bd_loss=1.22, acc=0.504, adv1=3.87, loss_d1=0.138, adv2=2.15, loss_d2=0.303] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 Train Avg — seg_loss: 0.4338, bd_loss: 1.3756, acc: 0.5560, adv1: 3.4220, loss_d1: 0.2223, adv2: 2.4836, loss_d2: 0.3242, \n\nEpoch 14 Validation (full_model):\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n  ==> full_model mIoU: 0.2982\n    Class  1: IoU = 0.3923\n    Class  2: IoU = 0.2668\n    Class  3: IoU = 0.2733\n    Class  4: IoU = 0.5149\n    Class  5: IoU = 0.0880\n    Class  6: IoU = 0.1615\n    Class  7: IoU = 0.3902\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/20: 100%|██████████| 192/192 [03:09<00:00,  1.02it/s, seg_loss=0.791, bd_loss=1.19, acc=0.639, adv1=4.58, loss_d1=0.331, adv2=2.27, loss_d2=0.619] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 Train Avg — seg_loss: 0.4237, bd_loss: 1.3754, acc: 0.5631, adv1: 3.8821, loss_d1: 0.2017, adv2: 2.5392, loss_d2: 0.3112, \n\nEpoch 15 Validation (full_model):\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n  ==> full_model mIoU: 0.2666\n    Class  1: IoU = 0.3707\n    Class  2: IoU = 0.2562\n    Class  3: IoU = 0.2433\n    Class  4: IoU = 0.4634\n    Class  5: IoU = 0.0727\n    Class  6: IoU = 0.1346\n    Class  7: IoU = 0.3252\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/20: 100%|██████████| 192/192 [03:17<00:00,  1.03s/it, seg_loss=0.36, bd_loss=1.44, acc=0.623, adv1=4.23, loss_d1=0.0936, adv2=2.94, loss_d2=0.23]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 16 Train Avg — seg_loss: 0.4173, bd_loss: 1.3759, acc: 0.5681, adv1: 3.8243, loss_d1: 0.2047, adv2: 2.4484, loss_d2: 0.3207, \n\nEpoch 16 Validation (full_model):\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n  ==> full_model mIoU: 0.2744\n    Class  1: IoU = 0.3614\n    Class  2: IoU = 0.2827\n    Class  3: IoU = 0.2771\n    Class  4: IoU = 0.4219\n    Class  5: IoU = 0.0698\n    Class  6: IoU = 0.1418\n    Class  7: IoU = 0.3659\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/20: 100%|██████████| 192/192 [03:16<00:00,  1.03s/it, seg_loss=0.536, bd_loss=1.28, acc=0.587, adv1=3.16, loss_d1=0.0915, adv2=2.64, loss_d2=0.234] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 Train Avg — seg_loss: 0.4211, bd_loss: 1.3734, acc: 0.5681, adv1: 3.8831, loss_d1: 0.1945, adv2: 2.4135, loss_d2: 0.3193, \n\nEpoch 17 Validation (full_model):\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n  ==> full_model mIoU: 0.2831\n    Class  1: IoU = 0.4311\n    Class  2: IoU = 0.2139\n    Class  3: IoU = 0.2800\n    Class  4: IoU = 0.4259\n    Class  5: IoU = 0.0918\n    Class  6: IoU = 0.1393\n    Class  7: IoU = 0.3996\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/20: 100%|██████████| 192/192 [03:07<00:00,  1.02it/s, seg_loss=0.325, bd_loss=1.31, acc=0.532, adv1=3.98, loss_d1=0.0964, adv2=1.8, loss_d2=0.323]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 Train Avg — seg_loss: 0.4112, bd_loss: 1.3691, acc: 0.5675, adv1: 3.8964, loss_d1: 0.1865, adv2: 2.3943, loss_d2: 0.3155, \n\nEpoch 18 Validation (full_model):\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n  ==> full_model mIoU: 0.2861\n    Class  1: IoU = 0.3695\n    Class  2: IoU = 0.3406\n    Class  3: IoU = 0.2575\n    Class  4: IoU = 0.4418\n    Class  5: IoU = 0.0834\n    Class  6: IoU = 0.1327\n    Class  7: IoU = 0.3768\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/20: 100%|██████████| 192/192 [03:11<00:00,  1.00it/s, seg_loss=0.552, bd_loss=1.26, acc=0.612, adv1=4.64, loss_d1=0.229, adv2=2.83, loss_d2=0.224]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 19 Train Avg — seg_loss: 0.4163, bd_loss: 1.3669, acc: 0.5722, adv1: 3.8546, loss_d1: 0.1953, adv2: 2.4167, loss_d2: 0.3103, \n\nEpoch 19 Validation (full_model):\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n  ==> full_model mIoU: 0.2598\n    Class  1: IoU = 0.3591\n    Class  2: IoU = 0.2375\n    Class  3: IoU = 0.2893\n    Class  4: IoU = 0.3877\n    Class  5: IoU = 0.0722\n    Class  6: IoU = 0.1229\n    Class  7: IoU = 0.3497\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/20: 100%|██████████| 192/192 [03:07<00:00,  1.02it/s, seg_loss=0.393, bd_loss=1.59, acc=0.479, adv1=4.55, loss_d1=0.0672, adv2=2.46, loss_d2=0.213] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 20 Train Avg — seg_loss: 0.4072, bd_loss: 1.3655, acc: 0.5742, adv1: 3.8663, loss_d1: 0.1726, adv2: 2.4736, loss_d2: 0.2981, \n\nEpoch 20 Validation (full_model):\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\n  ==> full_model mIoU: 0.2810\n    Class  1: IoU = 0.3858\n    Class  2: IoU = 0.2963\n    Class  3: IoU = 0.2802\n    Class  4: IoU = 0.4027\n    Class  5: IoU = 0.0796\n    Class  6: IoU = 0.1341\n    Class  7: IoU = 0.3882\n","output_type":"stream"}],"execution_count":13}]}